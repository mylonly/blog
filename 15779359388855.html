<!doctype html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
    
  Kubernets特性介绍 - 独自一人
  
  </title>
  
  
  <link href="atom.xml" rel="alternate" title="独自一人" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/foundation.min.css" />
    <link rel="stylesheet" href="asset/css/docs.css" />
    <script src="asset/js/vendor/modernizr.js"></script>
    <script src="asset/js/vendor/jquery.js"></script>
  <script src="asset/highlightjs/highlight.pack.js"></script>
  <link href="asset/highlightjs/styles/github.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script>hljs.initHighlightingOnLoad();</script>
<script type="text/javascript">
  function before_search(){
    var searchVal = 'site:blog.xgtian.com ' + document.getElementById('search_input').value;
    document.getElementById('search_q').value = searchVal;
    return true;
  }
</script>
  </head>
  <body class="antialiased hide-extras">
    
    <div class="marketing off-canvas-wrap" data-offcanvas>
      <div class="inner-wrap">


<nav class="top-bar docs-bar hide-for-small" data-topbar>


  <section class="top-bar-section">
  <div class="row">
      <div style="position: relative;width:100%;"><div style="position: absolute; width:100%;">
        <ul id="main-menu" class="left">
        
        <li id=""><a target="self" href="index.html">Home</a></li>
        
        <li id=""><a target="_self" href="archives.html">Archives</a></li>
        
        </ul>

        <ul class="right" id="search-wrap">
          <li>
<form target="_blank" onsubmit="return before_search();" action="https://google.com/search" method="get">
    <input type="hidden" id="search_q" name="q" value="" />
    <input tabindex="1" type="search" id="search_input"  placeholder="Search"/>
</form>
</li>
          </ul>
      </div></div>
  </div>
  </section>

</nav>

        <nav class="tab-bar show-for-small">
  <a href="javascript:void(0)" class="left-off-canvas-toggle menu-icon">
    <span> &nbsp; 独自一人</span>
  </a>
</nav>

<aside class="left-off-canvas-menu">
      <ul class="off-canvas-list">
        
        <li><a target="self" href="index.html">Home</a></li>
        
        <li><a target="_self" href="archives.html">Archives</a></li>
        

    <li><label>Categories</label></li>

        
            <li><a href="PM.html">PM</a></li>
        
            <li><a href="ios.html">iOS</a></li>
        
            <li><a href="linux.html">Linux</a></li>
        
            <li><a href="python.html">Python</a></li>
        
            <li><a href="tools.html">Tools</a></li>
        
            <li><a href="django-learn.html">Django学习</a></li>
        
            <li><a href="Tensorflow.html">Tensorflow</a></li>
        
            <li><a href="%E8%BF%9C%E5%8F%A4%E7%9F%A5%E8%AF%86.html">远古知识</a></li>
        
            <li><a href="Kali%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95.html">Kali渗透测试</a></li>
        
            <li><a href="docker.html">Docker</a></li>
        
            <li><a href="Kubernetes.html">Kubernetes</a></li>
         

      </ul>
    </aside>

<a class="exit-off-canvas" href="#"></a>


        <section id="main-content" role="main" class="scroll-container">
        
       

 <script type="text/javascript">
  $(function(){
    $('#menu_item_index').addClass('is_active');
  });
</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
<script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>

<div class="row">
  <div class="large-8 medium-8 columns">
      <div class="markdown-body article-wrap">
       <div class="article">
          
          <h1>Kubernets特性介绍</h1>
     
        <div class="read-more clearfix">
          <span class="date">2020/01/02</span>

          <span>posted in&nbsp;</span> 
          
              <span class="posted-in"><a href='Kubernetes.html'>Kubernetes</a></span>
           
         
          <span class="comments">
            

            
          </span>

        </div>
      </div><!-- article -->

      <div class="article-content">
      <blockquote>
<p>Kubernetes 源于希腊语，意为 “舵手” 或 “飞行员”。它是Google开源的一个容器编排引擎，它支持自动化部署、大规模可伸缩、应用容器化管理,由于Kubernetes单词中，k和s之间的单词数量为8个，所以也简称k8s。</p>
</blockquote>

<h2 id="toc_0">系统架构</h2>

<h3 id="toc_1">单主集群</h3>

<p>单主集群，是集群内只包含一个Control Panel节点的集群，一般用作开发环境，下图是单主集群的结构。<br/>
<img src="https://pic.mylonly.com/2020-01-07-061225.png" alt=""/><br/>
由于主节点只有一个，且work节点无法再主节点宕机后选举产生新的主节点，所有单主集群不适用与产品环境。</p>

<h3 id="toc_2">高可用集群</h3>

<p>高可用集群,是至少包含三个Control Panel节点的集群，Work节点不再直接与某一台主节点的api-server联系，而是通过负载均衡均匀分不到master节点，同时由于etcd节点可以使用外部集群替代，高可用集群又分为内部etcd以及外部etcd集群两种。</p>

<h4 id="toc_3">内部etcd节点集群</h4>

<p><img src="https://pic.mylonly.com/2020-01-08-062708.png" alt=""/></p>

<h4 id="toc_4">外部etcd节点集群</h4>

<p><img src="https://pic.mylonly.com/2020-01-08-062855.png" alt=""/></p>

<h2 id="toc_5">Kubernets的优势</h2>

<p>简化应用程序部署和维护工作，同时最大化利用硬件资源</p>

<h3 id="toc_6">自动修复</h3>

<p>k8s 会自动重新启动失败的容器、替换容器、杀死不响应用户定义的运行状况检查的容器，并且这些都是在用户无感知的情况下进行的（副本数量需要大于1且有正常运行的容器）。</p>

<p>要想介绍k8s的自动修复，必须要介绍存活探针，k8s正是利用存活探针来检查容器是否还在运行，如果探测失败，k8s将自动重启该容器。</p>

<p>k8s有三种探针机制:</p>

<ol>
<li><code>HTTP GET</code>: 针对容器的IP地址，端口以及路径，执行HTTP GET请求，如果收到的返回状态码为2xx或者3xx，则认为探测成功</li>
<li><code>TCP</code>: 针对Socket通信进行探测，尝试与指定端口建立TCP连接，如果连接成功则探测成功。</li>
<li><code>Exec</code>: 在容器内执行指定的命令，并检查命令的退出状态码，如果状态码为0，则探测成功。</li>
</ol>

<p>我们先声明一个拥有两个副本的pod，创建deployment.yaml文件，写入下面的内容</p>

<pre><code class="language-text">apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 2
  selector:
    app: kubia
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
        - name: kubia
          image: luksa/kubia
          livenessProbe:
              httpGet:
                path: /
                port: 8080
          ports:
            - containerPort: 8080
</code></pre>

<p>然后利用kubectl创建pod</p>

<pre><code class="language-text">kubectl create -f deployment.yaml -n default
</code></pre>

<p>查看pod状态</p>

<pre><code class="language-text">ubuntu@k8s-master:~$ kubectl get pods -n default
NAME          READY   STATUS    RESTARTS   AGE
kubia-dvdkl   1/1     Running   0          4m55s
kubia-g7z9g   1/1     Running   0          4m55s
</code></pre>

<p>等两个pod都完全运行起来之后,我们先模拟第一种情况，程序异常退出，我们删除掉其中一个Pod</p>

<pre><code class="language-text">ubuntu@k8s-master:~$ kubectl delete pod kubia-g7z9g -n default
pod &quot;kubia-g7z9g&quot; deleted
ubuntu@k8s-master:~$ kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
kubia-dvdkl   1/1     Running   0          7m43s
kubia-jtq6c   1/1     Running   0          39s
</code></pre>

<p>可以看到，很快另外一个应用以及重新运行起来了。</p>

<p>再来看另外一种情况，节点故障，我们关闭其中一台node节点</p>

<pre><code class="language-text">ubuntu@k8s-master:~$ kubectl get nodes
NAME          STATUS     ROLES    AGE     VERSION
k8s-master    Ready      master   26d     v1.16.2
k8s-node-01   Ready      &lt;none&gt;   26d     v1.16.2
k8s-node-02   Ready      &lt;none&gt;   26d     v1.16.2
k8s-node-03   Ready      &lt;none&gt;   26d     v1.16.2
k8s-node-04   Ready      &lt;none&gt;   18d     v1.16.2
k8s-node-05   Ready      &lt;none&gt;   16d     v1.16.2
k8s-node-06   Ready      &lt;none&gt;   14d     v1.16.2
k8s-node-07   Ready      &lt;none&gt;   4d20h   v1.16.2
k8s-node-08   NotReady   &lt;none&gt;   4d19h   v1.16.2
</code></pre>

<p>等待几分钟后，查看Pods</p>

<pre><code class="language-text">ubuntu@k8s-master:~$ kubectl get pods -n default -o wide
NAME          READY   STATUS        RESTARTS   AGE     IP            NODE          NOMINATED NODE   READINESS GATES
kubia-cpmbh   0/1     Pending       0          0s      &lt;none&gt;        k8s-node-06   &lt;none&gt;           &lt;none&gt;
kubia-dvdkl   1/1     Terminating   1          4d19h   10.244.8.44   k8s-node-08   &lt;none&gt;           &lt;none&gt;
kubia-jtq6c   1/1     Running       0          4d19h   10.244.7.5    k8s-node-07   &lt;none&gt;           &lt;none&gt;
</code></pre>

<p>针对节点故障，k8s并不会把出问题的节点上的所有Pod都迁移到别的Pod上，而是创建新的Pod,原来的Pod仍然保留，只是状态不再是Ready,如果节点恢复，原来的Pod状态也会恢复。</p>

<h3 id="toc_7">服务发现</h3>

<p>我们先为kubia应用创建一个服务,创建service.yaml,写入下面的内容</p>

<pre><code class="language-text">kind: apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  selector:
    app: kubia
  ports:
  - port: 80
    targetPort: 8080
</code></pre>

<p>执行<code>kubectl create -f service.yaml -n default</code>创建服务</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl create -f kubia/service.yaml -n default
service/kubia created
</code></pre>

<p>查看service状态</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl get svc -n default
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP   26d
kubia        ClusterIP   10.107.59.19   &lt;none&gt;        80/TCP    27s
</code></pre>

<p>k8s的服务发现分为两种，集群内部访问和集群外部访问</p>

<h4 id="toc_8">集群内部</h4>

<p>可以通过内置的DNS用服务名的方式访问集群内部的应用，同时也能利用环境变量获取开放的端口。</p>

<h6 id="toc_9">通过环境变量访问服务</h6>

<p>当pod创建时，k8s会把当前命名空间内已存在的服务已环境变量的方式导入到pod当中，所以只要你的pod晚于service的创建，pod里的进程就可以根据环境变量获得服务的IP和端口号。<br/>
我们先查看pod早于service创建的情况</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl exec kubia-8fffd4bff-k4tk4 env -n default
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=kubia-8fffd4bff-k4tk4
KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
KUBERNETES_SERVICE_HOST=10.96.0.1
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_PORT=tcp://10.96.0.1:443
KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_PORT=443
NPM_CONFIG_LOGLEVEL=info
NODE_VERSION=7.9.0
YARN_VERSION=0.22.0
HOME=/root
</code></pre>

<p>然后我们删除pod，让他自动修复</p>

<pre><code class="language-text">kubectl delete pod --all -n default
</code></pre>

<p>查看新的pod</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl get pods -n default
NAME                    READY   STATUS    RESTARTS   AGE
kubia-8fffd4bff-622rl   1/1     Running   0          44s
kubia-8fffd4bff-6h64s   1/1     Running   0          44s
</code></pre>

<p>查看新pod的环境变量</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl exec kubia-8fffd4bff-622rl env -n default
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=kubia-8fffd4bff-622rl
KUBIA_PORT_80_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_PORT=443
KUBIA_SERVICE_PORT=80
KUBERNETES_SERVICE_PORT=443
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
KUBIA_SERVICE_HOST=10.107.59.19
KUBIA_PORT_80_TCP_PORT=80
KUBERNETES_PORT=tcp://10.96.0.1:443
KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
KUBIA_PORT=tcp://10.107.59.19:80
KUBIA_PORT_80_TCP=tcp://10.107.59.19:80
KUBIA_PORT_80_TCP_ADDR=10.107.59.19
KUBERNETES_SERVICE_HOST=10.96.0.1
KUBERNETES_SERVICE_PORT_HTTPS=443
NPM_CONFIG_LOGLEVEL=info
NODE_VERSION=7.9.0
YARN_VERSION=0.22.0
HOME=/root
</code></pre>

<p>对比两种情况，可以发现，<code>KUBIA_SERVICE_HOST</code>和<code>KUBIA_SERVICE_PORT</code>已经存在于环境变量当中了。</p>

<h6 id="toc_10">通过DNS访问服务</h6>

<p>在k8s内部，借助CoreDNS,一旦一个服务创建好，我们可以用<code>hostname.namespace.svc.cluster.local</code>这个域名来获取到服务的IP地址，如果应用和服务同一个集群内部，甚至在同一个命名空间下，我就可以直接用hostname访问该服务<br/>
利用service名称访问服务</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl exec kubia-8fffd4bff-622rl curl http://kubia -n default
You&#39;ve hit kubia-8fffd4bff-622rl
</code></pre>

<pre><code class="language-text">kubectl exec kubia-8fffd4bff-622rl curl http://kubia.default.svc.cluster.local -n default
You&#39;ve hit kubia-8fffd4bff-6h64s
</code></pre>

<p>那么问题来了，利用DNS的方式访问服务确实很方便，但是DNS只能解决IP，针对端口问题怎么办？<br/>
针对这种情况我们的处理办法是固定端口，应为在k8s内部大部分应用独享一个IP的所有端口，所以不存在端口冲突的问题，我们可以针对不同的协议约定一个统一的端口，这样应用就不需要知道具体应用的端口了。</p>

<pre><code class="language-text">常用协议默认端口:
http: 80
https: 443
mysql: 3306
redis: 6379
gRPC: 9000
</code></pre>

<h4 id="toc_11">集群外部</h4>

<p>k8s提供了NodePort,LoadBlancer,Ingres三种服务暴露的方式，应用于各种需要对外暴露服务的场景。<br/>
其中<code>LoadBlancer</code>属于云服务商特有的功能，如果你是私有集群，大概率是用不了这个功能了。</p>

<h6 id="toc_12">NodePort</h6>

<p>NodePort的原理时，在所有Node节点上监听一个端口（所有node节点端口相同），并将传入的数据转发到对应的服务上。</p>

<p>我们修改之前的kubia应用的service.yaml</p>

<pre><code class="language-text">kind: apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  type: NodePort
  selector:
    app: kubia
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 20000
</code></pre>

<p>这次我们指定type类型为NodePort，同时注明nodePort的端口号为20000<br/>
我们更新看看效果</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl delete svc kubia -n default
service &quot;kubia&quot; deleted
ubuntu@k8s-master:~/k8s-dev$ kubectl create -f kubia/service-nodeport.yaml -n default
service/kubia created
</code></pre>

<p>查看新的service</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl get svc -n default
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP        26d
kubia        NodePort    10.111.0.106   &lt;none&gt;        80:30123/TCP   38s
</code></pre>

<p>对比之前的service，新的kubia服务多了一个30123端口，我们可以通过集群任意node节点的ip地址访问这个端口</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ curl http://127.0.0.1:30123
You&#39;ve hit kubia-8fffd4bff-622rl
</code></pre>

<h6 id="toc_13">Ingress</h6>

<p>ingress功能在k8s默认集群里没有提供，需要手动安装开启。ingress可以让应用在http层暴露给外部，通过配置不同的host来指向不同的服务</p>

<p>我们先为kubia这个应用创建一个ingress资源ingress.yaml</p>

<pre><code class="language-text">apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
  - host: kubia.dev.youxuetong.com
    http:
      paths:
        - path: /
          backend: 
            serviceName: kubia
            servicePort: 80
</code></pre>

<p>创建ingress资源并查看</p>

<pre><code class="language-text">ubuntu@k8s-master:~$ kubectl create -f k8s-dev/kubia/ingress.yaml -n default
ingress.networking.k8s.io/kubia created
ubuntu@k8s-master:~$ kubectl get ingress -n default
NAME    HOSTS                      ADDRESS   PORTS   AGE
kubia   kubia.dev.youxuetong.com             80      13s
</code></pre>

<p>我们试试通过域名访问这个服务</p>

<pre><code class="language-text">➜  ~ curl http://kubia.dev.youxuetong.com
You&#39;ve hit kubia-8fffd4bff-622rl
</code></pre>

<h3 id="toc_14">负载均衡</h3>

<p>k8s可以将网络流量随机分发(按照一定规则,在最新的1.17中，会选择一个最短路由的pod节点)到pod节点上。</p>

<p>我们可以看到每次访问同一服务，都可能会落到不同的pod节点上</p>

<pre><code class="language-text">➜  ~ curl http://kubia.dev.youxuetong.com
You&#39;ve hit kubia-8fffd4bff-622rl
➜  ~ curl http://kubia.dev.youxuetong.com
You&#39;ve hit kubia-8fffd4bff-622rl
➜  ~ curl http://kubia.dev.youxuetong.com
You&#39;ve hit kubia-8fffd4bff-622rl
➜  ~ curl http://kubia.dev.youxuetong.com
You&#39;ve hit kubia-8fffd4bff-6h64s
➜  ~ curl http://kubia.dev.youxuetong.com
You&#39;ve hit kubia-8fffd4bff-6h64s
➜  ~ curl http://kubia.dev.youxuetong.com
You&#39;ve hit kubia-8fffd4bff-6h64s
</code></pre>

<p>如果有些特殊应用，希望同一个用户的访问能指向同一个pod，比如处理socket长连接的应用，可以通过更改service的会话亲和性来达到效果</p>

<pre><code class="language-text">apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  sessionAffinity: ClientIP
  selector:
    app: kubia
  ports:
  - port: 80
    targetPort: 8080
</code></pre>

<p>重新创建service后我们来看看效果</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl delete svc kubia -n default
service &quot;kubia&quot; deleted
ubuntu@k8s-master:~/k8s-dev$ kubectl create -f kubia/service-session.yaml -n default
service/kubia created
</code></pre>

<p>利用curl访问</p>

<pre><code class="language-text">ubuntu@k8s-master:~$ curl http://127.0.0.1:30123
You&#39;ve hit kubia-8fffd4bff-6h64s
ubuntu@k8s-master:~$ curl http://127.0.0.1:30123
You&#39;ve hit kubia-8fffd4bff-6h64s
ubuntu@k8s-master:~$ curl http://127.0.0.1:30123
You&#39;ve hit kubia-8fffd4bff-6h64s
ubuntu@k8s-master:~$ curl http://127.0.0.1:30123
You&#39;ve hit kubia-8fffd4bff-6h64s
ubuntu@k8s-master:~$ curl http://127.0.0.1:30123
You&#39;ve hit kubia-8fffd4bff-6h64s
ubuntu@k8s-master:~$ curl http://127.0.0.1:30123
You&#39;ve hit kubia-8fffd4bff-6h64s    
</code></pre>

<h3 id="toc_15">配置文件</h3>

<p>k8s支持将一些敏感信息以及相关应用的配置文件单独存放，做到和应用无关。</p>

<p>创建ConfigMap,configmap.yaml</p>

<pre><code class="language-text">apiVersion: v1
kind: ConfigMap
metadata:
  name: kubia
  labels:
    app: kubia
   
data:
  config.json: |-
    {
      &quot;service&quot;:{
        &quot;host&quot;:&quot;http://kubia.dev.youxuetong.com&quot;,
        &quot;ip&quot;:&quot;10.9.22.1&quot;,
        &quot;port&quot;:80
      }    
    }
  config.yaml: |-
    service:
      host: &quot;http://kubia.dev.youxuetong.com&quot;
      ip: &quot;10.9.22.1&quot;
      port: 80
</code></pre>

<p>修改deployment.yaml</p>

<pre><code class="language-text">apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      volumes:
        - name: config
          configMap:
            name: kubia
      containers:
        - name: kubia
          image: luksa/kubia
          livenessProbe:
              httpGet:
                path: /
                port: 8080
          volumeMounts:
          - name: config
            mountPath: /opt/config
            readOnly: true
          ports:
            - containerPort: 8080
</code></pre>

<p>生成configmap，删除之前的deployment，创建新的</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl create -f kubia/configmap.yaml -n default
configmap/kubia created
ubuntu@k8s-master:~/k8s-dev$ kubectl delete deployment kubia -n default
deployment.apps/kubia deleted
ubuntu@k8s-master:~/k8s-dev$ kubectl create -f kubia/deployment-config.yaml -n default
deployment.apps/kubia created
</code></pre>

<p>我们进入pod中查看配置文件是否挂载成功</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl exec kubia-65cbb6d475-c5pqk ls /opt/config -n default
config.json
config.yaml
ubuntu@k8s-master:~/k8s-dev$ kubectl exec kubia-65cbb6d475-c5pqk cat /opt/config/config.json -n default
{
  &quot;service&quot;:{
    &quot;host&quot;:&quot;http://kubia.dev.youxuetong.com&quot;,
    &quot;ip&quot;:&quot;10.9.22.1&quot;,
    &quot;port&quot;:80
  }
}
ubuntu@k8s-master:~/k8s-dev$ kubectl exec kubia-65cbb6d475-c5pqk cat /opt/config/config.yaml -n default
service:
  host: &quot;http://kubia.dev.youxuetong.com&quot;
  ip: &quot;10.9.22.1&quot;
  port: 80
</code></pre>

<h3 id="toc_16">密钥管理</h3>

<p>密钥管理和配置文件类似，只是密钥当中存储的的是一些敏感信息而已。<br/>
我们以https证书举例，当我们在k8s当中，想给我们的域名绑上https证书的时候<br/>
先创建证书secret，需要实现准备好证书的key和证书文件</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev/ssl-dev.youxuetong.com$ kubectl create secret tls dev.youxuetong.com --cert=server.crt --key=server.key -n default
secret/dev.youxuetong.com created
</code></pre>

<p>这时我们如果想给之前通过ingress暴露的域名<a href="http://kubia.dev.youxuetong.com%E5%8A%A0%E4%B8%8Ahttps%E8%AF%81%E4%B9%A6%EF%BC%8C%E5%8F%AA%E9%9C%80%E8%A6%81%E4%BF%AE%E6%94%B9ingress.yaml">http://kubia.dev.youxuetong.com加上https证书，只需要修改ingress.yaml</a></p>

<pre><code class="language-text">apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
  - host: kubia.dev.youxuetong.com
    http:
      paths:
        - path: /
          backend: 
            serviceName: kubia
            servicePort: 80
  tls:
  - hosts:
    - kubia.dev.youxuetong.com
    secretName: dev.youxuetong.com
</code></pre>

<p>增加tls字段，为kubia.dev.youxuetong.com 绑上之前创建好的secret<br/>
我们利用curl查看https证书有没有生效</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ curl -k -v https://kubia.dev.youxuetong.com/
*   Trying 125.46.60.5...
* Connected to kubia.dev.youxuetong.com (125.46.60.5) port 443 (#0)
* found 149 certificates in /etc/ssl/certs/ca-certificates.crt
* found 596 certificates in /etc/ssl/certs
* ALPN, offering http/1.1
* SSL connection using TLS1.2 / ECDHE_RSA_AES_256_GCM_SHA384
*    server certificate verification SKIPPED
*    server certificate status verification SKIPPED
*    common name: *.dev.youxuetong.com (matched)
*    server certificate expiration date OK
*    server certificate activation date OK
*    certificate public key: RSA
*    certificate version: #3
*    subject: C=CN,ST=HeNan,L=ZhengZhou,O=JiangShan Technology Co.Ltd,OU=JiangShan Technology Co.Ltd,CN=*.dev.youxuetong.com,EMAIL=yxt@youxuetong.com
*    start date: Fri, 03 Jan 2020 02:27:09 GMT
*    expire date: Mon, 17 May 2021 02:27:09 GMT
*    issuer: C=CN,ST=henan,L=zhengzhou,O=Jiangshan Co.Ltd,OU=Jiangshan Co.Ltd,CN=Jiangshan Co.Ltd,EMAIL=yxt@youxuetong.com
*    compression: NULL
* ALPN, server accepted to use http/1.1
&gt; GET / HTTP/1.1
&gt; Host: kubia.dev.youxuetong.com
&gt; User-Agent: curl/7.47.0
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 200 OK
&lt; Server: openresty/1.15.8.2
&lt; Date: Tue, 14 Jan 2020 06:26:32 GMT
&lt; Transfer-Encoding: chunked
&lt; Connection: keep-alive
&lt; Strict-Transport-Security: max-age=15724800; includeSubDomains
&lt;
You&#39;ve hit kubia-65cbb6d475-c5pqk
</code></pre>

<h3 id="toc_17">统一存储</h3>

<p>k8s允许您自由选择的适合自己存储系统，无论是本地存储、远程目录共享还是云服务商提供的网络磁盘，并提供统一的分配和挂载方案。<br/>
存储分为静态存储和动态存储</p>

<h4 id="toc_18">静态存储</h4>

<p>静态存储需要事先在node上创建目录，分配空间，然后将目录挂载到pod上，和docker的目录挂载原理一样。这样的缺点就是每创建一个应用之前还需要手动创建目录，同时这样生成的pod的流动性就很差了，所以大部分情况下我们不会使用。</p>

<h4 id="toc_19">动态存储</h4>

<p>无需实现绑定，应用程序只需创建一个存储卷声明，声明所需磁盘空间大小和类型，即可自动完成创建和绑定。<br/>
我们先创建一个持久卷声明pvc.yaml</p>

<pre><code class="language-text">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: kubia-pvc
  labels:
    app: kubia
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
</code></pre>

<p>然后在deployment上绑定已经创建的pvc</p>

<pre><code class="language-text">apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      volumes:
        - name: kubia-data
          persistentVolumeClaim:
            claimName: kubia-pvc
      containers:
        - name: kubia
          image: luksa/kubia
          livenessProbe:
              httpGet:
                path: /
                port: 8080
          volumeMounts:
          - name: kubia-data
            mountPath: /data
          ports:
            - containerPort: 8080
</code></pre>

<p>重新创建各项资源</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl create -f kubia/pvc.yaml -n default
persistentvolumeclaim/kubia-pvc created
ubuntu@k8s-master:~/k8s-dev$ kubectl get pvc -n default
NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
kubia-pvc   Bound    pvc-bb91d0ac-5076-41ba-b1fa-beb26473a0dc   10Gi       RWO            nfs-client     29s
ubuntu@k8s-master:~/k8s-dev$ kubectl delete deployment kubia -n default
deployment.apps &quot;kubia&quot; deleted
ubuntu@k8s-master:~/k8s-dev$ kubectl create -f kubia/deployment-pvc.yaml -n default
deployment.apps/kubia created
</code></pre>

<p>我们进入目录查看目录是否挂载成功,并创建一个文件</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl exec kubia-5bb69778fc-n2v6k touch /data/readme.md -n default
</code></pre>

<p>去NFS服务器查看文件是否成功创建</p>

<pre><code class="language-text">ubuntu@K8S-NFS:/data$ cd default-kubia-pvc-pvc-bb91d0ac-5076-41ba-b1fa-beb26473a0dc/
ubuntu@K8S-NFS:/data/default-kubia-pvc-pvc-bb91d0ac-5076-41ba-b1fa-beb26473a0dc$ ll
total 8
drwxrwxrwx  2 root root 4096 Jan 14 00:54 ./
drwxrwxrwx 21 root root 4096 Jan 14 00:51 ../
-rw-r--r--  1 root root    0 Jan 14 00:54 readme.md
</code></pre>

<h3 id="toc_20">统一日志</h3>

<p>利用EFK（elasticsearch,fluentd,kibaba）很容易做到对这个集群的日志跟踪。</p>

<p>在k8s里，Fluentd会被安装在每个Node节点上，同时会挂载当前机器的/var/lib/docker/containers目录，由于docker所有的标准输出都会在该目录下记录日志，所以Fluentd可以很容易的做到将整个集群上所允许的应用的日志统一发送到后端。</p>

<h3 id="toc_21">统一监控</h3>

<p>利用Metric-Server可以获取pod集群的资源占用情况的数据，在加上Prometheus和Grafana很容搭建一套集群的监控报警系统。</p>

<h3 id="toc_22">滚动升级&amp;回滚</h3>

<p>k8s提供逐步更新应用程序的能力，并提供不同的升级策略，让用户无感知情况下升级或者回滚应用程序。</p>

<h3 id="toc_23">自动伸缩</h3>

<p>k8s不仅提供手动的应用程序扩容机制，同时还可以通过监视应用程序的CPU使用率或其他度量增长时自动对应用程序进行扩容，已应用短期的高并发请求。</p>


    

      </div>

      <div class="row">
        <div class="large-6 columns">
        <p class="text-left" style="padding:15px 0px;">
      
          <a href="15786388799784.html" 
          title="Previous Post: k8s-1.16单主节点集群部署">&laquo; k8s-1.16单主节点集群部署</a>
      
        </p>
        </div>
        <div class="large-6 columns">
      <p class="text-right" style="padding:15px 0px;">
      
          <a  href="15774160287586.html" 
          title="Next Post: K8S日志系统EFK">K8S日志系统EFK &raquo;</a>
      
      </p>
        </div>
      </div>
      <div class="comments-wrap">
        <div class="share-comments">
          

          

          
        </div>
      </div>
      <div id="gitalk-container"></div>
      <script type="text/javascript">
      var gitalk = new Gitalk({
        clientID: 'd91ffa1480549c466d2f',
        clientSecret: '92600b8db63745566efb39c0ad9a15c2462b0008',
        repo: 'blog',
        owner: 'mylonly',
        admin: ['mylonly'],
        id: location.pathname,      // Ensure uniqueness and length less than 50
        distractionFreeMode: false  // Facebook-like distraction free mode
      })
      gitalk.render('gitalk-container')
      </script>
    </div><!-- article-wrap -->
  </div><!-- large 8 -->




 <div class="large-4 medium-4 columns">
  <div class="hide-for-small">
    <div id="sidebar" class="sidebar">
          <div id="site-info" class="site-info">
            
                <div class="site-a-logo"><img src="https://pic.mylonly.com/2017-05-11-IMG_1164.JPG" /></div>
            
                <h1>独自一人</h1>
                <div class="site-des">独自一人,独自Coding...</div>
                <div class="social">









<a target="_blank" class="github" target="_blank" href="https://github.com/mylonly" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:root@mylonly.com" title="Email">Email</a>
  <a target="_blank" class="rss" href="atom.xml" title="RSS">RSS</a>
                
              	 </div>
          	</div>

             

              <div id="site-categories" class="side-item ">
                <div class="side-header">
                  <h2>Categories</h2>
                </div>
                <div class="side-content">

      	<p class="cat-list">
        
            <a href="PM.html"><strong>PM</strong></a>
        
            <a href="ios.html"><strong>iOS</strong></a>
        
            <a href="linux.html"><strong>Linux</strong></a>
        
            <a href="python.html"><strong>Python</strong></a>
        
            <a href="tools.html"><strong>Tools</strong></a>
        
            <a href="django-learn.html"><strong>Django学习</strong></a>
        
            <a href="Tensorflow.html"><strong>Tensorflow</strong></a>
        
            <a href="%E8%BF%9C%E5%8F%A4%E7%9F%A5%E8%AF%86.html"><strong>远古知识</strong></a>
        
            <a href="Kali%E6%B8%97%E9%80%8F%E6%B5%8B%E8%AF%95.html"><strong>Kali渗透测试</strong></a>
        
            <a href="docker.html"><strong>Docker</strong></a>
        
            <a href="Kubernetes.html"><strong>Kubernetes</strong></a>
         
        </p>


                </div>
              </div>

              <div id="site-categories" class="side-item">
                <div class="side-header">
                  <h2>Recent Posts</h2>
                </div>
                <div class="side-content">
                <ul class="posts-list">
	      
		      
			      <li class="post">
			        <a href="15786401810709.html">k8s-1.16高可用集群部署</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15786388799784.html">k8s-1.16单主节点集群部署</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15779359388855.html">Kubernets特性介绍</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15774160287586.html">K8S日志系统EFK</a>
			      </li>
		     
		  
		      
			      <li class="post">
			        <a href="15773284865375.html">WebDashboard UI部署</a>
			      </li>
		     
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		  
		      
		   
		  		</ul>
                </div>
              </div>
        </div><!-- sidebar -->
      </div><!-- hide for small -->
</div><!-- large 4 -->

</div><!-- row -->

 <div class="page-bottom clearfix">
  <div class="row">
   <p class="copyright">Copyright &copy; 2015
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
Theme used <a target="_blank" href="http://github.com">GitHub CSS</a>.</p>
  </div>
</div>

        </section>
      </div>
    </div>

  
    

    <script src="asset/js/foundation.min.js"></script>
    <script>
      $(document).foundation();
      function fixSidebarHeight(){
        var w1 = $('.markdown-body').height();
          var w2 = $('#sidebar').height();
          if (w1 > w2) { $('#sidebar').height(w1); };
      }
      $(function(){
        fixSidebarHeight();
      })
      $(window).load(function(){
          fixSidebarHeight();
      });
     
    </script>

    

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?902c93630d3463215e7d2ac5ce0f4003";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>


  </body>
</html>
