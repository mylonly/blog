<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
	<title>  
	  
  	独自一人
  	
	</title>

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

	<link href="atom.xml" rel="alternate" title="独自一人" type="application/atom+xml">

	<link href="asset/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<link href="asset/stylesheets/font-awesome.min.css" media="screen, projection" rel="stylesheet" type="text/css">
	<script src="asset/javascripts/jquery.min.js"></script>
	<script src="asset/highlightjs/highlight.pack.js"></script>
	<link href="asset/highlightjs/styles/solarized_dark.css" media="screen, projection" rel="stylesheet" type="text/css">
<script>hljs.initHighlightingOnLoad();</script>

	<!--[if lt IE 9]><script src="asset/javascripts/html5.js"></script><![endif]-->
	<!-- <link href='http://fonts.googleapis.com/css?family=Nunito:400,300,700' rel='stylesheet' type='text/css'> -->
	<style type="text/css">
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 300;
  src: local('Nunito-Light'), url(asset/font/1TiHc9yag0wq3lDO9cw0voX0hVgzZQUfRDuZrPvH3D8.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 400;
  src: local('Nunito-Regular'), url(asset/font/6TbRXKWJjpj6V2v_WyRbMX-_kf6ByYO6CLYdB4HQE-Y.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 700;
  src: local('Nunito-Bold'), url(asset/font/TttUCfJ272GBgSKaOaD7KoX0hVgzZQUfRDuZrPvH3D8.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
	</style>
	
	<style type="text/css">
	.container .left-col{ opacity: 1;}
	#pagenavi a{ font-size: 1.3em;}
	#pagenavi .next:before{ top: 3px;}
	#pagenavi .prev:before{ top: 3px;}
	.container .mid-col .mid-col-container #content .archives .title{ font-size: 1.5em;}
	.container .mid-col .mid-col-container #content article{ padding: 15px 0px;}
	#header .subtitle {
		line-height: 1.2em;
		padding-top: 8px;
	}
	article pre{ background: none; border: none; padding: 0;}
	article .entry-content{text-align: left;}
	.share-comment{ padding: 25px 0px; clear: both;}
	hr{ margin: 20px 0px;border: 0; border-top:solid 1px #ddd;}
	</style>
  

</head>


<body>
	<div class="container">
		<div class="left-col">
			<div class="intrude-less">
				<header id="header" class="inner">
				 
				 	<div class="profilepic">
						<img src="https://pic.mylonly.com/2017-05-11-IMG_1164.JPG" style="width:160px;">
					</div>
            	
					
					<h1><a href="index.html">独自一人</a></h1>
					<p class="subtitle">独自一人,独自Coding...</p>
					<nav id="main-nav">
						<ul class="main">
						
						  <li id=""><a target="self" href="index.html">Home</a></li>
						
						  <li id=""><a target="_self" href="archives.html">Archives</a></li>
						
						</ul>
					</nav>

					<nav id="sub-nav">
						<div class="social">










<a target="_blank" class="github" target="_blank" href="https://github.com/mylonly" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:root@mylonly.com" title="Email">Email</a>

								

								<a class="rss" href="atom.xml" title="RSS">RSS</a>
							
						</div>
					</nav>
				</header>				
			</div>
		</div>	
		<div class="mid-col">
			<div class="mid-col-container"> <div id="content" class="inner">
<div itemscope itemtype="http://schema.org/Blog">


	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2016-05-11T19:12:34+08:00" itemprop="datePublished">2016/5/11</time>
			</div>
			<div class="tags">posted in 
			
			    <a class='category' href='python.html'>Python</a>&nbsp;
			 
			</div>
		</div>
		<h1 class="title" itemprop="name"><a href="14945011548149.html" itemprop="url">
		简单Scrapy爬虫</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<p>只是个小小的demo，自己测试了下，总共down了20几张图片</p>

<pre><code class="language-Python">#coding: utf-8 #############################################################
# File Name: spiders/wallpaper.py
# Author: mylonly
# mail: tianxianggen@gmail.com
#Blog:www.mylonly.com
# Created Time: 2014年09月01日 星期一 14时20分07秒
#########################################################################
#!/usr/bin/python
import urllib2
import os
 
from scrapy.contrib.spiders import CrawlSpider,Rule
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
from scrapy.selector import Selector
from mylonly.items import wallpaperItem
from scrapy.http import Request
 
class wallpaper(CrawlSpider):
    name = &quot;wallpaperSpider&quot;
    allowed_domains = [&#39;sj.zol.com.cn&#39;]
    start_urls = [&#39;http://sj.zol.com.cn/&#39;]
    number = 0
    rules = (
    Rule(SgmlLinkExtractor(allow = (&#39;detail_\d{4}_\d{5}\.html&#39;)),callback =     &#39;parse_image&#39;,follow=True),)
    def parse_image(self,response):
        self.log(&#39;hi,this is an item page! %s&#39; % response.url)
        sel = Selector(response)
        sites = sel.xpath(&quot;//div[@class=&#39;wrapper        mt15&#39;]//dd[@id=&#39;tagfbl&#39;]//a[@target=&#39;_blank&#39;]/@href&quot;).extract()
        for site in sites:
        url = &#39;http://sj.zol.com.cn%s&#39; % (site)
        print &#39;one page:&#39;,url
        return Request(url,callback = self.parse_href)
    def parse_href(self,response):
        print &#39;I am in:&#39;,response.url
        sel = Selector(response)
        src = sel.xpath(&quot;//body//img/@src&quot;).extract()[0]
        self.download(src)
 
    def download(self,url):
        self.number += 1
        savePath = &#39;/mnt/python_image/%d.jpg&#39; % (self.number)
        print &#39;正在下载...&#39;,url
        try:
            u = urllib2.urlopen(url)
            r = u.read()
            downloadFile = open(savePath,&#39;wb&#39;)
            downloadFile.write(r)
            u.close()
            downloadFile.close()
        except:
            print savePath,&#39;can not download.&#39;
 
</code></pre>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2016-05-11T19:11:56+08:00" itemprop="datePublished">2016/5/11</time>
			</div>
			<div class="tags">posted in 
			
			    <a class='category' href='python.html'>Python</a>&nbsp;
			 
			</div>
		</div>
		<h1 class="title" itemprop="name"><a href="14945011166438.html" itemprop="url">
		Scrapy——crawlSpider爬虫</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<p>Scrapy中的BaseSpider爬虫类只能抓取start_urls中提供的链接，而利用Scrapy提供的crawlSpider类可以很方便的自动解析网页上符合要求的链接，从而达到爬虫自动抓取的功能。<br/>
 <br/>
要利用crawSpider和BaseSpider的区别在于crawSpider提供了一组Rule对象列表，这些Rule对象规定了爬虫抓取链接的行为，Rule规定的链接才会被抓取，交给相应的callback函数去处理。<br/>
  <br/>
在rules中通过SmglLinkExtractor提取希望获取的链接。<br/>
 <br/>
我此次的demo中rule只有一个，如下：</p>

<pre><code class="language-python">allowed_domains = [&#39;sj.zol.com.cn&#39;]
start_urls = [&#39;http://sj.zol.com.cn/bizhi/&#39;]
number = 0
rules = (
            Rule(SgmlLinkExtractor(allow = (&#39;detail_\d{4}_\d{5}\.html&#39;)),callback = &#39;parse_image&#39;,follow=True),
            )
</code></pre>

<p>搜索起始链接下面符合allow中正则表达式的链接，并跟进解析，如果follow = False，则只会解析起始链接中找到的符合要求的链接<br/>
 <br/>
SmglLinkExtractor主要参数：<br/>
 <br/>
- allow：满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。<br/>
- deny：与这个正则表达式(或正则表达式列表)不匹配的URL一定不提取。<br/>
- allow_domains：会被提取的链接的domains。<br/>
- deny_domains：一定不会被提取链接的domains。<br/>
- restrict_xpaths：使用xpath表达式，和allow共同作用过滤链接。<br/>
 <br/>
下面的内容就是利用Selector解析获得的response并赋值个item就行了：</p>

<pre><code class="language-python">#coding: utf-8 #############################################################
# File Name: spiders/wallpaper.py
# Author: mylonly
# mail: tianxianggen@gmail.com
#Blog:www.mylonly.com
# Created Time: 2014年09月01日 星期一 14时20分07秒
#########################################################################
#!/usr/bin/python
import urllib2
import os
import re
 
from scrapy.contrib.spiders import CrawlSpider,Rule
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
from scrapy.selector import Selector
from mylonly.items import wallpaperItem
from scrapy.http import Request
 
class wallpaper(CrawlSpider):
        name = &quot;wallpaperSpider&quot;
        allowed_domains = [&#39;sj.zol.com.cn&#39;]
        start_urls = [&#39;http://sj.zol.com.cn/bizhi/&#39;]
        number = 0
        rules = (
                        Rule(SgmlLinkExtractor(allow = (&#39;detail_\d{4}_\d{5}\.html&#39;)),callback = &#39;parse_image&#39;,follow=True),
                        )
        def parse_image(self,response):
                self.log(&#39;hi,this is an item page! %s&#39; % response.url)
                sel = Selector(response)
                sites = sel.xpath(&quot;//div[@class=&#39;wrapper mt15&#39;]//dd[@id=&#39;tagfbl&#39;]//a[@target=&#39;_blank&#39;]/@href&quot;).extract()       
                for site in sites:
                        url = &#39;http://sj.zol.com.cn%s&#39; % (site)
                        print &#39;one page:&#39;,url
                        item = wallpaperItem()
                        item[&#39;size&#39;] = re.search(&#39;\d*x\d*&#39;,site).group()
                        item[&#39;altInfo&#39;] = sel.xpath(&quot;//h1//a/text()&quot;).extract()[0]
                        return Request(url,meta = {&#39;item&#39;:item},callback = self.parse_href)
        def parse_href(self,response):
                print &#39;I am in:&#39;,response.url
                item = response.meta[&#39;item&#39;]
                items = []
                sel = Selector(response)
                src = sel.xpath(&quot;//body//img/@src&quot;).extract()[0]
                item[&#39;imgSrc&#39;] = src
                items.append(item)
                return items
                #self.download(src)
        def download(self,url):
                self.number += 1
                savePath = &#39;/mnt/python_image/%d.jpg&#39; % (self.number)
                print &#39;正在下载...&#39;,url
                try:
                        u = urllib2.urlopen(url)
                        r = u.read()
                        downloadFile = open(savePath,&#39;wb&#39;)
                        downloadFile.write(r)
                        u.close()
                        downloadFile.close()
                except:
                        print savePath,&#39;can not download.&#39;
</code></pre>

<p> <br/>
 <br/>
 <br/>
接下来看看我的Item.py的代码：</p>

<pre><code class="language-python">class wallpaperItem(scrapy.Item):
        size = scrapy.Field()
        altInfo = scrapy.Field()
        imgSrc = scrapy.Field()
</code></pre>

<p> <br/>
还有pipilines.py:</p>

<pre><code class="language-python"># -*- coding: utf-8 -*-
 
# Define your item pipelines here
#
# Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting
# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html
import MySQLdb
 
 
class MylonlyPipeline(object):
    def process_item(self, item, spider):
        return item
 
class wallpaperPipeline(object):
        def process_item(self,item,spider):
                print &#39;imgSrc:&#39;,item[&#39;imgSrc&#39;]
                db = MySQLdb.connect(&quot;rdsauvva2auvva2.mysql.rds.aliyuncs.com&quot;,&quot;mylonly&quot;,&quot;703003659txg&quot;,&quot;wallpaper&quot;)
                cursor = db.cursor()
                db.set_character_set(&#39;utf8&#39;)
                cursor.execute(&#39;SET NAMES utf8;&#39;)
                cursor.execute(&#39;SET CHARACTER SET utf8;&#39;)
                cursor.execute(&#39;SET character_set_connection=utf8;&#39;)
 
                sql =&quot;INSERT INTO mobile_download(wallpaper_size,wallpaper_info,wallpaper_src)\
                      VALUES(&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;)&quot;%(item[&#39;size&#39;],item[&#39;altInfo&#39;],item[&#39;imgSrc&#39;])
                try:
                        print sql
                        cursor.execute(sql)
                        db.commit()
                except MySQLdb.Error,e:
                        print &quot;Mysql Error %d: %s&quot; % (e.args[0], e.args[1])
                db.close()
                return item
</code></pre>

<p> <br/>
 <br/>
 <br/>
本例中我将传回的items数据存放到了数据库中，如果你不想这样，可以将我注释掉的self.download()取消注释，不反悔items，就可以将找到的所有图片链接全部下载下来，不过要找个大点的地方存储，因为总共有5W多条：<br/>
<img src="https://pic.mylonly.com/2016-06-29_061712481319744.png" alt="2016-06-29_061712481319744.png"/></p>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2016-05-11T19:10:15+08:00" itemprop="datePublished">2016/5/11</time>
			</div>
			<div class="tags">posted in 
			
			    <a class='category' href='linux.html'>Linux</a>&nbsp;
			 
			</div>
		</div>
		<h1 class="title" itemprop="name"><a href="14945010159790.html" itemprop="url">
		CentOS中添加Swap交换文件</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<p>1.先检查是否存在Swap文件</p>

<pre><code class="language-Bash">swapon -s #返回的信息为空，说明没有其他交换文件
</code></pre>

<p>2.创建一个Swap文件，使用dd命令</p>

<pre><code class="language-Bash">##of后面是swap文件的路径，可以自定义，count是swap文件的大小，推荐为内存的2倍
dd if=/dev/zero of=/home/swap bs=1024 count=1024000
</code></pre>

<p>3.将swap文件转换成swap格式</p>

<pre><code class="language-Bash">mkswap /home/swap
</code></pre>

<p>4.将swap文件挂载成swap分区</p>

<pre><code class="language-Bash">swapon /home/swap #创建完成后可以用`free -m`看看是否成功
</code></pre>

<p>5.让swap文件自动挂载</p>

<pre><code class="language-Bash">vim /etc/fstab 
#在文件末尾加上下面的命令
/home/swap swap swap default 0 0 
</code></pre>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2016-05-11T18:12:56+08:00" itemprop="datePublished">2016/5/11</time>
			</div>
			<div class="tags">posted in 
			
			    <a class='category' href='ios.html'>iOS</a>&nbsp;
			 
			</div>
		</div>
		<h1 class="title" itemprop="name"><a href="14944975766543.html" itemprop="url">
		CocoaPods master仓库替换为国内源</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<p><img src="https://pic.mylonly.com/2016-06-29_1396084357114.jpg" alt="2016-06-29_1396084357114.jpg"/><br/>
国内用CocoaPods 实在是太蛋疼了，一个pod update都要等好久，之前唐巧博客里面推荐的那个国内源已经不可用了，还好今天在V2EX上看到有人提供了别的CocoaPods源。</p>

<ol>
<li><a href="https://git.coding.net/hging/Specs.git">https://git.coding.net/hging/Specs.git</a></li>
<li><a href="http://git.oschina.net/akuandev/Specs">http://git.oschina.net/akuandev/Specs</a></li>
</ol>

<p>执行如下命令替换Pods源</p>

<pre><code class="language-Bash">$ pod repo remove master
$ pod repo add master &#39;http://git.oschina.net/akuandev/Specs.git&#39; 
</code></pre>

<p> <br/>
仅仅这样使用pod update时发现仍然会从一个master-1这个官方源中clone</p>

<pre><code class="language-Bash">tianxianggendeMacBook-Air:upyun-batch-upload mylonly$ pod install
Creating shallow clone of spec repo `master-1` from `https://github.com/CocoaPods/Specs.git`
</code></pre>

<p> <br/>
这时候需要在Podfile中加入source命令，就可以直接从国内源更新了。</p>

<pre><code class="language- Bash">source &#39;http://git.oschina.net/akuandev/Specs.git&#39;
platform :ios, &#39;6.0&#39;
pod &#39;AFNetworking&#39;, &#39;~&gt; 1.3.4&#39;
</code></pre>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2015-06-01T08:50:40+08:00" itemprop="datePublished">2015/6/1</time>
			</div>
			<div class="tags">posted in 
			
			    <a class='category' href='ios.html'>iOS</a>&nbsp;
			 
			</div>
		</div>
		<h1 class="title" itemprop="name"><a href="15278142400055.html" itemprop="url">
		Objective-C语言特性——强引用、弱引用（Strong、Weak）</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<p>编写Objective-C代码的很大一部分工作是管理可执行代码保持的对象引用，还有被引用对象保持的对另外一个对象的引用。在ARC出现之前，Objective-C在IOS中使用手动内存管理，在OS X中还可使用垃圾回收机制。这两个方法都有他们各自的优缺点。现在Objective-C使用ARC，其结合了这两种方法最好的一面。<br/>
ARC，中文简称自动引用计数，是一种编译器机制，在编译期间编译器将手动内存管理的调用插入到代码中。这段内存管理代码通过判断一旦对象没有任何进来的强引用，它将被释放，如果它是最后一个持有另一些对象强引用的对象，则这些对象也会被释放，对它们的其他对象的处理以此类推,编译器非常智能，ARC代码被优化的很好。尽管ARC将开发者从编写手动内存管理代码的工作中解放出来，但如同使用垃圾回收的平台，它仍然不能打破引用循环。因为这个原因，开发者需要给编译器某些指导以避免引用循环。下图展示了一个引用循环：</p>

<p><img src="https://pic.mylonly.com/2018-06-01-BA81AC12-9449-492E-8798-A04AB3FB2E31.jpg" alt=""/></p>

<p>只有对象C释放了对象A后其才能被释放，而只要对象B有一个对它的强引用那么对象C就永远不会被释放。最后，除非对象A被释放否则对象B永远不会被释放。这就是一个引用循环，将导致引用程序发生内存泄漏。<br/>
但是，如果我们在上图中将对象C持有的对象A的引用（在途中用实线表示）被替换成一个弱引用（用虚线表示），如下图</p>

<p><img src="media/15278142400055/633E3597-FBF7-4111-A7B2-77124042975D.jpg" alt="633E3597-FBF7-4111-A7B2-77124042975D"/></p>

<p>弱引用不阻止对象被释放，所以对象A被释放，接着是对象B，最后是对象C。<br/>
如果熟悉C或C++，你可能会注意到一个使用弱引用的问题。如果对象可在任何时候被释放，并且你有一个该对象的弱引用，那么最终会得到一个悬空指针（这个指针的地址指向的地址仍然存在，但是对象已经被释放）。在ARC中，这个现象不会存在，一旦对象被释放则指向这些对象的弱引用将会变为nil。<br/>
Objective-C中，默认情况下，没有使用任何关键字的声明的实例变量和局部变量都是强引用，将变量声明为弱引用需使用weak关键字。<br/>
如何在写代码的时候判断变量是该用强引用还是弱引用？</p>

<p>一个常见的方法就是将强引用和弱引用分别想象为“拥有”和“被拥有”。如果一个对象完全属于包含它的对象，那么它使用强引用。如果一个对象被两个以上的对象所拥有，并不完全属于一个对象，则使用弱引用，尽管该方法不是对于所有引用逗适用，但它确实是一个记住Objective-C内存管理约定的好技巧。</p>


			
			
		</div>

	</article>
  

</div>
<nav id="pagenavi">
	 <a class="prev" href="all_6.html">Prev</a>  
	 <a class="next" href="all_8.html">Next</a> 
	<div class="center"><a href="archives.html">Blog Archives</a></div>

</nav>

</div>



        </div>
			<footer id="footer" class="inner">Copyright &copy; 2014
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; 
Theme by <a href="http://shashankmehta.in/archive/2012/greyshade.html">Shashank Mehta</a>
      </footer>
		</div>
	</div>

  
    



</body>
</html>