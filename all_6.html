<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
	<title>  
	  
  	独自一人
  	
	</title>

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

	<link href="atom.xml" rel="alternate" title="独自一人" type="application/atom+xml">

	<link href="asset/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<link href="asset/stylesheets/font-awesome.min.css" media="screen, projection" rel="stylesheet" type="text/css">
	<script src="asset/javascripts/jquery.min.js"></script>
	<script src="asset/highlightjs/highlight.pack.js"></script>
	<link href="asset/highlightjs/styles/solarized_dark.css" media="screen, projection" rel="stylesheet" type="text/css">
<script>hljs.initHighlightingOnLoad();</script>

	<!--[if lt IE 9]><script src="asset/javascripts/html5.js"></script><![endif]-->
	<!-- <link href='http://fonts.googleapis.com/css?family=Nunito:400,300,700' rel='stylesheet' type='text/css'> -->
	<style type="text/css">
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 300;
  src: local('Nunito-Light'), url(asset/font/1TiHc9yag0wq3lDO9cw0voX0hVgzZQUfRDuZrPvH3D8.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 400;
  src: local('Nunito-Regular'), url(asset/font/6TbRXKWJjpj6V2v_WyRbMX-_kf6ByYO6CLYdB4HQE-Y.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 700;
  src: local('Nunito-Bold'), url(asset/font/TttUCfJ272GBgSKaOaD7KoX0hVgzZQUfRDuZrPvH3D8.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
	</style>
	
	<style type="text/css">
	.container .left-col{ opacity: 1;}
	#pagenavi a{ font-size: 1.3em;}
	#pagenavi .next:before{ top: 3px;}
	#pagenavi .prev:before{ top: 3px;}
	.container .mid-col .mid-col-container #content .archives .title{ font-size: 1.5em;}
	.container .mid-col .mid-col-container #content article{ padding: 15px 0px;}
	#header .subtitle {
		line-height: 1.2em;
		padding-top: 8px;
	}
	article pre{ background: none; border: none; padding: 0;}
	article .entry-content{text-align: left;}
	.share-comment{ padding: 25px 0px; clear: both;}
	hr{ margin: 20px 0px;border: 0; border-top:solid 1px #ddd;}
	</style>
  

</head>


<body>
	<div class="container">
		<div class="left-col">
			<div class="intrude-less">
				<header id="header" class="inner">
				 
				 	<div class="profilepic">
						<img src="https://pic.mylonly.com/2017-05-11-IMG_1164.JPG" style="width:160px;">
					</div>
            	
					
					<h1><a href="index.html">独自一人</a></h1>
					<p class="subtitle">独自一人,独自Coding...</p>
					<nav id="main-nav">
						<ul class="main">
						
						  <li id=""><a target="self" href="index.html">Home</a></li>
						
						  <li id=""><a target="_self" href="archives.html">Archives</a></li>
						
						</ul>
					</nav>

					<nav id="sub-nav">
						<div class="social">










<a target="_blank" class="github" target="_blank" href="https://github.com/mylonly" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:root@mylonly.com" title="Email">Email</a>

								

								<a class="rss" href="atom.xml" title="RSS">RSS</a>
							
						</div>
					</nav>
				</header>				
			</div>
		</div>	
		<div class="mid-col">
			<div class="mid-col-container"> <div id="content" class="inner">
<div itemscope itemtype="http://schema.org/Blog">


	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2016-09-11T19:10:48+08:00" itemprop="datePublished">2016/9/11</time>
			</div>
			<div class="tags">posted in 
			
			    <a class='category' href='linux.html'>Linux</a>&nbsp;
			 
			</div>
		</div>
		<h1 class="title" itemprop="name"><a href="14945010480752.html" itemprop="url">
		各种程序的源码安装</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<h2 id="toc_0">nginx</h2>

<pre><code class="language-text">#安装编译nginx必须的依赖
yum install gcc-c++
yum install pcre pcre-devel  
yum install zlib zlib-devel 
yum install openssl openssl--devel

#下载nginx源码
wget http://nginx.org/download/nginx-1.9.15.tar.gz

#解压
tar -zvxf nginx-1.9.15.tar.gz -C ../document/

#编译安装
cd ../document/nginx-1.9.15
./configure
make
make install
</code></pre>

<h2 id="toc_1">setuptools 安装</h2>

<pre><code class="language-text">wget https://bootstrap.pypa.io/ez_setup.py -O - | python

</code></pre>

<h2 id="toc_2">pip源码安装</h2>

<pre><code class="language-text">[pip官网安装](https://pypi.python.org/pypi/pip)
</code></pre>

<h2 id="toc_3">源码安装Git</h2>

<ol>
<li><p>安装依赖</p>
<pre><code class="language-text"> yum install curl
 yum install curl-devel<br/>
 yum install zlib-devel<br/>
 yum install openssl-devel<br/>
 yum install perl<br/>
 yum install perl-devel<br/>
 yum install cpio<br/>
 yum install expat-devel<br/>
 yum install gettext-devel
</code></pre></li>
<li><p>下载源码编译安装</p>
<pre><code class="language-text">wget https://www.kernel.org/pub/software/scm/git/git-2.8.2.tar.gz
tar xzvf git-2.8.2.tar.gz<br/>
cd git-2.8.2<br/>
autoconf<br/>
./configure<br/>
make | make install
</code></pre></li>
<li><p>配置环境变量</p>
<p>修改用户目录下的.bashrc或者bash_profile,末尾增加</p>
<pre><code class="language-text">export PATH=$PATH:/usr/local/bin
</code></pre>
<p>然后重新加载profile</p>
<pre><code class="language-text">source ~/.bashrc
</code></pre></li>
</ol>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2016-09-11T18:16:48+08:00" itemprop="datePublished">2016/9/11</time>
			</div>
			<div class="tags">posted in 
			
			    <a class='category' href='linux.html'>Linux</a>&nbsp;
			 
			</div>
		</div>
		<h1 class="title" itemprop="name"><a href="14944978089747.html" itemprop="url">
		Linux常用命令</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<ol>
<li>磁盘挂载</li>
</ol>

<pre><code class="language-text">fdisk -l #查看磁盘信息
mount /dev/xvdb1 /mnt #挂载磁盘
</code></pre>

<ol>
<li>设置开机自动加载磁盘</li>
</ol>

<pre><code class="language-text">vim /etc/fstab
写入 /dev/xvdb1 /mnt ext4 default 1 1
</code></pre>

<ol>
<li>修改主机名</li>
</ol>

<pre><code class="language-text">vim /etc/sysconfig/network #修改里面的HOSTNAME值
</code></pre>

<ol>
<li>设置ssh自动认证</li>
</ol>

<pre><code class="language-text">ssh-keygen -t rsa #在客户机生成秘钥，
scp ~/.ssh/id_rsa.pub root@xxx.com:/home/xxx/ #将客户端生成的公钥文件发送到服务器上
#将id_rsa.pub文件写入服务器的.ssh/authorized_keys中，
最好用cat命令写入,手动创建authorized_keys文件会出现各种各样的权限认证问题
cat id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys
</code></pre>

<ol>
<li>dd命令实现ISO文件块拷贝</li>
</ol>

<pre><code class="language-text">dd if=kali-linux-2018.2-amd64.iso of=/dev/sdb bs=1M
</code></pre>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2016-07-13T15:39:33+08:00" itemprop="datePublished">2016/7/13</time>
			</div>
			<div class="tags">posted in 
			
			    <a class='category' href='linux.html'>Linux</a>&nbsp;
			 
			</div>
		</div>
		<h1 class="title" itemprop="name"><a href="14945010337427.html" itemprop="url">
		利用树莓派搭建迅雷远程下载服务器</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<blockquote>
<p>树莓派系统为Raspberry Pi</p>
</blockquote>

<ol>
<li><p>树莓派安装迅雷路由器固件<br/>
可以在迅雷论坛上下载到最新的固件:<a href="http://luyou.xunlei.com/">官网论坛</a>,记得要下arm版本的固件<code>armel_v5te_glibc</code><br/>
或者下载我共享的这个<a href="http://pan.baidu.com/s/1nvvoX7b">Xware1.0.31_armel_v5te_glibc</a> 提取码:e223<br/>
在树莓派上解压到某个目录，例如我的是<code>/root/xunlei</code>下面</p>
<pre><code class="language-shell">unzip Xware1.0.31_armel_v5te_glibc.zip -d /root/xunlei
</code></pre>
<p>进入<code>/root/xunlei</code>目录 执行:</p>
<pre><code class="language-shell">./portal
</code></pre>
<p>稍等片刻，会在最后输出一个激活码，类似下图中涂红的部分</p>
<p><img src="https://pic.mylonly.com/2016-07-13_15:19:59.jpg" alt="2016-07-13_15:19:59.jpg"/> </p></li>
<li><p>在迅雷远程下载页面绑定树莓派<br/>
登录<a href="http://yuancheng.xunlei.com/">迅雷远程下载主页</a>,登录之后，左侧会有一个添加按钮，点击添加按钮，弹出如下界面:<br/>
<img src="https://pic.mylonly.com/2016-07-13_15:22:45.jpg" alt="2016-07-13_15:22:45.jpg"/></p>
<p>将树莓派上获得的激活码填入框中，点击<code>绑定</code>后左侧就会出现树莓派对应的设备列表了，但是，如果我们此时就在右侧点击<code>新建</code>之后会发现,弹出的新建页面中会提示找不到挂载磁盘，如下图：</p>
<p><img src="http://pic.mylonly.com/2016-07-13_15:25:18.jpg" alt="2016-07-13_15:25:18.jpg"/></p></li>
<li><p>自定义迅雷的下载目录</p>
<p>进入<code>/mnt</code>目录，创建目录TDDOWNLOAD(名字随意)<br/>
执行命令:</p>
<pre><code class="language-shell">mount --bind /data/TDDOWNLOAD /mnt/TDDOWNLOAD
</code></pre>
<p>其中/data/TDDOWNLOAD就是自定义的下载目录，你可以指定为其他任何目录。</p>
<p>然后再刚刚迅雷固件的解压目录下创建目录<code>etc</code>,同时创建文件<code>thunder_mounts.cfg</code>,编辑此文件</p>
<pre><code class="language-shell">vim /root/xunlei/etc/thunder_mounts.cfg
</code></pre>
<p>内容为:</p>
<pre><code class="language-shell">avaliable_mount_path_pattern
{<br/>
    /mnt/TDDOWNLOAD<br/>
}
</code></pre>
<p>保存后重启迅雷路由器固件:</p>
<pre><code class="language-shell">./root/xunlei/portal
</code></pre>
<p>再进入远程下载界面新建下载就没有了没挂载磁盘的提示了</p></li>
<li><p>迅雷路由器固件开机启动</p>
<p>在/etc/init.d/下新建xunlei脚本，写入:</p>
<pre><code class="language-text">```Shell
#!/bin/sh<br/>
#<br/>
# Xunlei initscript<br/>
#<br/>
### BEGIN INIT INFO<br/>
# Provides:          xunlei<br/>
# Required-Start:    $network $local_fs $remote_fs<br/>
# Required-Stop::    $network $local_fs $remote_fs<br/>
# Should-Start:      $all<br/>
# Should-Stop:       $all<br/>
# Default-Start:     2 3 4 5<br/>
# Default-Stop:      0 1 6<br/>
# Short-Description: Start xunlei at boot time<br/>
# Description:       A downloader<br/>
### END INIT INFO<br/>
do_start()<br/>
{<br/>
        ./root/xunlei/portal<br/>
}<br/>
do_stop()<br/>
{<br/>
        ./root/xunlei/portal -s<br/>
}<br/>
case &quot;$1&quot; in<br/>
  start)<br/>
    do_start<br/>
    ;;<br/>
  stop)<br/>
    do_stop<br/>
    ;;<br/>
esac<br/>
``` 
</code></pre>
<p>然后将该脚本加入默认自启动中</p>
<pre><code class="language-shell">update-rc.d xunlei defaults
</code></pre></li>
</ol>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2016-07-04T19:12:04+08:00" itemprop="datePublished">2016/7/4</time>
			</div>
			<div class="tags">posted in 
			
			    <a class='category' href='python.html'>Python</a>&nbsp;
			 
			</div>
		</div>
		<h1 class="title" itemprop="name"><a href="14945011244738.html" itemprop="url">
		Scrapy抓取Ajax动态页面</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<blockquote>
<p>一般来说爬虫类框架抓取Ajax动态页面都是通过一些第三方的webkit库去手动执行html页面中的js代码， 最后将生产的html代码交给spider分析。本篇文章则是通过浏览器提供的Debug工具分析Ajax页面的具体请求内容，找到获取数据的接口url，直接调用该接口获取数据，省去了引入python-webkit库的麻烦，而且由于一般ajax请求的数据都是结构化数据，这样更省去了我们利用xpath解析html的痛苦。</p>
</blockquote>

<p>这次我们要抓取的网站是<a href="https://mm.taobao.com">淘女郎</a>的页面,全站都是通过Ajax获取数据然后重新渲染生产的。</p>

<p>这篇文章的代码已上传至我的<a href="https://github.com/mylonly/Spiders">Github</a>,由于后面有部分内容并没有提供完整代码，所以贴上地址供各位参考。</p>

<h3 id="toc_0">分析工作</h3>

<p>用Chrome打开淘女郎的首页中的<a href="https://mm.taobao.com/search_tstar_model.htm">美人库</a>，这个页面毫无疑问是会展示所有的模特的信息，同时打开Debug工具，在network选项中查看浏览器发送了哪些请求？</p>

<p><img src="https://pic.mylonly.com/2016-07-04_16:11:01.jpg" alt="2016-07-04_16:11:01.jpg"/></p>

<p>在截图的左下角可以看到总共产生了86个请求，那么有什么办法可以快速定位到Ajax请求的链接了，利用Network当中提供的Filter功能，选中Filter，最后选择右边的XHR过滤(XHR时XMLHttpRequest对象，一般Ajax请求的数据都是结构化数据)，这样就剩下了为数不多的几个请求，剩下的就靠我们自己一个一个的检查吧</p>

<p><img src="https://pic.mylonly.com/2016-07-04_16:22:18.jpg" alt="2016-07-04_16:22:18.jpg"/></p>

<p>很幸运，通过分析每个接口返回的request和response信息，发现最后一个请求就是我们需要的接口url</p>

<p><img src="https://pic.mylonly.com/2016-07-04_16:25:56.jpg" alt="2016-07-04_16:25:56.jpg"/></p>

<p>Request中得参数很简单,根据英文意思就可以猜出意义,由于我们要抓取所有模特的信息，所以不需要定制这些参数，后面直接将这些参数post给接口就行了</p>

<p><img src="https://pic.mylonly.com/2016-07-04_16:29:06.jpg" alt="2016-07-04_16:29:06.jpg"/></p>

<p>在Response中可以获得到的有用数据有两个:所有模特信息的列表<code>searchDOList</code>、以及总页数<code>totolPage</code></p>

<p><img src="https://pic.mylonly.com/2016-07-04_16:35:05.jpg" alt="2016-07-04_16:35:05.jpg"/></p>

<p>searchDOList列表中得对象都有如上图所示的json格式，它也正是我们需要的模特信息的数据</p>

<h3 id="toc_1">Scrapy编码</h3>

<ol>
<li><p>定义Item</p>
<pre><code class="language-python">class tbModelItem(scrapy.Item):
    avatarUrl = scrapy.Field()<br/>
    cardUrl = scrapy.Field()<br/>
    city = scrapy.Field()<br/>
    height = scrapy.Field()<br/>
    identityUrl = scrapy.Field()<br/>
    modelUrl = scrapy.Field()<br/>
    realName = scrapy.Field()<br/>
    totalFanNum = scrapy.Field()<br/>
    totalFavorNum = scrapy.Field()<br/>
    userId = scrapy.Field()<br/>
    viewFlag = scrapy.Field()<br/>
    weight = scrapy.Field()
</code></pre>
<p>根据上面的分析得到的json格式，我们可以很轻松的定义出item</p></li>
<li><p>Spider编写</p>
<pre><code class="language-python">import urllib2
import os<br/>
import re<br/>
import codecs<br/>
import json<br/>
import sys<br/>
from scrapy import Spider<br/>
from scrapy.selector import Selector<br/>
from MySpider.items import tbModelItem,tbThumbItem<br/>
from scrapy.http import Request<br/>
from scrapy.http import FormRequest<br/>
from scrapy.utils.response import open_in_browser<br/>
reload(sys)<br/>
sys.setdefaultencoding(&#39;utf8&#39;)<br/>
class tbmmSpider(Spider):<br/>
    name = &quot;tbmm&quot;<br/>
    allow_domians = [&quot;mm.taobao.com&quot;]<br/>
    custom_settings = {<br/>
        &quot;DEFAULT_REQUEST_HEADERS&quot;:{<br/>
            &#39;authority&#39;:&#39;mm.taobao.com&#39;,<br/>
            &#39;accept&#39;:&#39;application/json, text/javascript, */*; q=0.01&#39;,<br/>
            &#39;accept-encoding&#39;:&#39;gzip, deflate&#39;,<br/>
            &#39;accept-language&#39;:&#39;zh-CN,zh;q=0.8,en;q=0.6,zh-TW;q=0.4&#39;,<br/>
            &#39;origin&#39;:&#39;https://mm.taobao.com&#39;,<br/>
            &#39;referer&#39;:&#39;https://mm.taobao.com/search_tstar_model.htm?spm=719.1001036.1998606017.2.KDdsmP&#39;,<br/>
            &#39;user-agent&#39;:&#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.97 Safari/537.36&#39;,<br/>
            &#39;x-requested-with&#39;:&#39;XMLHttpRequest&#39;,<br/>
            &#39;cookie&#39;:&#39;cna=/oN/DGwUYmYCATFN+mKOnP/h; tracknick=adimtxg; _cc_=Vq8l%2BKCLiw%3D%3D; tg=0; thw=cn; v=0; cookie2=1b2b42f305311a91800c25231d60f65b; t=1d8c593caba8306c5833e5c8c2815f29; _tb_token_=7e6377338dee7; CNZZDATA30064598=cnzz_eid%3D1220334357-1464871305-https%253A%252F%252Fmm.taobao.com%252F%26ntime%3D1464871305; CNZZDATA30063600=cnzz_eid%3D1139262023-1464874171-https%253A%252F%252Fmm.taobao.com%252F%26ntime%3D1464874171; JSESSIONID=8D5A3266F7A73C643C652F9F2DE1CED8; uc1=cookie14=UoWxNejwFlzlcw%3D%3D; l=Ahoatr-5ycJM6M9x2/4hzZdp6so-pZzm; mt=ci%3D-1_0&#39;<br/>
        },<br/>
        &quot;ITEM_PIPELINES&quot;:{<br/>
            &#39;MySpider.pipelines.tbModelPipeline&#39;: 300<br/>
        }<br/>
    } <br/>
    def start_requests(self):<br/>
        url = &quot;https://mm.taobao.com/tstar/search/tstar_model.do?_input_charset=utf-8&quot;<br/>
        requests = []<br/>
        for i in range(1,60):<br/>
            formdata = {&quot;q&quot;:&quot;&quot;,<br/>
                        &quot;viewFlag&quot;:&quot;A&quot;,<br/>
                        &quot;sortType&quot;:&quot;default&quot;,<br/>
                        &quot;searchStyle&quot;:&quot;&quot;,<br/>
                        &quot;searchRegion&quot;:&quot;city:&quot;,<br/>
                        &quot;searchFansNum&quot;:&quot;&quot;,<br/>
                        &quot;currentPage&quot;:str(i),<br/>
                        &quot;pageSize&quot;:&quot;100&quot;}<br/>
            request = FormRequest(url,callback=self.parse_model,formdata=formdata)<br/>
            requests.append(request)<br/>
        return requests<br/>
    def parse_model(self,response):<br/>
        jsonBody = json.loads(response.body.decode(&#39;gbk&#39;).encode(&#39;utf-8&#39;))<br/>
        models = jsonBody[&#39;data&#39;][&#39;searchDOList&#39;]<br/>
        modelItems = []<br/>
        for dict in models:<br/>
            modelItem = tbModelItem()<br/>
            modelItem[&#39;avatarUrl&#39;] = dict[&#39;avatarUrl&#39;]<br/>
            modelItem[&#39;cardUrl&#39;] = dict[&#39;cardUrl&#39;]<br/>
            modelItem[&#39;city&#39;] = dict[&#39;city&#39;]<br/>
            modelItem[&#39;height&#39;] = dict[&#39;height&#39;]<br/>
            modelItem[&#39;identityUrl&#39;] = dict[&#39;identityUrl&#39;]<br/>
            modelItem[&#39;modelUrl&#39;] = dict[&#39;modelUrl&#39;]<br/>
            modelItem[&#39;realName&#39;] = dict[&#39;realName&#39;]<br/>
            modelItem[&#39;totalFanNum&#39;] = dict[&#39;totalFanNum&#39;]<br/>
            modelItem[&#39;totalFavorNum&#39;] = dict[&#39;totalFavorNum&#39;]<br/>
            modelItem[&#39;userId&#39;] = dict[&#39;userId&#39;]<br/>
            modelItem[&#39;viewFlag&#39;] = dict[&#39;viewFlag&#39;]<br/>
            modelItem[&#39;weight&#39;] = dict[&#39;weight&#39;]<br/>
            modelItems.append(modelItem)<br/>
        return modelItems  
</code></pre>
<p>代码不长，一点一点来分析:</p>
<ol>
<li>由于分析这个页面并不需要递归遍历网页，所以就不要crawlSpider了，只继承最简单的spider</li>
<li>custome_setting可用于自定义每个spider的设置，而setting.py中的都是全局属性的，当你的scrapy工程里有多个spider的时候这个custom_setting就显得很有用了</li>
<li>ITEM_PIPELINES，自定义管道模块，当item获取到数据后会调用你指定的管道处理命令，这个后面会贴上代码，因为这个不影响本文的内容，数据的处理可以因人而异。</li>
<li>依然重写start_request,带上必要的参数请求我们分析得到的借口url，这里我省了一个懒，只遍历了前60页的数据，各位当然可以先调用1次借口确定总的页数(totalPage)之后再写这个for循环。</li>
<li>parse函数里利用json库解析了返回来得数据，赋值给item的相应字段</li>
</ol></li>
<li><p>数据后续处理</p>
<p>数据处理也就是我上面配置ITEM_PIPELINES的目的，这里，我将获取到的item数据存储到了本地的mysql数据中，各位也可以通过FEED_URL参数直接输出json格式文本文件</p>
<pre><code class="language-python">import MySQLdb
class tbModelPipeline(object):<br/>
    def process_item(self,item,spider):<br/>
        db = MySQLdb.connect(&quot;localhost&quot;,&quot;用户名&quot;,&quot;密码&quot;,&quot;spider&quot;)<br/>
        cursor = db.cursor()<br/>
        db.set_character_set(&#39;utf8&#39;)<br/>
        cursor.execute(&#39;SET NAMES utf8;&#39;)<br/>
        cursor.execute(&#39;SET CHARACTER SET utf8;&#39;)<br/>
        cursor.execute(&#39;SET character_set_connection=utf8;&#39;)<br/>
        sql =&quot;INSERT INTO tb_model(user_id,avatar_url,card_url,city,height,identity_url,model_url,real_name,total_fan_num,total_favor_num,view_flag,weight)\<br/>
                      VALUES(&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;)&quot;%(item[&#39;userId&#39;],item[&#39;avatarUrl&#39;],item[&#39;cardUrl&#39;],item[&#39;city&#39;],item[&#39;height&#39;],item[&#39;identityUrl&#39;],\<br/>
                      item[&#39;modelUrl&#39;],item[&#39;realName&#39;],item[&#39;totalFanNum&#39;],item[&#39;totalFavorNum&#39;],item[&#39;viewFlag&#39;],item[&#39;weight&#39;])<br/>
        try:<br/>
                print sql<br/>
                cursor.execute(sql)<br/>
                db.commit()<br/>
        except MySQLdb.Error,e:<br/>
                print &quot;Mysql Error %d: %s&quot; % (e.args[0], e.args[1])<br/>
        db.close()<br/>
        return item
</code></pre></li>
</ol>

<h3 id="toc_2">更重要的内容</h3>

<p>获取所有的淘女郎的基本信息并不是<a href="https://mm.taobao.com">淘女郎</a>这个网站的全部内容，还有一些更有意思的数据,比如:</p>

<p>点击进入模特的页面之后发现左侧会有有个相册选项卡，点击后右边出现了各种相册，而每个相册里面都是各种各样的模特照片</p>

<p><img src="https://pic.mylonly.com/2016-07-04_17:04:22.jpg" alt="2016-07-04_17:04:22.jpg"/></p>

<p><img src="https://pic.mylonly.com/2016-07-04_17:04:49.jpg" alt="2016-07-04_17:04:49.jpg"/></p>

<p>通过network的分析，这些页面的数据通通都是Ajax请求获得的，具体的接口如下:</p>

<p><img src="https://pic.mylonly.com/2016-07-04_17:09:51.jpg" alt="2016-07-04_17:09:51.jpg"/></p>

<p><img src="https://pic.mylonly.com/2016-07-04_17:10:16.jpg" alt="2016-07-04_17:10:16.jpg"/></p>

<ol>
<li><p>获取相册列表的接口是一个GET请求，其中只有一个很重要的user_id，而这个user_id在上面拿去模特的基本信息已经拿到了，还有个page参数用于标识获取的是第几页数据(由于这个是第一页，并没有在url中显现出来，可以通过返回的html中包含的totalPage元素获得)不过这个接口的返回就不是标准的json格式了，而是一段html，这时候又到了利用scrapy中提供的强大的xpath功能了</p>
<pre><code class="language-python">def parse_album(self,response):
   sel = Selector(response)<br/>
   tbThumbItems = []<br/>
   thumb_url_list = sel.xpath(&quot;//div[@class=&#39;mm-photo-cell-middle&#39;]//h4//a/@href&quot;).extract()       <br/>
   thumb_name_list = sel.xpath(&quot;//div[@class=&#39;mm-photo-cell-middle&#39;]//h4//a/text()&quot;).extract()<br/>
   user_id = response.meta[&#39;user_id&#39;]<br/>
   for i in range(0,len(thumb_url_list)-1):<br/>
       thumbItem = tbThumbItem()<br/>
       thumbItem[&#39;thumb_name&#39;] = thumb_name_list[i].replace(&#39;\r\n&#39;,&#39;&#39;).replace(&#39; &#39;,&#39;&#39;)<br/>
       thumbItem[&#39;thumb_url&#39;] = thumb_url_list[i]<br/>
       thumbItem[&#39;thumb_userId&#39;] = str(user_id)<br/>
       temp = self.urldecode(thumbItem[&#39;thumb_url&#39;])<br/>
       thumbItem[&#39;thumb_id&#39;] = temp[&#39;album_id&#39;][0]<br/>
       tbThumbItems.append(thumbItem)<br/>
   return tbThumbItems
</code></pre></li>
<li><p>获取相册里照片的接口就是一个完全的json格式的接口了,其中参数包括我们已经拿到的user_id以及album_id，page的最大范围totalPage依然可以通过第一次返回的response中的totalPage字段获得</p>
<p><img src="https://pic.mylonly.com/2016-07-04_17:25:23.jpg" alt="2016-07-04_17:25:23.jpg"/></p>
<p><img src="https://pic.mylonly.com/2016-07-04_17:25:46.jpg" alt="2016-07-04_17:25:46.jpg"/></p></li>
</ol>

<h3 id="toc_3">总结</h3>

<ol>
<li>这种通过分析Ajax接口直接调用获取原始数据应该是效率最高的抓取数据方式，但并不是所有的Ajax页面都适用，还是要具体对待，比如我们上面获取相册列表当中就要去分析html来获得相册的基本信息。</li>
<li>获取相册和相册里的照片列表写的比较简略，基本没展示什么代码，这样写是有原因的:一个是因为我已经挂了代码的链接,而且后面这两部分的原理和我主要讲的第一部分获取模特信息的原理基本类似，不想花太多的篇幅花在这种重复的内容上，另外一个我希望想掌握Scrapy的同学能在明白我第一部分的讲解下自己能顺利完成后面的工作，遇到不明白的时候可以看看我Github上的源码，看看有什么不对的地方，只有自己写一遍才能掌握，这是编程界的硬道理。</li>
</ol>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2016-07-02T19:09:49+08:00" itemprop="datePublished">2016/7/2</time>
			</div>
			<div class="tags">posted in 
			
			    <a class='category' href='linux.html'>Linux</a>&nbsp;
			 
			</div>
		</div>
		<h1 class="title" itemprop="name"><a href="14945009892942.html" itemprop="url">
		利用Github的Webhook功能和Node.js完成项目的自动部署</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<p><u>本文对任何提供Webhook的git仓库都适用</u></p>

<p><img src="https://pic.mylonly.com/2016-06-29_14635623250348.jpg" alt="2016-06-29_14635623250348.jpg"/></p>

<h3 id="toc_0">首先完成Node.js服务器的代码构建，先上代码，再解释</h3>

<pre><code class="language-node.js">var http = require(&#39;http&#39;)
var createHandler = require(&#39;github-webhook-handler&#39;)
var handler = createHandler({ path: &#39;/&#39;, secret: &#39;root&#39; })
// 上面的 secret 保持和 GitHub 后台设置的一致

function run_cmd(cmd, args, callback) {
  var spawn = require(&#39;child_process&#39;).spawn;
  var child = spawn(cmd, args);
  var resp = &quot;&quot;;

  child.stdout.on(&#39;data&#39;, function(buffer) { resp += buffer.toString(); });
  child.stdout.on(&#39;end&#39;, function() { callback (resp) });
}

http.createServer(function (req, res) {
  handler(req, res, function (err) {
    res.statusCode = 404
    res.end(&#39;no such location&#39;)
  })
}).listen(7777)

handler.on(&#39;error&#39;, function (err) {
  console.error(&#39;Error:&#39;, err.message)
})

handler.on(&#39;push&#39;, function (event) {
  console.log(&#39;Received a push event for %s to %s&#39;,
    event.payload.repository.name,
    event.payload.ref);
    run_cmd(&#39;sh&#39;, [&#39;./deploy.sh&#39;,event.payload.repository.name], function(text){ console.log(text) });
})
</code></pre>

<p>上面的代码中用到了一个<code>github-webhook-handler</code>的中间价，你可以用<code>npm install -g github-webhook-handler</code>来全局安装</p>

<p>还有代码这行:</p>

<pre><code class="language-text">var handler = createHandler({ path: &#39;/&#39;, secret: &#39;root&#39; }) 
</code></pre>

<p>其中secret后的参数是你在github的项目中添加webhook时设置的secret值，替换成自己的就行了</p>

<h3 id="toc_1">完成deploy.sh脚本</h3>

<p>deploy.sh脚本负责进入项目的目录，然后利用git命令拉取最新的代码，还是直接贴代码:</p>

<pre><code class="language-bash"> #!/bin/bash

WEB_PATH=&#39;/root/tools/&#39;$1
WEB_USER=&#39;root&#39;
WEB_USERGROUP=&#39;root&#39;

echo &quot;Start deployment&quot;
cd $WEB_PATH
echo &quot;pulling source code...&quot;
git reset --hard origin/master
git clean -f
git pull
git checkout master
echo &quot;changing permissions...&quot;
chown -R $WEB_USER:$WEB_USERGROUP $WEB_PATH
echo &quot;Finished.&quot;
</code></pre>

<p>deploy.sh 会接受第一个参数当做项目名字，然后进入这个项目的目录执行git操作，这个参数是在deploy.js中根据hook返回的项目名字来的，代码应该比较容易懂，都是些简单的git命令。</p>

<blockquote>
<p>如果是全新的项目，需要在你的服务器上先clone要部署的项目<br/>
你需要根据自己的实际项目位置，修改WEB_PATH的值</p>
</blockquote>

<h3 id="toc_2">后台运行deploy.js</h3>

<p>利用Linux提供的nohup命令，让deploy.js运行在后台</p>

<pre><code class="language-text">nohup node deploy.js &gt; deploy.log &amp;
</code></pre>

<h3 id="toc_3">去Github后台添加webhook</h3>

<p>进入你需要自动部署的项目的github地址，进入项目的设置页面，点击左侧的<code>Webhooks &amp; services</code><br/>
<img src="https://pic.mylonly.com/2016-06-29_14635620989191.jpg" alt="2016-06-29_14635620989191.jpg"/></p>


			
			
		</div>

	</article>
  

</div>
<nav id="pagenavi">
	 <a class="prev" href="all_5.html">Prev</a>  
	 <a class="next" href="all_7.html">Next</a> 
	<div class="center"><a href="archives.html">Blog Archives</a></div>

</nav>

</div>



        </div>
			<footer id="footer" class="inner">Copyright &copy; 2014
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; 
Theme by <a href="http://shashankmehta.in/archive/2012/greyshade.html">Shashank Mehta</a>
      </footer>
		</div>
	</div>

  
    



</body>
</html>