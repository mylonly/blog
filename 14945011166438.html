<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
	<title>  
	  
  	Scrapy——crawlSpider爬虫 - 独自一人
  	
	</title>

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

	<link href="atom.xml" rel="alternate" title="独自一人" type="application/atom+xml">

	<link href="asset/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<link href="asset/stylesheets/font-awesome.min.css" media="screen, projection" rel="stylesheet" type="text/css">
	<script src="asset/javascripts/jquery.min.js"></script>
	<script src="asset/highlightjs/highlight.pack.js"></script>
	<link href="asset/highlightjs/styles/solarized_dark.css" media="screen, projection" rel="stylesheet" type="text/css">
<script>hljs.initHighlightingOnLoad();</script>

	<!--[if lt IE 9]><script src="asset/javascripts/html5.js"></script><![endif]-->
	<!-- <link href='http://fonts.googleapis.com/css?family=Nunito:400,300,700' rel='stylesheet' type='text/css'> -->
	<style type="text/css">
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 300;
  src: local('Nunito-Light'), url(asset/font/1TiHc9yag0wq3lDO9cw0voX0hVgzZQUfRDuZrPvH3D8.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 400;
  src: local('Nunito-Regular'), url(asset/font/6TbRXKWJjpj6V2v_WyRbMX-_kf6ByYO6CLYdB4HQE-Y.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 700;
  src: local('Nunito-Bold'), url(asset/font/TttUCfJ272GBgSKaOaD7KoX0hVgzZQUfRDuZrPvH3D8.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
	</style>
	
	<style type="text/css">
	.container .left-col{ opacity: 1;}
	#pagenavi a{ font-size: 1.3em;}
	#pagenavi .next:before{ top: 3px;}
	#pagenavi .prev:before{ top: 3px;}
	.container .mid-col .mid-col-container #content .archives .title{ font-size: 1.5em;}
	.container .mid-col .mid-col-container #content article{ padding: 15px 0px;}
	#header .subtitle {
		line-height: 1.2em;
		padding-top: 8px;
	}
	article pre{ background: none; border: none; padding: 0;}
	article .entry-content{text-align: left;}
	.share-comment{ padding: 25px 0px; clear: both;}
	hr{ margin: 20px 0px;border: 0; border-top:solid 1px #ddd;}
	</style>
  

</head>


<body>
	<div class="container">
		<div class="left-col">
			<div class="intrude-less">
				<header id="header" class="inner">
				 
				 	<div class="profilepic">
						<img src="https://pic.mylonly.com/2017-05-11-IMG_1164.JPG" style="width:160px;">
					</div>
            	
					
					<h1><a href="index.html">独自一人</a></h1>
					<p class="subtitle">独自一人,独自Coding...</p>
					<nav id="main-nav">
						<ul class="main">
						
						  <li id=""><a target="self" href="index.html">Home</a></li>
						
						  <li id=""><a target="_self" href="archives.html">Archives</a></li>
						
						</ul>
					</nav>

					<nav id="sub-nav">
						<div class="social">










<a target="_blank" class="github" target="_blank" href="https://github.com/mylonly" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:root@mylonly.com" title="Email">Email</a>

								

								<a class="rss" href="atom.xml" title="RSS">RSS</a>
							
						</div>
					</nav>
				</header>				
			</div>
		</div>	
		<div class="mid-col">
			<div class="mid-col-container"> <div id="content" class="inner">

	<article class="post" itemscope itemtype="http://schema.org/BlogPosting">
		<h1 class="title" itemprop="name">Scrapy——crawlSpider爬虫</h1>
		<div class="entry-content" itemprop="articleBody">
			<p>Scrapy中的BaseSpider爬虫类只能抓取start_urls中提供的链接，而利用Scrapy提供的crawlSpider类可以很方便的自动解析网页上符合要求的链接，从而达到爬虫自动抓取的功能。<br/>
 <br/>
要利用crawSpider和BaseSpider的区别在于crawSpider提供了一组Rule对象列表，这些Rule对象规定了爬虫抓取链接的行为，Rule规定的链接才会被抓取，交给相应的callback函数去处理。<br/>
  <br/>
在rules中通过SmglLinkExtractor提取希望获取的链接。<br/>
 <br/>
我此次的demo中rule只有一个，如下：</p>

<pre><code class="language-python">allowed_domains = [&#39;sj.zol.com.cn&#39;]
start_urls = [&#39;http://sj.zol.com.cn/bizhi/&#39;]
number = 0
rules = (
            Rule(SgmlLinkExtractor(allow = (&#39;detail_\d{4}_\d{5}\.html&#39;)),callback = &#39;parse_image&#39;,follow=True),
            )
</code></pre>

<p>搜索起始链接下面符合allow中正则表达式的链接，并跟进解析，如果follow = False，则只会解析起始链接中找到的符合要求的链接<br/>
 <br/>
SmglLinkExtractor主要参数：<br/>
 <br/>
- allow：满足括号中“正则表达式”的值会被提取，如果为空，则全部匹配。<br/>
- deny：与这个正则表达式(或正则表达式列表)不匹配的URL一定不提取。<br/>
- allow_domains：会被提取的链接的domains。<br/>
- deny_domains：一定不会被提取链接的domains。<br/>
- restrict_xpaths：使用xpath表达式，和allow共同作用过滤链接。<br/>
 <br/>
下面的内容就是利用Selector解析获得的response并赋值个item就行了：</p>

<pre><code class="language-python">#coding: utf-8 #############################################################
# File Name: spiders/wallpaper.py
# Author: mylonly
# mail: tianxianggen@gmail.com
#Blog:www.mylonly.com
# Created Time: 2014年09月01日 星期一 14时20分07秒
#########################################################################
#!/usr/bin/python
import urllib2
import os
import re
 
from scrapy.contrib.spiders import CrawlSpider,Rule
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
from scrapy.selector import Selector
from mylonly.items import wallpaperItem
from scrapy.http import Request
 
class wallpaper(CrawlSpider):
        name = &quot;wallpaperSpider&quot;
        allowed_domains = [&#39;sj.zol.com.cn&#39;]
        start_urls = [&#39;http://sj.zol.com.cn/bizhi/&#39;]
        number = 0
        rules = (
                        Rule(SgmlLinkExtractor(allow = (&#39;detail_\d{4}_\d{5}\.html&#39;)),callback = &#39;parse_image&#39;,follow=True),
                        )
        def parse_image(self,response):
                self.log(&#39;hi,this is an item page! %s&#39; % response.url)
                sel = Selector(response)
                sites = sel.xpath(&quot;//div[@class=&#39;wrapper mt15&#39;]//dd[@id=&#39;tagfbl&#39;]//a[@target=&#39;_blank&#39;]/@href&quot;).extract()       
                for site in sites:
                        url = &#39;http://sj.zol.com.cn%s&#39; % (site)
                        print &#39;one page:&#39;,url
                        item = wallpaperItem()
                        item[&#39;size&#39;] = re.search(&#39;\d*x\d*&#39;,site).group()
                        item[&#39;altInfo&#39;] = sel.xpath(&quot;//h1//a/text()&quot;).extract()[0]
                        return Request(url,meta = {&#39;item&#39;:item},callback = self.parse_href)
        def parse_href(self,response):
                print &#39;I am in:&#39;,response.url
                item = response.meta[&#39;item&#39;]
                items = []
                sel = Selector(response)
                src = sel.xpath(&quot;//body//img/@src&quot;).extract()[0]
                item[&#39;imgSrc&#39;] = src
                items.append(item)
                return items
                #self.download(src)
        def download(self,url):
                self.number += 1
                savePath = &#39;/mnt/python_image/%d.jpg&#39; % (self.number)
                print &#39;正在下载...&#39;,url
                try:
                        u = urllib2.urlopen(url)
                        r = u.read()
                        downloadFile = open(savePath,&#39;wb&#39;)
                        downloadFile.write(r)
                        u.close()
                        downloadFile.close()
                except:
                        print savePath,&#39;can not download.&#39;
</code></pre>

<p> <br/>
 <br/>
 <br/>
接下来看看我的Item.py的代码：</p>

<pre><code class="language-python">class wallpaperItem(scrapy.Item):
        size = scrapy.Field()
        altInfo = scrapy.Field()
        imgSrc = scrapy.Field()
</code></pre>

<p> <br/>
还有pipilines.py:</p>

<pre><code class="language-python"># -*- coding: utf-8 -*-
 
# Define your item pipelines here
#
# Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting
# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html
import MySQLdb
 
 
class MylonlyPipeline(object):
    def process_item(self, item, spider):
        return item
 
class wallpaperPipeline(object):
        def process_item(self,item,spider):
                print &#39;imgSrc:&#39;,item[&#39;imgSrc&#39;]
                db = MySQLdb.connect(&quot;rdsauvva2auvva2.mysql.rds.aliyuncs.com&quot;,&quot;mylonly&quot;,&quot;703003659txg&quot;,&quot;wallpaper&quot;)
                cursor = db.cursor()
                db.set_character_set(&#39;utf8&#39;)
                cursor.execute(&#39;SET NAMES utf8;&#39;)
                cursor.execute(&#39;SET CHARACTER SET utf8;&#39;)
                cursor.execute(&#39;SET character_set_connection=utf8;&#39;)
 
                sql =&quot;INSERT INTO mobile_download(wallpaper_size,wallpaper_info,wallpaper_src)\
                      VALUES(&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;)&quot;%(item[&#39;size&#39;],item[&#39;altInfo&#39;],item[&#39;imgSrc&#39;])
                try:
                        print sql
                        cursor.execute(sql)
                        db.commit()
                except MySQLdb.Error,e:
                        print &quot;Mysql Error %d: %s&quot; % (e.args[0], e.args[1])
                db.close()
                return item
</code></pre>

<p> <br/>
 <br/>
 <br/>
本例中我将传回的items数据存放到了数据库中，如果你不想这样，可以将我注释掉的self.download()取消注释，不反悔items，就可以将找到的所有图片链接全部下载下来，不过要找个大点的地方存储，因为总共有5W多条：<br/>
<img src="https://pic.mylonly.com/2016-06-29_061712481319744.png" alt="2016-06-29_061712481319744.png"/></p>

		</div>
	</article>
	<div class="share-comment">
	 <!--PC和WAP自适应版-->
<div id="SOHUCS" ></div> 
<script type="text/javascript"> 
(function(){ 
var appid = 'cytztSWS2'; 
var conf = 'prod_b4255ea3b300e547eb51db263247f76f'; 
var width = window.innerWidth || document.documentElement.clientWidth; 
if (width < 960) { 
window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id=' + appid + '&conf=' + conf + '"><\/script>'); } else { var loadJs=function(d,a){var c=document.getElementsByTagName("head")[0]||document.head||document.documentElement;var b=document.createElement("script");b.setAttribute("type","text/javascript");b.setAttribute("charset","UTF-8");b.setAttribute("src",d);if(typeof a==="function"){if(window.attachEvent){b.onreadystatechange=function(){var e=b.readyState;if(e==="loaded"||e==="complete"){b.onreadystatechange=null;a()}}}else{b.onload=a}}c.appendChild(b)};loadJs("https://changyan.sohu.com/upload/changyan.js",function(){window.changyan.api.config({appid:appid,conf:conf})}); } })(); </script>


	  

	  

	</div>
</div>        </div>
			<footer id="footer" class="inner">Copyright &copy; 2014
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; 
Theme by <a href="http://shashankmehta.in/archive/2012/greyshade.html">Shashank Mehta</a>
      </footer>
		</div>
	</div>

  
    


<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?6d4f2f6760a5d2778c062a1a7df8c960";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script>
</body>
</html>