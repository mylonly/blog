<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[独自一人]]></title>
  <link href="https://blog.mylonly.com/atom.xml" rel="self"/>
  <link href="https://blog.mylonly.com/"/>
  <updated>2018-12-06T18:40:19+08:00</updated>
  <id>https://blog.mylonly.com/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.coderforart.com/">CoderForArt</generator>

  
  <entry>
    <title type="html"><![CDATA[利用SoftetherVPN和Bind9穿透内网，共享局域网内部资源]]></title>
    <link href="https://blog.mylonly.com/15440788666202.html"/>
    <updated>2018-12-06T14:47:46+08:00</updated>
    <id>https://blog.mylonly.com/15440788666202.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">写在前面</h2>

<p>我想很多公司由于安全考虑，一些内部系统(Git仓库、OA之类）的一定是放在公司内部的服务器上，只有在公司的网络下才能够访问，但是一些公司的随着业务的发展，肯定会出现一些外地的同事（外地的办事处、研发中心、销售团队等等）这种情况，这样一来，在内部服务不迁移到外网的前提下，如何让外地的同事能访问到内网这些资源就成了一个必须要解决的问题了。<br/>
其实内网穿透有很很多种解决方案，本文只是利用SoftetherVPN以及Bind9这两个工具提供另一种解决思路。<br/>
本文所提到的解决方案有如下优点:</p>

<pre><code class="language-text">1. 原来的内部资源不需要额外的配置改动
2. 需要身份认证，只有拥有权限的外部人员才能访问内部资源
3. 只有加入了共享网络的内部资源才能被外部访问，没有开放共享的内部资源的无法访问
4. 可以做到通过相同的域名访问同一个内部资源
5. 通过LPTP,LPSec协议支持OS X以及iOS设备访问
</code></pre>

<p>当然也有如下不足:<br/>
    1. 需要一台拥有公网IP的服务器作为中心服务器<br/>
    2. 由于本文的解决方案是基于虚拟局域网来实现的，因此拥有局域网内一些无法回避的缺点。</p>

<h2 id="toc_1">原理</h2>

<p><img src="https://pic.mylonly.com/2018-12-06-%E7%BD%91%E7%BB%9C%E5%9B%BE%20-1-.png" alt=""/><br/>
本解决方案利用了SoftetherVPN提供的Ad-Hoc VPN将内网服务器和外部用户通过VLAN连接在同一个局域网内，然后通过支持智能解析的DNS服务器，将域名解析到对应的虚拟局域网IP。<br/>
如果是内网用户，则直接通过DNS服务器将域名解析到内部资源对应的内网IP。</p>

<h2 id="toc_2">部署</h2>

<p>本次部署大部分采用Docker方式部署，如果需要手动安装的，可以在<a href="https://www.softether-download.com/cn.aspx?product=softether">这个网站</a>下载对应的安装包</p>

<h3 id="toc_3">Docker 环境</h3>

<p>略，不再本文讨论范围之内</p>

<h3 id="toc_4">安装Softether Server</h3>

<ol>
<li><p>拉取镜像</p>
<pre><code class="language-text">siomiz/softethervpn
</code></pre></li>
<li><p>启动vpnserver</p>
<pre><code class="language-text">docker run -d --cap-add NET_ADMIN \
 -p 500:500/udp \<br/>
 -p 4500:4500/udp \<br/>
 -p 1701:1701/tcp \<br/>
 -p 1194:1194/udp \<br/>
 -p 5555:5555/tcp \<br/>
 -p 443:443/tcp \<br/>
 -p 992:992 \<br/>
 siomiz/softethervpn
</code></pre>
<p>记得要开放对应的端口</p></li>
</ol>

<h3 id="toc_5">安装Softether ServerManger</h3>

<p>虽然Softether 提供了mac版的安装包，但本人装完之后发现其实是套了一个wine的环境，在mac上体验极差，所以此处推荐找一个windows电脑安装vpn server manager，<a href="https://www.softether-download.com/cn.aspx?product=softether">下载地址</a>在此<br/>
由于是windows上的安装教程，大部分都是图片，为了不占文章篇幅，就不放在文章里介绍了，各位可以参考<a href="https://www.softether.org/4-docs/2-howto/1.VPN_for_On-premise/1.Ad-hoc_VPN">官网</a>或者这篇<a href="http://www.leqii.com/download/yulan/d77/d77.html">博客</a></p>

<p>其中需要注意的是，在<code>虚拟HUB管理页面</code>中的<code>虚拟NAT和虚拟DHCP服务器</code>，点击<code>SecureNAT</code>,然后设置DHCP服务器的网段，以及DNS服务器为下面将要搭建的DNS服务器地址<br/>
<img src="https://pic.mylonly.com/2018-12-06-101214.png" alt=""/></p>

<h3 id="toc_6">部署DNS服务器</h3>

<ol>
<li><p>安装bind9</p>
<pre><code class="language-text">sudo apt-get install bind9
</code></pre></li>
<li><p>增加两个视图文件,在/etc/bind/目录下<br/>
name.2.conf</p>
<pre><code class="language-text">acl &quot;2&quot; {
    192.168.2.0/24;<br/>
};   
</code></pre>
<p>name.111.conf </p>
<pre><code class="language-text">acl &quot;111&quot; {
    192.168.111.0/24;<br/>
};
</code></pre>
<p>两个视图文件主要是为了区分来源IP所在的网段，然后根据不同的网段返回不同的解析结果，在本解决方案当中，192.168.2.0/24网段为内网网段，   192.168.111.0/24为VPN生成的虚拟局域网的网段</p></li>
<li><p>新建不同网段对应的不同的解析记录文件，也在/etc/bind/目录下</p>
<p>1234tv.lan-2.zone</p>
<pre><code class="language-text">$TTL    604800
@       IN      SOA      xxxx.xx. root.xxxx.xx. (<br/>
                              2         ; Serial<br/>
                         604800         ; Refresh<br/>
                          86400         ; Retry<br/>
                        2419200         ; Expire<br/>
                         604800 )       ; Negative Cache TTL<br/>
;<br/>
@       IN      NS      xxxx.xx.<br/>
@       IN      A      192.168.2.222<br/>
gitlab  IN      A       192.168.2.84
</code></pre>
<p>1234tv.lan-111.zone</p>
<pre><code class="language-text">$TTL    604800
@       IN      SOA     xxxx.xx. root.xxxx.xx. (<br/>
                              2         ; Serial<br/>
                         604800         ; Refresh<br/>
                          86400         ; Retry<br/>
                        2419200         ; Expire<br/>
                         604800 )       ; Negative Cache TTL<br/>
;<br/>
@       IN      NS      xxxx.xx.<br/>
@       IN      A       192.168.111.222<br/>
gitlab  IN      A       192.168.111.84
</code></pre></li>
<li><p>将视图文件和解析文件写入Bind9主配置文件</p>
<pre><code class="language-text">sudo vim /etc/bind/name.conf
</code></pre>
<p>增加如下内容，声明两个视图，view_2,view_11,可以注释掉默认带的配置文件，如果你没有其他的用途的话,注意每一行后面的分号，不可缺少。</p>
<pre><code class="language-text">#include &quot;/etc/bind/named.conf.options&quot;;
#include &quot;/etc/bind/named.conf.local&quot;;<br/>
#include &quot;/etc/bind/named.conf.default-zones&quot;;<br/>
include &quot;/etc/bind/name.2.conf&quot;;<br/>
view &quot;View_2&quot; {<br/>
        match-clients {&quot;2&quot;;};<br/>
        zone &quot;xxxx.xxx&quot; IN {<br/>
                type master;<br/>
                file &quot;xxxx.xxx-2.zone&quot;;<br/>
                allow-update {none;};<br/>
        };<br/>
};<br/>
include &quot;/etc/bind/name.111.conf&quot;;<br/>
view &quot;View_111&quot; {<br/>
        match-clients {&quot;111&quot;;};<br/>
        zone &quot;xxxx.xxx&quot; IN {<br/>
                type master;<br/>
                file &quot;xxxx.xxx-111.zone&quot;;<br/>
                allow-update {none;};<br/>
        };<br/>
};
</code></pre></li>
<li><p>然后启动Bind9 服务</p>
<pre><code class="language-text">sudo /etc/init.d/bind9 start
</code></pre></li>
<li><p>验证解析</p>
<pre><code class="language-text">dig gitlab.xxx.xx @192.168.2.222
</code></pre>
<p><img src="media/15440788666202/15440920519061.jpg" alt="" style="width:460px;"/></p></li>
</ol>

<h4 id="toc_7">安装Vpn Client</h4>

<p>所有需要共享给外部访问的内部服务器都需要安装vpn client连接上vpn server，下面的方法为linux 下docker方式部署vpn_client方法，windows端请参考这篇<a href="http://www.leqii.com/download/yulan/d77/d77.html">文章</a></p>

<ol>
<li><p>拉取镜像</p>
<pre><code class="language-text">sudo docker pull mitsutaka/softether-vpnclient
</code></pre></li>
<li><p>启动镜像连接之前创建的vpn server</p>
<pre><code class="language-text">docker run -d --name=softether-vpnclient \
--net=host --privileged \<br/>
-e VPN_SERVER=&lt;Softether VPN server&gt; \<br/>
-e VPN_PORT=&lt;Softether VPN port&gt; \<br/>
-e ACCOUNT_USER=&lt;Registered username&gt; \<br/>
-e ACCOUNT_PASS=&lt;Registered password&gt; \<br/>
-e VIRTUAL_HUB=&lt;Virtual Hub name&gt; \<br/>
-e TAP_IPADDR=&lt;IP address/netmask&gt; \<br/>
mitsutaka/softether-vpnclient<br/>
```<br/>
服务器端，VIRTUAL_HUB端填入之前在manager上新建的虚拟HUB名称 ，TAP_IPADDR，最好采用指定IP的方式，尤其是DNS服务器，需要固定IP地址
</code></pre></li>
</ol>

<h2 id="toc_8">结束语</h2>

<p>至此，这个简单的内网穿透方案算是部署完成，内网用户可以直接通过设置内网DNS服务器来通过域名访问内部服务器，外网用户在使用vpn clinet连接上虚拟局域网之后也能通过相同的域名访问内部服务器资源。<br/>
写这篇文章，一方面是觉得自己以后可能还会有此需求，怕自己遗忘，权且当做记录。另外也是想也可能有别的同学有这方面的需要，写出来共享一下。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kali Linux 安装w3af]]></title>
    <link href="https://blog.mylonly.com/15423821126836.html"/>
    <updated>2018-11-16T23:28:32+08:00</updated>
    <id>https://blog.mylonly.com/15423821126836.html</id>
    <content type="html"><![CDATA[
<h3 id="toc_0">更新软件源</h3>

<pre><code class="language-text">apt-get update
</code></pre>

<h3 id="toc_1">下载源码</h3>

<pre><code class="language-text">git clone --depth 1 https://github.com/andresriancho/w3af.git
</code></pre>

<h3 id="toc_2">安装node(如果已安装请忽略)</h3>

<pre><code class="language-text">wget https://nodejs.org/dist/v10.13.0/node-v10.13.0-linux-x64.tar.xz
xz -d node-v10.13.0-linux-x64.tar.xz
tar -vxf node-v10.13.0-linux-x64.tar
mv node-v10.13.0-linux-x64.tar /usr/local/node
</code></pre>

<p>node 加入到环境变量当中</p>

<pre><code class="language-text">vim ~/.basrc
</code></pre>

<p>最后一行添加</p>

<pre><code class="language-text">export PATH=$PATH:/usr/local/node/bin
</code></pre>

<h3 id="toc_3">获取w3af依赖安装脚本</h3>

<pre><code class="language-text">./w3af_console
./tmp/w3af_dependency_install.sh #安装依赖
</code></pre>

<h4 id="toc_4">提示: ld can not found -lcrypto</h4>

<p>没有找到crypto库，crypto库是openssl中的，一般没有找到的话基本都是/usr/lib下没有改动态库的链接文件</p>

<p>利用<code>find / -name *libcrypto.so*</code>指令找到libcrypto.so的位置，然后在/usr/lib下建立起软连接</p>

<pre><code class="language-text">ln -s /usr/lib/x86_64-linux-gnu/libcrypto.so.1.1 /usr/lib/libcrypto.so
</code></pre>

<h4 id="toc_5">提示: Cannot uninstall &#39;scapy&#39;. It is a distutils installed project</h4>

<p>系统以及安装了相应的版本的scapy，但是和w3af中要求的版本不符，w3af不能卸载它<br/>
此时可以手动删除它</p>

<pre><code class="language-text">apt-get remove python-scapy
</code></pre>

<p>或者在<code>w3af/core/controllers/dependency_check/requirements.py</code>中找到依赖包对应的版本，将其修改为系统自带的版本<br/>
改完之后仍然需要在<code>w3af/core/controllers/dependency_check/platforms/mac.py</code>找到包对应的版本，修改为系统版本</p>

<h4 id="toc_6">执行<code>./w3af_gui</code>提示: No module named webkit</h4>

<p>由于新版本的Kali的软件源中已经没有了python-webkit和python-webkit-dev，所以我们需要手动下载并安装它们</p>

<pre><code class="language-text">#下载缺省的deb二进制包
wget http://ftp.br.debian.org/debian/pool/main/p/pywebkitgtk/python-webkit_1.1.8-3_amd64.deb
wget http://ftp.br.debian.org/debian/pool/main/w/webkitgtk/libjavascriptcoregtk-1.0-0_2.4.11-3_amd64.deb
wget http://ftp.br.debian.org/debian/pool/main/p/python-support/python-support_1.0.15_all.deb
wget http://ftp.br.debian.org/debian/pool/main/w/webkitgtk/libwebkitgtk-1.0-0_2.4.11-3_amd64.deb
#依次安装deb二进制包
dpkg -i libjavascriptcoregtk-1.0-0_2.4.11-3_amd64.deb 
dpkg -i python-support_1.0.15_all.deb 
dpkg -i libwebkitgtk-1.0-0_2.4.11-3_amd64.deb
dpkg -i python-webkit_1.1.8-3_amd64.deb
#修复依赖
apt-get -f install -y
</code></pre>

<h4 id="toc_7">执行<code>./w3af_gui</code>提示: ImportError: No module named gtksourceview2</h4>

<p>缺少python-gtksourceview2</p>

<pre><code class="language-text">apt-get install python-getsourceview2
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[PT924光猫 更改桥接模式/路由模式]]></title>
    <link href="https://blog.mylonly.com/15319308951102.html"/>
    <updated>2018-07-19T00:21:35+08:00</updated>
    <id>https://blog.mylonly.com/15319308951102.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p><a href="http://192.168.1.1:8080/cgi-bin/index2.asp">http://192.168.1.1:8080/cgi-bin/index2.asp</a> 进入这个页面<br/>
用useradmin账号密码登录</p></li>
<li><p>将地址改为<a href="http://192.168.1.1:8080/cgi-bin/net-wanset.asp">http://192.168.1.1:8080/cgi-bin/net-wanset.asp</a> 进入internet设置页面(更改其他地址也会进入其他页面)</p>
<p><img src="media/15319308951102/15319310908143.jpg" alt="" style="width:532px;"/></p></li>
</ol>

<p>其中<code>6_INTERNET_B_VID_229</code>是桥接模式，路由器需要拨号，而<code>5_INTERNET_R_VID_229</code>是路由模式，由猫拨号，路由器不需要拨号</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[我的豆瓣电影影评抓取之旅]]></title>
    <link href="https://blog.mylonly.com/15308616208129.html"/>
    <updated>2018-07-06T15:20:20+08:00</updated>
    <id>https://blog.mylonly.com/15308616208129.html</id>
    <content type="html"><![CDATA[
<p><a href="https://juejin.im/entry/5b42b4125188251b3950c25c/detail"><img src="https://badge.juejin.im/entry/5b42b4125188251b3950c25c/likes.svg?style=flat-square" alt=""/></a></p>

<h2 id="toc_0">前言</h2>

<p>由于最近一直在研究基于机器学习的推荐系统，需要大量的数据来训练AI模型，但是在模型的测试验证过程中，苦于中文数据集的缺失(或者说根本没有，国人在这方面做得实在是太差了)，只能利用国外公开的推荐系统数据集，有著名的<a href="https://grouplens.org/datasets/movielens/">MovieLens电影评分数据集</a>和<a href="http://del.icio.us">Del.icio.us链接推荐数据集</a>，虽然通过计算损失函数也能大致的评估推荐模型的优劣程度从而进行相应的优化，但是由于语言环境、文化等等的不同，国外人对某个电影的评分毕竟跟我们还是有一定差距的，在输出推荐结果时，即使给出的某个电影或者某个网站链接其相似度很高时，我仍然不确定这个推荐结果是否真的如损失函数计算的那样准确。所以，为了能拥有一个可以用于训练的中文的数据集，就有了本文所记录的豆瓣影评的抓取过程。</p>

<h2 id="toc_1">网站分析</h2>

<p>首先还是要分析一下要抓取的网站<a href="https://movie.douban.com/">豆瓣电影</a>,主要是通过搜索引擎或者浏览器的调试工具看看有没有可以利用的API，在没有找到任何api的前提下才开始分析网站的页面结构，找到可以提取的信息。<br/>
通过搜索引擎，我找到了<a href="https://developers.douban.com/wiki/?title=api_v2">豆瓣开发者平台</a>，在<a href="https://developers.douban.com/wiki/?title=movie_v2">豆瓣电影</a>的文档中有获取电影，获取影评等等的详细接口，正当我以为接下来的数据采集将会变得非常简单之时，下面这张图还是让我冷静了下来<br/>
<img src="https://pic.mylonly.com/2018-07-07-122829.jpg" alt=""/></p>

<blockquote>
<p>如果你在2015年之前注册过豆瓣的开发者，那么恭喜你,你可以通过豆瓣提供的API或者SDK获取你想获得的任何数据</p>
</blockquote>

<h3 id="toc_2">电影信息获取</h3>

<p>虽然APIKey是不可能拿到了，但是通过文档我仍然发现了一些GET请求并不需要AUTH认证，也就是说有没有APIKey并不影响使用。其中，对我们有用的就是获取<code>TOP250电影列表</code>的接口:</p>

<pre><code class="language-text">http://api.douban.com/v2/movie/top250
</code></pre>

<p>接口返回的格式大概如下:<br/>
<img src="https://pic.mylonly.com/2018-07-07-122848.jpg" alt=""/><br/>
里面包含了电影一些详细信息，对于推荐系统来说，这些数据足够了。</p>

<p>此外，通过利用Chrome的调试工具，在<a href="https://movie.douban.com/tag/#/">豆瓣电影-分类</a>这个页面我发现了他们使用的一个JQuery接口,也是一个GET请求，不需要AUTH。</p>

<p><img src="https://pic.mylonly.com/2018-07-07-122901.jpg" alt=""/><br/>
<img src="https://pic.mylonly.com/2018-07-07-123038.jpg" alt=""/></p>

<p>详细接口如下,可以通过更改start来迭代获取所有电影条目</p>

<pre><code class="language-text">https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=0,10&amp;tags=&amp;start=20
</code></pre>

<p>这个接口可以获取所有豆瓣上收录的电影，经过我的测试，start改为10000时，返回的数据就已经是空的了</p>

<h3 id="toc_3">影评信息获取</h3>

<p>获取电影信息的方法有了，接下来就是要分析如何获取影评信息了。<br/>
在每部电影的详情页面里,如<a href="https://movie.douban.com/subject/1292064/">楚门的世界</a>，我们找到了如下这几个详情页面,分别显示了针对这部电影的影评和短评信息</p>

<pre><code class="language-text">https://movie.douban.com/subject/1292064/reviews ##影评页面
https://movie.douban.com/subject/1292064/comments ##短评页面
</code></pre>

<p><img src="https://pic.mylonly.com/2018-07-06-085101.png" alt=""/></p>

<p>照例先用调试工具看看有没有可以用的api接口后，发现这次并没有那么好运了，这个影评页面是由服务器渲染完成的。</p>

<p>没有了接口，我们来分析页面，依然通过调试工具:<br/>
<img src="https://pic.mylonly.com/2018-07-07-122913.jpg" alt=""/><br/>
每条评论都是在一个review-item的div块里面，而所有评论都是在一个review-list的div块里吗，我们通过xpath语法可以很容易的定位到每条评论的详细信息,下面是所有信息的xpath语句，在我们写爬虫时候就靠他提取内容了</p>

<pre><code class="language-text">评论列表: &quot;//div[contains(@class,&#39;review-list&#39;)]//div[contains(@class,&#39;review-item&#39;)]&quot;

评论ID: &quot;.//div[@class=&#39;main-bd&#39;]//div[@class=&#39;review-short&#39;]/@data-rid&quot;
作者头像: &quot;./header[@class=&#39;main-hd&#39;]//a[@class=&#39;avator&#39;]//img/@src &quot;
作者昵称: &quot;.//header//a[@class=&#39;name&#39;]/text()&quot;
推荐程度(评分): &quot;.//header//span[contains(@class,&#39;main-title-rating&#39;)]/@title&quot;
影评标题: &quot;.//div[@class=&#39;main-bd&#39;]//h2//a/text()&quot;
影评摘要: &quot;.//div[@class=&#39;main-bd&#39;]//div[@class=&#39;short-content&#39;]/text()&quot;
影评详情页链接: &quot;.//div[@class=&#39;main-bd&#39;]//h2//a/@href&quot;
</code></pre>

<p><img src="https://pic.mylonly.com/2018-07-06-090909.png" alt=""/></p>

<p>其中具体的评分我们不能直接拿到，而是只能拿到具体的文字描述,经过我的验证，具体如下对应关系如下:</p>

<pre><code class="language-text">&#39;力荐&#39;: 5,
&#39;推荐&#39;: 4,
&#39;还行&#39;: 3,
&#39;较差&#39;: 2,
&#39;很差&#39;: 1,  
</code></pre>

<p>在后续的代码编写过程中，我们会根据这个对应关系将其转换为对应的评分信息</p>

<h2 id="toc_4">实现爬虫</h2>

<p>既然已经分析的差不多了，我们所需要的信息基本都有途径可以获得，那么接下来我们就开始具体的爬虫实现，我们采用Scrapy这个Python爬虫框架来帮我们简化爬虫的开发过程。Scrapy的安装以及VirtualEnv环境的搭建就不详细说了，其并不再本文的讨论范围之内，附上<a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/overview.html">Scrapy的中文文档地址</a></p>

<h3 id="toc_5">创建项目工程</h3>

<pre><code class="language-text">##创建DoubanSpider工程
scrapy startproject Douban
</code></pre>

<p>创建好的工程目录大致如下:<br/>
<img src="https://pic.mylonly.com/2018-07-07-122926.jpg" alt=""/><br/>
其中：</p>

<pre><code class="language-text">    spiders: 爬虫文件夹,存放具体的爬虫代码，我们待会要编写的两个爬虫(电影信息和影评信息)就需要放在这个文件夹下
    items.py: 模型类，所有需要结构化的数据都要预先在此文件中定义
    middlewares.py: 中间件类，scrapy的核心之一，我们会用到其中的downloadMiddleware,
    pipelines.py: 管道类，数据的输出管理，是存数据库还是存文件在这里决定
    settings.py: 设置类，一些全局的爬虫设置，如果每个爬虫需要有自定义的地方，可以在爬虫中直接设置custom_settings属性
</code></pre>

<h3 id="toc_6">电影信息爬虫</h3>

<blockquote>
<p>由于电影信息的获取有API接口可以使用，所以此处页可以不采用爬虫来处理数据。</p>
</blockquote>

<p>在spiders中新建一个movies.py的文件，定义我们的爬虫</p>

<p>由于我们爬取电影是通过api接口的形式获取，因此并不需要跟进解析，所以我们的爬虫直接继承Spider就可以了</p>

<h4 id="toc_7">定义爬虫</h4>

<pre><code class="language-text">class MovieSpider(Spider):
    name = &#39;movie&#39; #爬虫名称
    allow_dominas = [&quot;douban.com&quot;] #允许的域名
    
    #自定义的爬虫设置，会覆盖全局setting中的设置
    custom_settings = {
        &quot;ITEM_PIPELINES&quot;: {
            &#39;Douban.pipelines.MoviePipeline&#39;: 300
        },
        &quot;DEFAULT_REQUEST_HEADERS&quot;: {
            &#39;accept&#39;: &#39;application/json, text/javascript, */*; q=0.01&#39;,
            &#39;accept-encoding&#39;: &#39;gzip, deflate&#39;,
            &#39;accept-language&#39;: &#39;zh-CN,zh;q=0.8,en;q=0.6,zh-TW;q=0.4&#39;,
            &#39;referer&#39;: &#39;https://mm.taobao.com/search_tstar_model.htm?spm=719.1001036.1998606017.2.KDdsmP&#39;,
            &#39;user-agent&#39;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.97 Safari/537.36&#39;,
            &#39;x-requested-with&#39;: &#39;XMLHttpRequest&#39;,
        },
        &quot;ROBOTSTXT_OBEY&quot;:False #需要忽略ROBOTS.TXT文件
    }

</code></pre>

<p>custom_setting中，<code>ITEM_PIPELINES</code>指定了获取数据后数据输出时使用的管道接口<br/>
<code>DEFAULT_REQUEST_HEADERS</code>则是让我们的Spider伪装成一个浏览器，防止被豆瓣拦截掉。<br/>
而<code>ROBOTSTXT_OBEY</code>则是让我们的爬虫忽略ROBOTS.txt的警告</p>

<p>接下来通过<code>start_request</code>告诉爬虫要爬取的链接：</p>

<pre><code class="language-python">
    def start_requests(self):
        url = &#39;&#39;&#39;https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=0,10&amp;tags=&amp;start={start}&#39;&#39;&#39;
        requests = []
        for i in range(500):
            request = Request(url.format(start=i*20), callback=self.parse_movie)
            requests.append(request)
        return requests
</code></pre>

<p>由于我们之前分析网站的时候已经分析过了，start参数到10000时就获取不到数据了，所以此处直接用这个数字循环获得所有链接</p>

<p>接下来解析每个接口返回的内容:</p>

<pre><code class="language-python">
def parse_movie(self, response):
    jsonBody = json.loads(response.body)
    subjects = jsonBody[&#39;data&#39;]
    movieItems = []
    for subject in subjects:
        item = MovieItem()
        item[&#39;id&#39;] = int(subject[&#39;id&#39;])
        item[&#39;title&#39;] = subject[&#39;title&#39;]
        item[&#39;rating&#39;] = float(subject[&#39;rate&#39;])
        item[&#39;alt&#39;] = subject[&#39;url&#39;]
        item[&#39;image&#39;] = subject[&#39;cover&#39;]
        movieItems.append(item)
    return movieItems
</code></pre>

<p>在request中，我们指定了一个parse_movie的方法来解析返回的内容，此处我们需要使用一个在items.py中定义的Item,具体Item如下:</p>

<h4 id="toc_8">定义Item</h4>

<pre><code class="language-python">#定义你需要获取的数据
class MovieItem(scrapy.Item):
    id = scrapy.Field()
    title = scrapy.Field()
    rating = scrapy.Field()
    genres = scrapy.Field()
    original_title = scrapy.Field()
    alt = scrapy.Field()
    image = scrapy.Field()
    year = scrapy.Field()
</code></pre>

<p>items返回给Scrapy之后，Scrapy会调用我们之前在custom_setting中指定的<code>Douban.pipelines.MoviePipeline</code>来处理获取到的item，MoviePipeline定义在pipelines.py中，具体内容如下:</p>

<h4 id="toc_9">定义Pipeline</h4>

<pre><code class="language-python">class MoviePipeline(object):

    movieInsert = &#39;&#39;&#39;insert into movies(id,title,rating,genres,original_title,alt,image,year) values (&#39;{id}&#39;,&#39;{title}&#39;,&#39;{rating}&#39;,&#39;{genres}&#39;,&#39;{original_title}&#39;,&#39;{alt}&#39;,&#39;{image}&#39;,&#39;{year}&#39;)&#39;&#39;&#39;

    def process_item(self, item, spider):

        id = item[&#39;id&#39;]
        sql = &#39;select * from movies where id=%s&#39;% id
        self.cursor.execute(sql)
        results = self.cursor.fetchall()
        if len(results) &gt; 0:
            rating = item[&#39;rating&#39;]
            sql = &#39;update movies set rating=%f&#39; % rating
            self.cursor.execute(sql)
        else:
            sqlinsert = self.movieInsert.format(
                id=item[&#39;id&#39;],
                title=pymysql.escape_string(item[&#39;title&#39;]),
                rating=item[&#39;rating&#39;],
                genres=item.get(&#39;genres&#39;),
                original_title=item.get(&#39;original_title&#39;),
                alt=pymysql.escape_string(item.get(&#39;alt&#39;)),
                image=pymysql.escape_string(item.get(&#39;image&#39;)),
                year=item.get(&#39;year&#39;)
            )
            self.cursor.execute(sqlinsert)
        return item

    def open_spider(self, spider):
        self.connect = pymysql.connect(&#39;localhost&#39;,&#39;root&#39;,&#39;******&#39;,&#39;douban&#39;, charset=&#39;utf8&#39;, use_unicode=True)
        self.cursor = self.connect.cursor()
        self.connect.autocommit(True)


    def close_spider(self, spider):
        self.cursor.close()
        self.connect.close()

</code></pre>

<p>在此Pipeline中，我们通过连接mysql数据库将每次获取到的item插入到具体的数据表中</p>

<h4 id="toc_10">运行爬虫</h4>

<p>在命令行下输入:</p>

<pre><code class="language-text">scrapy crawl movie
</code></pre>

<p><img src="https://pic.mylonly.com/2018-07-06-101458.png" alt=""/><br/>
<img src="https://pic.mylonly.com/2018-07-06-101353.png" alt=""/></p>

<h3 id="toc_11">影评爬虫</h3>

<p>影评爬虫的难度要大很多了，因为获取电影信息我们是通过接口直接拿到的，这种接口返回的数据格式统一，基本不会出现异常情况，而且电影数量有限，很短时间就能爬取完毕，并不会触发豆瓣的防爬虫机制，而在影评爬虫的编写过程中，这些都会遇到。</p>

<h4 id="toc_12">爬虫逻辑</h4>

<pre><code class="language-text">class ReviewSpider(Spider):
    name = &quot;review&quot;
    allow_domain = [&#39;douban.com&#39;]
    custom_settings = {
        &quot;ITEM_PIPELINES&quot;: {
            &#39;Douban.pipelines.ReviewPipeline&#39;: 300
        },
        &quot;DEFAULT_REQUEST_HEADERS&quot;: {
            &#39;connection&#39;:&#39;keep-alive&#39;,
            &#39;Upgrade-Insecure-Requests&#39;:&#39;1&#39;,
            &#39;DNT&#39;:1,
            &#39;Accept&#39;:&#39;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&#39;,
            &#39;Accept-Encoding&#39;:&#39;gzip, deflate, br&#39;,
            &#39;Accept-Language&#39;:&#39;zh-CN,zh;q=0.9,en;q=0.8,zh-TW;q=0.7&#39;,
            &#39;Cookie&#39;:&#39;bid=wpnjOBND4DA; ll=&quot;118159&quot;; __utmc=30149280;&#39;,            &#39;user-agent&#39;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) AppleWebKit/537.36 (KHTML, like Gecko) &#39;
                          &#39;Chrome/67.0.3396.87 Safari/537.36&#39;,
        },
        &quot;ROBOTSTXT_OBEY&quot;: False,
        # &quot;DOWNLOAD_DELAY&quot;: 1,
        &quot;RETRY_TIMES&quot;: 9,
        &quot;DOWNLOAD_TIMEOUT&quot;: 10
    }
</code></pre>

<p>对比获取电影信息的爬虫，在custom_setting中多了几个设置：<br/>
<code>RETRY_TIMES</code>：用来控制最大重试次数，因为豆瓣有反爬虫机制，当一个IP访问次数过多时就会限制这个IP访问，所以为了绕过这个机制，我们通过代理IP来爬取对应的页面，每爬取一个页面就更换一次IP，但是由于代理IP的质量参差不齐，收费的可能会好点，但还是会存在，为了避免出现因为代理连接不上导致某个页面被忽略掉，我们设置这个值，当重试次数大于设定的值时仍然没有获取到页面就会pass掉这个连接。如果你的代理IP质量不好，请增大此处的次数。<br/>
<code>DOWNLOAD_TIMEOUT</code>: 下载超时时间，默认是60秒，此处修改为10秒是想让整体的爬取速度加快，因为RETRY_TIMES的缘故，需要RETRY的判定时间为1分钟，如果有很多这种有问题的页面，那么整个爬取的过程会十分漫长。<br/>
<code>DOWNLOAD_DELAY</code>: 下载延迟，如果你使用代理IP之后还是会出现访问返回403的情况，请设置此值，因为某IP太频繁的访问页面会触发豆瓣的防爬虫机制。</p>

<pre><code class="language-python">  def start_requests(self):
        #从数据库中找到所有的moviesId
        self.connect = pymysql.connect(&#39;localhost&#39;,&#39;root&#39;,&#39;******&#39;,&#39;douban&#39;, charset=&#39;utf8&#39;, use_unicode=True)
        self.cursor = self.connect.cursor()
        self.connect.autocommit(True)
        sql = &quot;select id,current_page,total_page from movies&quot;
        self.cursor.execute(sql)
        results = self.cursor.fetchall()
        url_format = &#39;&#39;&#39;https://movie.douban.com/subject/{movieId}/reviews?start={offset}&#39;&#39;&#39;
        for row in results:
            movieId = row[0]
            current_page = row[1]
            total_page = row[2]
            if current_page != total_page: ##说明评论没有爬完
                url = url_format.format(movieId=movieId, offset=current_page*20)
                request = Request(url, callback=self.parse_review, meta={&#39;movieId&#39;: movieId}, dont_filter=True)
                yield request
</code></pre>

<p>照例，我们在start_request中告诉Scrapy要爬取的起始网址链接，通过我们之前的分析，影评页面的地址格式为:</p>

<pre><code class="language-python">https://movie.douban.com/subject/{movieId}/reviews?start={offset}
</code></pre>

<p>而movieId,我们之前的爬虫已经将所有电影的信息抓取了下来，所以我们在此先通过查询数据库将所有的已抓取的电影信息获取到，取到其中的movieId，然后构造一个页面链接。</p>

<pre><code class="language-python">url = url_format.format(movieId=movieId, offset=current_page*20)
</code></pre>

<p>因为抓取豆瓣影评的过程十分漫长，中间会出现各种各样的问题导致爬虫意外退出，因此我们需要一个机制让爬虫能从上次停止的地方继续爬取，current_page和total_page就是为此而服务的，在后面的数据解析过程中，每解析一个页面，就会将当期页面的页数存储下来，防止出现意外情况。</p>

<pre><code class="language-python">    def parse_review(self, response):
        movieId = response.request.meta[&#39;movieId&#39;]
        review_list = response.xpath(&quot;//div[contains(@class,&#39;review-list&#39;)]//div[contains(@class,&#39;review-item&#39;)]&quot;)
        for review in review_list:
            item = ReviewItem()
            item[&#39;id&#39;] = review.xpath(&quot;.//div[@class=&#39;main-bd&#39;]//div[@class=&#39;review-short&#39;]/@data-rid&quot;).extract()[0]
            avator = review.xpath(&quot;.//header//a[@class=&#39;avator&#39;]/@href&quot;).extract()[0]
            item[&#39;username&#39;] = avator.split(&#39;/&#39;)[-2]
            item[&#39;avatar&#39;] = review.xpath(&quot;./header[@class=&#39;main-hd&#39;]//a[@class=&#39;avator&#39;]//img/@src&quot;).extract()[0]
            item[&#39;nickname&#39;] = review.xpath(&quot;.//header//a[@class=&#39;name&#39;]/text()&quot;).extract()[0]
            item[&#39;movieId&#39;] = movieId
            rate = review.xpath(&quot;.//header//span[contains(@class,&#39;main-title-rating&#39;)]/@title&quot;).extract()
            if len(rate)&gt;0:
                rate = rate[0]
                item[&#39;rating&#39;] = RATING_DICT.get(rate)
                item[&#39;create_time&#39;] = review.xpath(&quot;.//header//span[@class=&#39;main-meta&#39;]/text()&quot;).extract()[0]
                item[&#39;title&#39;] = review.xpath(&quot;.//div[@class=&#39;main-bd&#39;]//h2//a/text()&quot;).extract()[0]
                item[&#39;alt&#39;] = review.xpath(&quot;.//div[@class=&#39;main-bd&#39;]//h2//a/@href&quot;).extract()[0]
                summary = review.xpath(&quot;.//div[@class=&#39;main-bd&#39;]//div[@class=&#39;short-content&#39;]/text()&quot;).extract()[0]
                item[&#39;summary&#39;] = summary.strip().replace(&#39;\n&#39;, &#39;&#39;).replace(&#39;\xa0(&#39;,&#39;&#39;)
                yield item

        current_page = response.xpath(&quot;//span[@class=&#39;thispage&#39;]/text()&quot;).extract()
        total_page = response.xpath(&quot;//span[@class=&#39;thispage&#39;]/@data-total-page&quot;).extract()
        paginator = response.xpath(&quot;//div[@class=&#39;paginator&#39;]&quot;).extract()
        if len(paginator) == 0 and len(review_list): ##不存在导航条，但是评论列表存在，说明评论只有一页

            sql = &quot;update movies set current_page = 1, total_page=1 where id=&#39;%s&#39;&quot; % movieId
            self.cursor.execute(sql)

        elif len(paginator) and len(review_list):
            current_page = int(current_page[0])
            total_page = int(total_page[0])
            sql = &quot;update movies set current_page = %d, total_page=%d where id=&#39;%s&#39;&quot; % (current_page, total_page, movieId)
            self.cursor.execute(sql)
            if current_page != total_page:
                url_format = &#39;&#39;&#39;https://movie.douban.com/subject/{movieId}/reviews?start={offset}&#39;&#39;&#39;
                next_request = Request(url_format.format(movieId=movieId, offset=current_page*20),
                                       callback=self.parse_review,
                                       dont_filter=True, meta={&#39;movieId&#39;: movieId})
                yield next_request

        else:
            yield response.request
</code></pre>

<p>接下来，分析解析函数，DoubanItem的数据获取就不额外介绍了，利用之前分析时用到的xpath语句可以很容易的定义到具体内容。<br/>
其中movieId是起始链接中通过Request中Meta属性传递过来的，当然你可以通过分析网页找到包含movieId的地方。</p>

<pre><code class="language-text">current_page = response.xpath(&quot;//span[@class=&#39;thispage&#39;]/text()&quot;).extract()
total_page = response.xpath(&quot;//span[@class=&#39;thispage&#39;]/@data-total-page&quot;).extract()
paginator = response.xpath(&quot;//div[@class=&#39;paginator&#39;]&quot;).extract()
</code></pre>

<p>上面基础代码的作用主要是为了获取影评页面的底部导航条</p>

<p><img src="https://pic.mylonly.com/2018-07-07-122943.jpg" alt=""/></p>

<p>但是这个导航条会有两种情况获取不到:</p>

<pre><code class="language-text">1. 当某个电影的评论不足20条时，也就是只有一页评论。
2. 当触发了豆瓣的反爬虫的机制时，返回的页面并不是评论页面，而是一个验证页面，自然也找不到导航条
</code></pre>

<p>所以在下面的代码中，我通过这几个变量来判断了以上几种情况：</p>

<pre><code class="language-text">1. 情况1时，不需要继续爬取剩下的评论，直接将current_page和total_page设置为1保存到movie表即可
2. 情况2时，由于此时触发了反爬虫机制，返回的页面没有我们的数据，如果我们直接忽略掉的话，会损失大量的数据（这种情况很常见），所以我们就干脆再试一次，返回request，让Scrapy重新爬取这个页面，因为每次重新爬取都会换一个新的代理IP，所以我们有很大概率下次抓取就是正常的。此处有一点需要注意：因为Scrapy默认会过滤掉重复请求，所以我们需要在构造Request的时候讲dont_filter参数设置为True,让其不要过滤重复链接。
3. 正常情况时，通过xpath语法获取的下一页评论的链接地址然后构造一个request交给Scrapy继续爬取
</code></pre>

<h4 id="toc_13">影评下载中间件</h4>

<p>上面说过，抓取影评页面时需要通过使用代理IP的方式来达到绕过豆瓣的反爬虫机制，具体代理的设置就需要在DownloadMiddleware中设置</p>

<pre><code class="language-python">class DoubanDownloaderMiddleware(object):
# Not all methods need to be defined. If a method is not defined,
# scrapy acts as if the downloader middleware does not modify the
# passed objects.

ip_list = None

@classmethod
def from_crawler(cls, crawler):
    # This method is used by Scrapy to create your spiders.
    s = cls()
    crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
    return s

def process_request(self, request, spider):
    # Called for each request that goes through the downloader
    # middleware.

    # Must either:
    # - return None: continue processing this request
    # - or return a Response object
    # - or return a Request object
    # - or raise IgnoreRequest: process_exception() methods of
    #   installed downloader middleware will be called

    if self.ip_list is None or len(self.ip_list) == 0:
        response = requests.request(&#39;get&#39;,&#39;http://api3.xiguadaili.com/ip/?tid=555688914990728&amp;num=10&amp;protocol=https&#39;).text
        self.ip_list = response.split(&#39;\r\n&#39;)

    ip = random.choice(self.ip_list)
    request.meta[&#39;proxy&#39;] = &quot;https://&quot;+ip
    print(&quot;当前proxy:%s&quot; % ip)
    self.ip_list.remove(ip)
    return None

def process_response(self, request, response, spider):
    # Called with the response returned from the downloader.
    # Must either;
    # - return a Response object
    # - return a Request object
    # # - or raise IgnoreRequest

    if response.status == 403:
        res = parse.urlparse(request.url)
        res = parse.parse_qs(res.query)
        url = res.get(&#39;r&#39;)
        if url and len(url) &gt; 0 :
            request = request.replace(url=res[&#39;r&#39;][0])
        return request

    return response
</code></pre>

<p>其中主要就要实现两个函数，process_request和process_response，前者是每次爬取页面前Scrapy会调用这个函数，后者则是每次爬取完页面之后调用。<br/>
    在前者方法里，我们通过调用一个在线的代理ip获取接口，获取一个代理IP，然后设置request的proxy属性达到更换代理的功能，当然，你也可以通过文件读取代理IP。<br/>
    在后者的方法里，我们判断了状态码为403的状况，因为这个状态码标识当前的request被反爬虫禁止侦测并禁止了，而我们要做的就是把这个禁止的request地址重新包装下放到Scrapy的爬取队列当中。</p>

<h4 id="toc_14">影评Item</h4>

<pre><code class="language-python">class ReviewItem(scrapy.Item):
id = scrapy.Field()
username = scrapy.Field()
nickname = scrapy.Field()
avatar = scrapy.Field()
movieId = scrapy.Field()
rating = scrapy.Field()
create_time = scrapy.Field()
title = scrapy.Field()
summary = scrapy.Field()
alt = scrapy.Field()
</code></pre>

<p>没啥好说的，想存啥就写啥</p>

<h4 id="toc_15">影评Pipeline</h4>

<pre><code class="language-python">
class ReviewPipeline(object):

    reviewInsert = &#39;&#39;&#39;insert into reviews(id,username,nickname,avatar,summary,title,movieId,rating,create_time,alt) values (&quot;{id}&quot;,&quot;{username}&quot;, &quot;{nickname}&quot;,&quot;{avatar}&quot;, &quot;{summary}&quot;,&quot;{title}&quot;,&quot;{movieId}&quot;,&quot;{rating}&quot;,&quot;{create_time}&quot;,&quot;{alt}&quot;)&#39;&#39;&#39;

    def process_item(self, item, spider):
        sql_insert = self.reviewInsert.format(
            id=item[&#39;id&#39;],
            username=pymysql.escape_string(item[&#39;username&#39;]),
            nickname=pymysql.escape_string(item[&#39;nickname&#39;]),
            avatar=pymysql.escape_string(item[&#39;avatar&#39;]),
            summary=pymysql.escape_string(item[&#39;summary&#39;]),
            title=pymysql.escape_string(item[&#39;title&#39;]),
            rating=item[&#39;rating&#39;],
            movieId=item[&#39;movieId&#39;],
            create_time=pymysql.escape_string(item[&#39;create_time&#39;]),
            alt=pymysql.escape_string(item[&#39;alt&#39;])
        )
        print(&quot;SQL:&quot;, sql_insert)
        self.cursor.execute(sql_insert)
        return item

    def open_spider(self, spider):
        self.connect = pymysql.connect(&#39;localhost&#39;,&#39;root&#39;,&#39;******&#39;,&#39;douban&#39;, charset=&#39;utf8&#39;, use_unicode=True)
        self.cursor = self.connect.cursor()
        self.connect.autocommit(True)


    def close_spider(self, spider):
        self.cursor.close()
        self.connect.close()
</code></pre>

<pre><code class="language-text">和之前的电影的pipeline类似，就是基本的数据库写操作。
</code></pre>

<h4 id="toc_16">运行爬虫</h4>

<pre><code class="language-shell">scrapy crawl review
</code></pre>

<p>在我写完这篇文章时，影评的爬虫仍然还在爬取当中：<br/>
<img src="https://pic.mylonly.com/2018-07-06-113420.png" alt=""/><br/>
查看数据库，已经有97W的数据了:</p>

<p><img src="https://pic.mylonly.com/2018-07-06-113210.png" alt=""/><br/>
<img src="https://pic.mylonly.com/2018-07-06-113607.png" alt=""/></p>

<p><em>如果你觉得我的文章对你有帮助，请赞助一杯☕️</em></p>

<p><img src="https://pic.mylonly.com/2018-07-06-IMG_2094-1.JPG" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MySql 相关问题]]></title>
    <link href="https://blog.mylonly.com/15307740267953.html"/>
    <updated>2018-07-05T15:00:26+08:00</updated>
    <id>https://blog.mylonly.com/15307740267953.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>查看数据库服务器字符集</p>
<pre><code class="language-text">mysql&gt; show variables like &#39;%char%&#39;;
+--------------------------+-------------------------------------+------<br/>
| Variable_name | Value |......<br/>
+--------------------------+-------------------------------------+------<br/>
| character_set_client | utf8 |...... -- 客户端字符集<br/>
| character_set_connection | utf8 |......<br/>
| character_set_database | utf8 |...... -- 数据库字符集<br/>
| character_set_filesystem | binary |......<br/>
| character_set_results | utf8 |......<br/>
| character_set_server | utf8 |...... -- 服务器字符集<br/>
| character_set_system | utf8 |......<br/>
| character_sets_dir | D:\MySQL Server 5.0\share\charsets\ |......<br/>
+--------------------------+-------------------------------------+------
</code></pre></li>
<li><p>查看数据表(table)字符集</p>
<pre><code class="language-text">mysql&gt; show table status from sqlstudy_db like &#39;%countries%&#39;;
+-----------+--------+---------+------------+------+-----------------+------<br/>
| Name | Engine | Version | Row_format | Rows | Collation |......<br/>
+-----------+--------+---------+------------+------+-----------------+------<br/>
| countries | InnoDB | 10 | Compact | 11 | utf8_general_ci |......<br/>
+-----------+--------+---------+------------+------+-----------------+------
</code></pre></li>
<li><p>查看数据列(column)字符集</p>
<pre><code class="language-text">mysql&gt; show full columns from countries;
+----------------------+-------------+-----------------+--------<br/>
| Field | Type | Collation | .......<br/>
+----------------------+-------------+-----------------+--------<br/>
| countries_id | int(11) | NULL | .......<br/>
| countries_name | varchar(64) | utf8_general_ci | .......<br/>
| countries_iso_code_2 | char(2) | utf8_general_ci | .......<br/>
| countries_iso_code_3 | char(3) | utf8_general_ci | .......<br/>
| address_format_id | int(11) | NULL | .......<br/>
+----------------------+-------------+-----------------+--------
</code></pre></li>
<li><p>修改数据库(database)，数据表(table)，数据列(column)字符集</p>
<pre><code class="language-text">alter database name character set utf8;
create database name character set utf8;<br/>
alter table 表名 convert to character set gbk;<br/>
alter table 表名 modify column &#39;字段名&#39; varchar(30) character set gbk not null;
</code></pre></li>
<li><p>通过数据库my.cnf配置文件设置字符集</p>
<pre><code class="language-text">vi /etc/my.cnf
#在[client]下添加<br/>
default-character-set=utf8<br/>
#在[mysqld]下添加<br/>
default-character-set=utf8
</code></pre>
<p>重启mysql</p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[制作Kali Linux 加密U盘启动盘]]></title>
    <link href="https://blog.mylonly.com/15278621156583.html"/>
    <updated>2018-06-01T22:08:35+08:00</updated>
    <id>https://blog.mylonly.com/15278621156583.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">前置条件</h2>

<ol>
<li>1个至少是32GB的U盘，最好是USB3.0接口的</li>
<li>一台可以安装Virtual Box的电脑</li>
</ol>

<h2 id="toc_1">需要下载的文件</h2>

<ol>
<li>Kali Linux 64位 iso镜像文件: <a href="https://www.kali.org/downloads/">下载地址</a></li>
<li>Virtual Box 安装包 <a href="https://www.virtualbox.org/wiki/Downloads">下载地址</a></li>
<li>Kali Linux OVA镜像文件(OVA格式) <a href="https://www.offensive-security.com/kali-linux-vm-vmware-virtualbox-hyperv-image-download/">下载地址</a></li>
<li>Virtual Box Extension Pack <a href="https://www.virtualbox.org/wiki/Downloads">下载地址</a></li>
</ol>

<h2 id="toc_2">安装Kali Linux 虚拟机</h2>

<p>我们需要借助Kali里面的一些工具来帮助我们制作加密U盘，所以我们需要首先安装一个Kali Linux虚拟机，如果你已经有了类似的虚拟机，可以忽略这个步骤。</p>

<p>利用安装包安装完VirtualBox之后：</p>

<ol>
<li>在菜单项<code>管理</code>当中选择<code>导入虚拟电脑</code>，然后选中之前下载好的Kali的OVA格式虚拟机镜像文件。</li>
<li>安装Virtual Box Extension Pack，这步主要是让虚拟机能支持USB3.0的设备</li>
<li>修改已经导入的Kali虚拟机的设置，在USB设备选项中选择<code>USB 3.0 控制器</code>
<img src="https://pic.mylonly.com/2018-06-01-589FAFA6C598ACDFD296B6D320D525C7.jpg" alt=""/></li>
<li>启动Kali虚拟机，进入Kali之后，点击VirtualBox的<code>设备</code>菜单项，选择<code>安装增强功能</code>,如果提示安装失败，可以直接将桌面上出现的光盘中的VBoxLinuxAdditions.run 拷贝至其他目录，修改其权限为755，然后手动运行此脚本即可。
<img src="https://pic.mylonly.com/2018-06-01-151200.png" alt=""/></li>
<li>设置共享文件夹，在VirtualBox当中操作，这步主要是方便后面将下载在windows里的Kali的iso文件拷贝至Kali当中。
<img src="https://pic.mylonly.com/2018-06-01-56483D9C90D387A88824988AFDE2DCF1.jpg" alt=""/></li>
</ol>

<h2 id="toc_3">U盘初始设置</h2>

<ol>
<li>选择VirtualBox的<code>设备</code>菜单，在<code>USB</code>子菜单选中已经插在主机上的U盘设备，将U盘映射到虚拟机当中
<img src="https://pic.mylonly.com/2018-06-01-151239.png" alt=""/></li>
<li>找到Kali当中的GParted工具打开，选择已经挂载的U盘(如果你的虚拟机之前没有挂载过其他U盘,默认的挂载分区应该是<code>/dev/sdb</code>,本文演示截图当中显示为/dev/sdc是因为之前已经挂载过一个U盘)
<img src="https://pic.mylonly.com/2018-07-07-123121.jpg" alt=""/></li>
<li><p>然后先将U盘卸载，然后删除分区（记得执行顶部工具栏的回车样式的按钮）<br/>
<img src="https://pic.mylonly.com/2018-07-07-123132.jpg" alt=""/></p>
<h2 id="toc_4">写入镜像、U盘分区</h2></li>
<li><p>将之前共享文件夹里已经下载好的Kali的iso文件拷贝至Kali当中</p></li>
<li><p>利用如下命令将该iso文件拷贝至U盘当中</p></li>
</ol>

<pre><code class="language-text">dd if=kali-linux-2018.2-amd64.iso of=/dev/sdc bs=1M  
</code></pre>

<ol>
<li>利用parted工具对U盘当中未使用的其他部分进行分区，命令如下</li>
</ol>

<pre><code class="language-bash">parted  #进入parted界面
select /dev/sdc #选择U盘分区
print #查看当前分区信息
mkpart primary 2937 23417 #制作新的分区，起始位置从2937M开始，到23417位置结束
print #查看新的分区信息
</code></pre>

<p><img src="https://pic.mylonly.com/2018-06-01-152223.jpg" alt=""/></p>

<h2 id="toc_5">制作加密U盘</h2>

<ol>
<li><p>制作加密分区</p>
<pre><code class="language-text">cryptsetup --verbose --verify-passphrase luksFormat /dev/sdc3 
</code></pre>
<p>确认覆盖分区，输入大写的YES，然后输入两遍密码之后，加密分区制作完毕<br/>
<strong>注意/dev/sdc3,在上面利用parted制作新分区时，其属于/dev/sdc下面的第三个分区</strong><br/>
<img src="https://pic.mylonly.com/2018-06-01-15A3CBD2C0856CCC83089E164F496957.png" alt=""/></p></li>
<li><p>对加密分区格式化分区,分配卷标</p>
<pre><code class="language-bash">cryptsetup luksOpen /dev/sdc3 usb  #打开加密的/dev/sdc3分区至usb文件
ls /dev/mapper/usb #上面的命令执行完毕之后会在/dev/mapper目录下生成一个usb文件<br/>
mkfs.ext4 /dev/mapper/usb #将打开的分区格式化成ext4格式<br/>
e2label /dev/mapper/usb persistence #将分区卷标指定为persistence,名字必须为persistence
</code></pre></li>
<li><p>挂载新的分区,写入验证文件</p>
<pre><code class="language-bash">mkdir -p /mnt/usb 
mount /dev/mapper/usb /mnt/usb #将之前格式化的usb设备挂载到/mnt/usb目录下<br/>
echo &quot;/ union&quot; &gt; /mnt/usb/persistence.conf<br/>
#写入一个persistence.conf文件，此文件会在启动时用来确认此U盘是用来加密存储的分区
</code></pre></li>
<li><p>卸载分区，退出加密分区</p>
<pre><code class="language-bash">umount /dev/mapper/usb
cryptsetup luksClose /dev/mapper/usb
</code></pre>
<p><img src="https://pic.mylonly.com/2018-07-07-123146.jpg" alt=""/></p></li>
</ol>

<h2 id="toc_6">重启物理电脑，选择从U盘启动</h2>

<p>进入Boot Menu之后，选择LIve USB Encrypted Persistence 进入系统，之后会让你输入之前创建加密分区时输入的密码，密码输入正确才能正确的进入系统，至此，整个制作加密Kali Linux U盘启动盘过程结束。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[uwsgi重启shell脚本]]></title>
    <link href="https://blog.mylonly.com/15277767431151.html"/>
    <updated>2018-05-31T22:25:43+08:00</updated>
    <id>https://blog.mylonly.com/15277767431151.html</id>
    <content type="html"><![CDATA[
<pre><code class="language-text">#!/bin/bash
if [ ! -n &quot;$1&quot; ]
then
    echo &quot;Usages: sh uwsgiserver.sh [start|stop|restart]&quot;
    exit 0
fi

if [ $1 = start ]
then
    psid=`ps aux | grep &quot;uwsgi&quot; | grep -v &quot;grep&quot; | wc -l`
    if [ $psid -gt 4 ]
    then
        echo &quot;uwsgi is running!&quot;
        exit 0
    else
        uwsgi /etc/uwsgi.ini
        echo &quot;Start uwsgi service [OK]&quot;
    fi
    

elif [ $1 = stop ];then
    killall -9 uwsgi
    echo &quot;Stop uwsgi service [OK]&quot;
elif [ $1 = restart ];then
    killall -9 uwsgi
    /usr/bin/uwsgi --ini /etc/uwsgi.ini #修改成自己业务的配置文件或命令
    echo &quot;Restart uwsgi service [OK]&quot;

else
    echo &quot;Usages: sh uwsgiserver.sh [start|stop|restart]&quot;
fi
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux问题集锦]]></title>
    <link href="https://blog.mylonly.com/15277762940932.html"/>
    <updated>2018-05-31T22:18:14+08:00</updated>
    <id>https://blog.mylonly.com/15277762940932.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>CentOS python升级之后yum无法使用解决</p>
<p>yum是基于python2.6.6实现的，修改办法如下,编辑yum脚本</p>
<pre><code class="language-text">vim /usr/bin/yum
</code></pre>
<p>将文件头部的<code>#!/usr/bin/python</code>修改为<code>#!/usr/bin/python2.6.6</code></p></li>
<li><p>Supervisor 只能监控处于前台状态的程序，如果程序是后台状态，需要改为前台状态</p>
<pre><code class="language-text">[program:ErosUpdate]
command=/root/.local/share/virtualenvs/ErosUpdate-itO0ZcNZ/bin/uwsgi -i /data/weex/projects/ErosUpdate/uwsgi.ini           ; the program (relative uses PATH, can take args)<br/>
;priority=999                ; the relative start priority (default 999)<br/>
autostart=true              ; start at supervisord start (default: true)<br/>
autorestart=true            ; retstart at unexpected quit (default: true)<br/>
startsecs=10                ; number of secs prog must stay running (def. 10)<br/>
startretries=3              ; max # of serial start failures (default 3)<br/>
nodaemon=true<br/>
;exitcodes=0,2               ; &#39;expected&#39; exit codes for process (default 0,2)<br/>
;stopsignal=QUIT             ; signal used to kill process (default TERM)<br/>
;stopwaitsecs=10             ; max num secs to wait before SIGKILL (default 10)<br/>
user=root                 ; setuid to this UNIX account to run the program<br/>
log_stdout=true             ; if true, log program stdout (default true)<br/>
log_stderr=true             ; if true, log program stderr (def false)<br/>
logfile=/data/logs/supervisord/ErosUpdate.log    ; child log path, use NONE for none; default AUTO<br/>
logfile_maxbytes=50MB        ; max # logfile bytes b4 rotation (default 50MB)<br/>
logfile_backups=20          ; # of logfile backups (default 10)
</code></pre></li>
<li><p>uwsgi 修改为前台运行，只需要去掉<code>daemonize</code>配置项即可</p>
<pre><code class="language-text">[uwsgi]
socket=127.0.0.1:3030<br/>
chdir=/data/weex/projects/ErosUpdate<br/>
wsgi-file = ErosUpdate/wsgi.py<br/>
max-requests=5000<br/>
processes = 4<br/>
threads = 4<br/>
logto=/data/logs/uwsgi/ErosUpdate.log<br/>
#daemonize=/data/logs/uwsgi/ErosUpdate.log<br/>
pidfile=/tmp/ErosUpdate.pid
</code></pre></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pyplot相关]]></title>
    <link href="https://blog.mylonly.com/15277545119360.html"/>
    <updated>2018-05-31T16:15:11+08:00</updated>
    <id>https://blog.mylonly.com/15277545119360.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>subplot</p>
<pre><code class="language-text">plt.subplot(221) # 表示分成两行两列，占用第一个，即第一行第一列的子图
plt.subplot(222) # 表示分成两行两列，占用第二个，即第一行第二列的子图<br/>
plt.subplot(212) # 表示分成两行一列，占用第二个，即第二行第一列的子图
</code></pre>
<p>subplot(numRows, numCols, plotNum)<br/>
如果3个参数都小于10的话，可以统一成一个整数<br/>
subplot(221) = subplot(2,2,1)</p></li>
<li><p><img src="media/15277545119360/15277557443176.jpg" alt=""/></p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pandas相关]]></title>
    <link href="https://blog.mylonly.com/15276700222671.html"/>
    <updated>2018-05-30T16:47:02+08:00</updated>
    <id>https://blog.mylonly.com/15276700222671.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>Pandas 读取列数据</p>
<pre><code class="language-python">city_names = pd.Series([&#39;San Francisco&#39;, &#39;San Jose&#39;, &#39;Sacramento&#39;])
population = pd.Series([852469, 1015785, 485199])<br/>
data = pd.DataFrame({ &#39;City name&#39;: city_names, &#39;Population&#39;: population })<br/>
data[&#39;City_Name&#39;]  #获取&#39;City_Name&#39;这个列对象，返回值类型为Series<br/>
data[[&#39;City_Name&#39;]] #获取&#39;City_Name&#39;列包含的数据，返回值类型为DateFrame
</code></pre></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tensorflow错误集锦]]></title>
    <link href="https://blog.mylonly.com/15274889837567.html"/>
    <updated>2018-05-28T14:29:43+08:00</updated>
    <id>https://blog.mylonly.com/15274889837567.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA<br/>
加入以下代码</p>
<pre><code class="language-text">import os 
os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;2&#39; 
</code></pre></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Weex 问题集锦]]></title>
    <link href="https://blog.mylonly.com/15253577270506.html"/>
    <updated>2018-05-03T22:28:47+08:00</updated>
    <id>https://blog.mylonly.com/15253577270506.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>weex中通过ref访问组件的两种情况:</p>
<pre><code class="language-text">&lt;div ref=&quot;test&quot;&gt;&lt;/div&gt;
</code></pre>
<p>这种直接写入的可以通过this.$refs[&#39;test&#39;]获取到组件对象</p>
<pre><code class="language-text">&lt;div ref=&quot;&#39;test&#39;+index&quot; v-for=&quot;(item,index) in items&quot;&gt;&lt;/div&gt;
</code></pre>
<p>上面这种通过v-for或者其他vue语法动态嵌入的组件，则需要通过this.\(refs[`test\){index}<code>][0]去获取，因为淡出的this.$refs[&quot;</code>test${index}`&quot;]获取到的是包含一个元素的数组对象</p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[解决VSCode编写Django代码时经常提示objects属性等不存在的错误]]></title>
    <link href="https://blog.mylonly.com/15249709962104.html"/>
    <updated>2018-04-29T11:03:16+08:00</updated>
    <id>https://blog.mylonly.com/15249709962104.html</id>
    <content type="html"><![CDATA[
<p>如果你仅仅是装了pylint用来检测python代码，那么你在写django程序，尤其是使用model的一些查询语句时，如</p>

<pre><code class="language-text">App.objects.all()
</code></pre>

<p>肯定会经常会被VSCode提示<code>App 没有objects这个属性</code>，虽然这个不影响代码的运行，但作为有强迫症的我们，怎么能容忍我们的代码还没运行就被标识为错误,实在是很影响心情。</p>

<blockquote>
<p>Django使用了大量的元编程思想，其中会有大量的修改对象属性和行为的操作，pylint提示的不存在的属性和方法会在程序运行中被django动态的加入，所以并不会影响程序运行。</p>
</blockquote>

<p>所以在网上稍微找了下，发现这个叫做<code>pylint-django</code>的<code>pylint</code>的插件可以去掉这些恼人的提示。</p>

<p>安装很简单,和pylint一样</p>

<pre><code class="language-text">pip3 install pylint-django
</code></pre>

<p>然后通过pylint加载这个插件</p>

<pre><code class="language-text">pylint --load-plugins pylint_django 
</code></pre>

<p>在VSCode里可以通过修改setting中的<code>python.linting.pylintArgs</code>这个键的值达到同样的目的</p>

<pre><code class="language-text">&quot;python.linting.pylintArgs&quot;: [&quot;--load-plugins&quot;, &quot;pylint_django&quot;]
</code></pre>

<p>然后重启VSCode就好了</p>

<p>参考:<br/>
<a href="https://blog.landscape.io/using-pylint-on-django-projects-with-pylint-django.html">blog.landscape.io</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[nginx 相关技巧]]></title>
    <link href="https://blog.mylonly.com/15247238721302.html"/>
    <updated>2018-04-26T14:24:32+08:00</updated>
    <id>https://blog.mylonly.com/15247238721302.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>防止域名被人恶意指向，由于nginx存在默认的空主机头问题，可以通过添加如下配置，将未配置的域名强行重定向，或者return 404</p>
<pre><code class="language-text">  server {
      listen 80 default;<br/>
      server_name _;<br/>
      rewrite ^(.*) http://www.mylonly.com permanent;<br/>
    }
</code></pre></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Django问题集锦]]></title>
    <link href="https://blog.mylonly.com/15244672126240.html"/>
    <updated>2018-04-23T15:06:52+08:00</updated>
    <id>https://blog.mylonly.com/15244672126240.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>Csrf验证</p>
<pre><code class="language-text">需要在表单中加入`csrfmiddlewaretoken`或者在http头中`X-CSRFToken`请求头，具体值来自cookie当中的`csrftoken`
</code></pre></li>
<li><p>Django允许跨域</p>
<p>安装<code>django-cors-headers</code></p>
<pre><code class="language-text">pip install django-cors-headers
</code></pre>
<p>在setting.py中增加如下代码</p>
<pre><code class="language-text">INSTALLED_APPS = [
...<br/>
&#39;corsheaders&#39;，<br/>
...<br/>
 ] <br/>
MIDDLEWARE_CLASSES = (<br/>
    ...<br/>
    &#39;corsheaders.middleware.CorsMiddleware&#39;,<br/>
    &#39;django.middleware.common.CommonMiddleware&#39;, # 注意顺序<br/>
    ...<br/>
)<br/>
#跨域增加忽略<br/>
CORS_ALLOW_CREDENTIALS = True<br/>
CORS_ORIGIN_ALLOW_ALL = True<br/>
CORS_ORIGIN_WHITELIST = (<br/>
    &#39;*&#39;<br/>
)<br/>
CORS_ALLOW_METHODS = (<br/>
    &#39;DELETE&#39;,<br/>
    &#39;GET&#39;,<br/>
    &#39;OPTIONS&#39;,<br/>
    &#39;PATCH&#39;,<br/>
    &#39;POST&#39;,<br/>
    &#39;PUT&#39;,<br/>
    &#39;VIEW&#39;,<br/>
)<br/>
CORS_ALLOW_HEADERS = (<br/>
    &#39;XMLHttpRequest&#39;,<br/>
    &#39;X_FILENAME&#39;,<br/>
    &#39;accept-encoding&#39;,<br/>
    &#39;authorization&#39;,<br/>
    &#39;content-type&#39;,<br/>
    &#39;dnt&#39;,<br/>
    &#39;origin&#39;,<br/>
    &#39;user-agent&#39;,<br/>
    &#39;x-csrftoken&#39;,<br/>
    &#39;x-requested-with&#39;,<br/>
    &#39;Pragma&#39;,<br/>
)
</code></pre></li>
<li><p>axios 默认支持csrf功能，需要修改其属性和服务器返回的csrfcookiename一致</p>
<pre><code class="language-text">#Django返回的cookie中的csrf字段名字为csrftoken,会从request的header中读取X-CSRFTOKEN来校验csrf
axios.defaults.xsrfHeaderName = &quot;X-CSRFTOKEN&quot;;<br/>
axios.defaults.xsrfCookieName = &quot;csrftoken&quot;;
</code></pre>
<blockquote>
<p>如果axios要访问的接口和前端页面属于跨域状态，那么axios无法读取xsrfCookie,则也无法设置xsrfHeader,此时则需要后端将csrftoken放入response中而不是cookie当中，axios在request阻截器中手动设置xsrfHeader</p>
</blockquote>
<p><img src="media/15244672126240/15258764072402.jpg" alt=""/></p></li>
<li><p>Django使用django-rest-framework 单独某个view取消csrf验证</p>
<p>需要两步：</p>
<ul>
<li>关掉django的csrf验证,利用<code>@csrf_exempt</code>(针对function view)，或者用<code>@method_decorator(csrf_exempt, name=&quot;dispatch&quot;)</code>(针对class view)</li>
<li>关掉drf框架的验证，在class view 中将<code>authentication_classes</code>设置为<code>BasicAuthentication</code></li>
</ul>
<pre><code class="language-text">@method_decorator(csrf_exempt, name=&quot;dispatch&quot;)
class login(views.APIView):<br/>
authentication_classes = (BasicAuthentication,)<br/>
def post(self, request, *args, **kwargs):<br/>
username = request.data.get(&quot;username&quot;)<br/>
password = request.data.get(&quot;password&quot;)<br/>
user = auth.authenticate(request, username=username, password=password)<br/>
if user is not None:<br/>
  auth.login(request, user)<br/>
  return ErosResponse()<br/>
else:<br/>
  return ErosResponse(status=ErosResponseStatus.INVALID_USER)
</code></pre></li>
<li><p>使用<code>OrderFilter</code>过程中遇到<code>OrderingFilter object has no attribute &#39;filter_queryset&#39;</code>:</p>
<p>需要使用从rest_framework.filters中导入的OrderingFilter，而不是django_filters</p>
<pre><code class="language-python">from django_filters.rest_framework import DjangoFilterBackend
from rest_framework.filters import OrderingFilter
</code></pre></li>
<li><p>Django链接Mysql 8.0 出现错误(1045:Access denied for user &#39;root&#39;@&#39;localhost&#39; (using password: NO) 的一种解决方法</p>
<p>出现此错误的原因是MySQL8.0 密码的加密方式发生了改变，采用了cha2加密方法，我们可以利用下面的语句将MySQL的密码加密方式修改为之前的版本</p>
<pre><code class="language-text">mysql -u root -p
use mysql；<br/>
ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED WITH mysql_native_password BY &#39;newpassword&#39;;  <br/>
FLUSH PRIVILEGES; 
</code></pre></li>
<li><p>Django-Filter自定义查询过滤字段</p>
<pre><code class="language-python">from django_filters import FilterSet
from django_filters import CharFilter<br/>
class TagFilter(FilterSet):<br/>
    ##自定义search字段<br/>
    search = CharFilter(name=&quot;name&quot;, lookup_expr=&quot;contains&quot;)<br/>
    class Meta:<br/>
        model = Tag<br/>
        fields = {<br/>
          &#39;id&#39;: [&#39;exact&#39;],<br/>
          &#39;name&#39;: [&#39;exact&#39;, &#39;contains&#39;],<br/>
          &#39;search&#39;: [&#39;exact&#39;],<br/>
        }
</code></pre>
<p>具体Django-Filter的用法可以<a href="http://django-filter.readthedocs.io/en/1.1.0/">参考文档</a></p></li>
<li><p>DRF的OrderingFilter默认排序</p>
<pre><code class="language-python">filter_backends = (DjangoFilterBackend, OrderingFilter)
ordering_fields = (&#39;id&#39;, &#39;hot&#39;)<br/>
ordering = (&#39;-hot&#39;,)
</code></pre></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python 错误集锦]]></title>
    <link href="https://blog.mylonly.com/15244545128606.html"/>
    <updated>2018-04-23T11:35:12+08:00</updated>
    <id>https://blog.mylonly.com/15244545128606.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>ValueError: unknown locale: UTF-8 <br/>
<img src="https://pic.mylonly.com/2018-04-23-15244561735556.jpg" alt=""/></p>
<p>添加下面代码至用户目录的.bash_profile文件   </p>
<pre><code class="language-bash">export LANG=&quot;en_US.UTF-8&quot;
export LC_COLLATE=&quot;en_US.UTF-8&quot;<br/>
export LC_CTYPE=&quot;en_US.UTF-8&quot;<br/>
export LC_MESSAGES=&quot;en_US.UTF-8&quot;<br/>
export LC_MONETARY=&quot;en_US.UTF-8&quot;<br/>
export LC_NUMERIC=&quot;en_US.UTF-8&quot;<br/>
export LC_TIME=&quot;en_US.UTF-8&quot;<br/>
export LC_ALL=
</code></pre></li>
<li><p>zip()函数在Python2和Python3中的区别</p>
<p>在Python2中，zip返回的是元祖的列表<br/>
而在Python3中，zip返回的是元祖组成的迭代器，得套个list函数才能将其转换为列表</p>
<pre><code class="language-text">x = [1, 2, 3]
y = [a, b, c]<br/>
#python3<br/>
n = list(zip(x,y))<br/>
print(n)<br/>
#python2<br/>
n = zip(x,y)<br/>
print n
</code></pre></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[利用GitLab提供的GitLab-CI以及GitLab-Runner搭建持续集成/部署环境]]></title>
    <link href="https://blog.mylonly.com/15241105353902.html"/>
    <updated>2018-04-19T12:02:15+08:00</updated>
    <id>https://blog.mylonly.com/15241105353902.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">简介</h2>

<ol>
<li><p>GitLab</p>
<blockquote>
<p>是一套基于Ruby开发的开源Git项目管理应用，其提供的功能和Github类似，不同的是GitLab提供一个GitLab CE社区版本，用户可以将其部署在自己的服务器上，这样就可以用于团队内部的项目代码托管仓库。</p>
</blockquote></li>
<li><p>GitLab CI</p>
<blockquote>
<p>是GitLab 提供的持续集成服务(从8.0版本之后，GitLab CI已经集成在GitLab中了)，只要在你的仓库根目录下创建一个.gitlab-ci.yml 文件， 并为该项目指派一个Runner，当有合并请求或者Push操作时，你写在.gitlab-ci.yml中的构建脚本就会开始执行。</p>
</blockquote></li>
<li><p>GitLab Runner</p>
<blockquote>
<p>是配合GitLab CI进行构建任务的应用程序，GitLab CI负责yml文件中各种阶段流程的执行，而GitLab Runner就是具体的负责执行每个阶段的脚本执行，一般来说GitLab Runner需要安装在单独的机器上通过其提供的注册操作跟GitLab CI进行绑定，当然，你也可以让其和GitLab安装在一起，只是有的情况下，你代码的构建过程对资源消耗十分严重的时候，会拖累GitLab给其他用户提供政策的Git服务。</p>
</blockquote></li>
<li><p>持续集成/部署环境</p>
<blockquote>
<p>持续集成是程序开发人员在频繁的提交代码之后，能有相应的环境能对其提交的代码自动执行构建(Build)、测试(Test),然后根据测试结果判断新提交的代码能否合并加入主分支当中,而持续部署也就是在持续集成之后自动将代码部署(Deploy)到生成环境上</p>
</blockquote></li>
</ol>

<h2 id="toc_1">开启GitLab CI功能</h2>

<p>在GitLab 8.0版本之后,你可以通过如下两部启用GitLab CI功能</p>

<ol>
<li> 新建一个<code>.gitlab-ci.yml</code>文件在你项目的根目录</li>
<li> 为你的项目配置一个GitLab Runner</li>
</ol>

<h4 id="toc_2">配置一个<code>.gitlab-ci.yml</code>文件</h4>

<p><code>.gitlab-ci.yml</code>文件是用来配置GitLab CI进行构建流程的配置文件，其采用YAML语法,所以你需要额外注意要用空格来代替缩进，而不是Tabs。下面通过我自己项目中的<code>.gitlab-ci.yml</code>文件来具体介绍其规则</p>

<pre><code class="language-yaml">stages:
  - init
  - check
  - build
  - deploy

cache:
  key: ${CI_BUILD_REF_NAME}
  paths:
  - node_modules/
  - dist/

#定义一个叫init的Job任务
init:
  stage: init
  script:
  -  cnpm install

#master_check Job:检查master分支上关键内容
master_check:
  stage: check
  script:
  - echo &quot;Start Check Eros Config ...&quot;
  - bash check.sh release 
  only:
  - master
  
#dev_check Job: 检查dev分支上关键内容
dev_check:
  stage: check
  script:
  - echo &quot;Start Check Eros Config ...&quot;
  - bash check.sh debug
  only:
  - dev

js_build:
  stage: build
  script:
  - eros build

master_deploy:
  stage: deploy
  script:
  - bash deploy.sh release
  only:
  - master

dev_deploy:
  stage: deploy
  script:
  - bash deploy.sh debug
  only:
  - dev

</code></pre>

<p>在上面的例子中，我们利用<code>stages</code>关键字来定义持续构建过程中的四个阶段init、chec、build、deploy</p>

<blockquote>
<p>关于GitLab CI中的stages,有如下几个特点:</p>
</blockquote>

<pre><code class="language-text">1. 所有 Stages 会按照顺序运行，即当一个 Stage 完成后，下一个 Stage 才会开始
2. 只有当所有 Stages 完成后，该构建任务 (Pipeline) 才会成功
3. 如果任何一个 Stage 失败，那么后面的 Stages 不会执行，该构建任务 (Pipeline) 失败
</code></pre>

<p>然后我们利用<code>caches</code>字段来指定下面将要进行的job任务中需要共享的文件目录,如果没有，每个Job开始的时候，GitLab Runner都会删掉<code>.gitignore</code>里面的文件</p>

<p>紧接着，我们定义了一个叫做<code>init</code>的job，其通过stage字段声明属于<code>init</code>阶段，因此，这个job会第一个执行，我们在这个job当中，执行一些环境的初始化工作。</p>

<p>接下来是<code>check</code>阶段,用来检查代码的一些基础错误(代码规范之类不会被编译器发现的问题)，以及一些配置文件的检查，我将其命名为<code>master_check</code>和<code>dev_check</code>,通过only字段来告诉GitLab CI 只有当对应的分支有push操作的时候才会触发这个job。</p>

<p>然后就是代码的<code>build</code>阶段，由于此阶段不像上个极端，没有需要区分不同分支的命令，所以就只需要定义一个job就够了</p>

<p>最后的<code>deploy</code>，因为不同的分支需要发布到不同的环境，所以依然通过only来区分两个job。</p>

<blockquote>
<p>关于GitLab CI中的Jobs,也有如下几个特点:</p>
</blockquote>

<pre><code class="language-text">1. 相同 Stage 中的 Jobs 会并行执行
2. 相同 Stage 中的 Jobs 都执行成功时，该 Stage 才会成功
3. 如果任何一个 Job 失败，那么该 Stage 失败，即该构建任务 (Pipeline) 失败
</code></pre>

<p>在我的这个构建任务当中，根据我的业务情况只用到了少许关键字,还有更多的类似于<code>before_script</code>、<code>after_script</code>等关键字，具体的可以参阅<a href="https://docs.gitlab.com/ce/ci/yaml/README.html">GitLab的官方文档</a></p>

<p>在我们完成<code>.gitlab-ci.yml</code>的流程编写之后，就可以将其放在项目的根目录下，然后push到我们的GitLab上，这时，如果你打开项目首页的Piplines标签页，会发现一个状态标识为<code>pending</code>的构建任务，如下图所示：<br/>
<img src="https://pic.mylonly.com/2018-04-19-075009.jpg" alt=""/><br/>
这时由于这个构建任务还有找到可用的GitLab Runner来执行其构建脚本，等我们接下来为我们的项目接入GitLab Runner之后，这些任务的状态就会由<code>pendding</code>变成<code>running</code>了</p>

<h4 id="toc_3">安装GitLab Runner</h4>

<p>找一台适合安装GitLab Runner的机器，无论是Windows或者Mac还是Linux都行，最好是那种比较空闲的能24小时开启的机器，我们在<a href="https://docs.gitlab.com/runner/install/">GitLab Runner的官网</a>找到我们平台的安装文件，以及对应的安装流程。由于笔者我准备安装GitLab Runner是一台闲置的iMac电脑，因此我就在演示MacOS下GitLab Runner的安装：</p>

<blockquote>
<p>GitLab Runner 在macOS和Linux/UNIX下安装流程是一样的，都是直接下载已编译好的二进制包</p>
</blockquote>

<ol>
<li><p>下载对应GitLab 版本的GitLab Runner</p>
<ul>
<li>如果你的GitLab是10.0之后的版本，GitLab Runner可执行文件改名为<code>gitlab-runner</code></li>
</ul>
<pre><code class="language-text">sudo curl --output /usr/local/bin/gitlab-runner https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-darwin-amd64
sudo chmod +x /usr/local/bin/gitlab-runner
</code></pre>
<ul>
<li>9.0~10.0之间的版本</li>
</ul>
<pre><code class="language-text">sudo curl --output /usr/local/bin/gitlab-ci-multi-runner https://gitlab-ci-multi-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-ci-multi-runner-darwin-amd64
sudo chmod +x /usr/local/bin/gitlab-ci-multi-runner
</code></pre>
<ul>
<li>9.0 之前,由于9.0之后启用全新的API4接口，所以如果你的GitLab是9.0以前的版本,需要下载下面的版本,否则会导致你的GitLab Runner注册不上</li>
</ul>
<pre><code class="language-text">sudo curl --output /usr/local/bin/gitlab-ci-multi-runnerhttps://gitlab-ci-multi-runner-downloads.s3.amazonaws.com/v1.11.1/binaries/gitlab-ci-multi-runner-darwin-amd64
sudo chmod +x /usr/local/bin/gitlab-ci-multi-runner
</code></pre>
<blockquote>
<p>不知道各位有没有注意到上面下载地址链接当中的v1.11.1,这个就是对应的Gitlab Runner,如果你的GitLab是9.0之前的版本，使用GitLab Runner v1.11.1这个版本仍然注册不上，可以尝试使用降几个版本的GitLab Runner,所有GitLab Runner发行的版本可以在<a href="https://gitlab.com/gitlab-org/gitlab-runner/tags">GitLab Runner Tags</a>找到</p>
</blockquote>
<p>假如你遇到不能通过登录服务器来确定GitLab版本号时，可以通过直接访问gitlab的首页，后面加上help，如下图：<br/>
<img src="https://pic.mylonly.com/2018-04-19-083100.png" alt=""/></p></li>
<li><p>注册GitLab Runner</p>
<p>执行下面命令，</p>
<pre><code class="language-text">sudo gitlab-ci-multi-runner register
</code></pre>
<p>如果你的终端提示找不到命令，请通过<code>export PATH=/usr/local/bin:$PATH</code>将/usr/local/bin目录加入环境变量,或者你遗漏了上面的chmod命令导致文件不可执行。</p>
<p>执行完上面的命令之后，会让你输入下面的信息:</p>
<ul>
<li>Please enter the gitlab-ci coordinator URL:</li>
<li>Please enter the gitlab-ci token for this runner:</li>
<li>Please enter the gitlab-ci description for this runner</li>
<li>Please enter the gitlab-ci tags for this runner (comma separated):</li>
<li>Whether to run untagged builds [true/false]:</li>
<li>Please enter the executor:</li>
</ul>
<p>其中coordinator URL和token可以在你需要进行持续集成的项目的Runner标签页中找到<br/>
<img src="https://pic.mylonly.com/2018-04-19-084819.jpg" alt=""/><br/>
description和tags可以自己定义，是否build没有tag的提交这个也是根据你自己的需求来选择，默认是false，executer选择shell</p>
<p>填写完成之后如果提示<code>Registering runner... succeeded</code>表明这个Runner已经被注册成功了，之后你在返回进入项目的Runners页面，会发现下面多了一个处于Active状态的Runner</p>
<p>紧接着最后一步，启动我们刚注册的Runner</p>
<pre><code class="language-text">sudo  gitlab-ci-multi-runner start
</code></pre>
<p>现在，我们切回项目的Pipelines当中，肯定会发现之前处于peding状态的任务已经开始running了，我们可以通过点击这个状态按钮来实时查看每个阶段的输出日志<br/>
<img src="https://pic.mylonly.com/2018-04-19-085612.jpg" alt=""/></p></li>
</ol>

<h2 id="toc_4">开启构建状态的邮件提醒</h2>

<p>如果你想收到关于每个构建任务的实时状态的邮件，你可以在在项目中的service标签野种启用Build Emails这个服务。</p>

<p><img src="https://pic.mylonly.com/2018-04-19-085906.png" alt=""/></p>

<h2 id="toc_5">结语</h2>

<p>搭建一个快速方便的持续集成、持续部署环境对于项目的开发来说是一个很重要的举措，无论你是采用大名鼎鼎的jenkins还是本文介绍的GitLab CI,它不仅仅可以帮助我们节省大量的时间在调试发布部署当中，也减少了我们因为人为因素导致的发布过程中出现的意外，有效的较低了项目开发当中的风险。</p>

<h4 id="toc_6">如果你想和我交流，可以关注我的订阅号:</h4>

<p><img src="https://pic.mylonly.com/2018-04-19-wechat-mylonly.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在wxc-rich-text以及wxc-special-rich-text的思路上实现类似QQ表情的富文本多行排版]]></title>
    <link href="https://blog.mylonly.com/15241067803366.html"/>
    <updated>2018-04-19T10:59:40+08:00</updated>
    <id>https://blog.mylonly.com/15241067803366.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">先上效果图:</h2>

<p><img src="https://pic.mylonly.com/2018-03-30-093130.jpg" alt=""/></p>

<h2 id="toc_1">wxc-rich-text和wxc-special-rich-text的实现思路</h2>

<p>weex-ui中为我们提供了<code>wxc-rich-text</code>和<code>wxc-special-rich-text</code>两种富文本控件，其中，<code>wxc-rich-text</code> 只支持单行富文本显示,而<code>wxc-special-rich-text</code>只能支持最多两行特定种类的图文混排(<code>标签+文本</code>以及<code>图标+文本</code>) </p>

<p>首先我们来分析<code>wxc-rich-text</code>的源码来看看为什么这个控件不支持多行的样式</p>

<p>在<code>wxc-rich-text.vue</code>的代码中，我们找到这样一段css的样式,下图红色箭头所指处</p>

<p><img src="https://pic.mylonly.com/2018-03-30-093159.jpg" alt=""/></p>

<p>这个样式中并没有指定flex-wrap属性，而在flex布局中，flex-wrap的默认属性就是<code>nowrap</code>不换行。</p>

<pre><code class="language-text">是不是就是这个原因导致`wxc-rich-text`不支持换行呢？
如果真是这样，那么weex-ui的开发者为什么不把flex-wrap属性设置为wrap而提供一个支持多行的`wxc-rich-text`控件呢？
</code></pre>

<p>带着这个疑问，我们手动在<code>wxc-rich-text.vue</code>中将flex-wrap属性加上，这段css样式改为:</p>

<pre><code class="language-css">.wxc-rich-text {
    align-items: center;
    flex-direction: row;
    flex-wrap: wrap;
} 
</code></pre>

<p>好了，我们尝试输入一个多行的文本来看看效果:<br/>
<img src="https://pic.mylonly.com/2018-03-30-093213.jpg" alt=""/></p>

<p>在上图中我们发现，虽然整个控件的内容虽然确实有两行了，但是并不是我们想要的效果，文字部分并没有紧接着前面一个文字或者图片的后面，我想这也是为什么weex-ui的开发者在<code>wxc-rich-text</code>控件中不将flex-wrap属性设置为wrap的原因了。</p>

<p>好了，分析完<code>wxc-rich-text</code>不支持多行的原因，我们再来看看为什么<code>wxc-special-rich-text</code>可以支持两行的富文本呢？</p>

<p>当然还是先看源码,在<code>wxc-special-rich-text.vue</code>中，有下面一段代码:<br/>
<img src="https://pic.mylonly.com/2018-03-30-093224.jpg" alt=""/></p>

<p>代码当中有两个text控件，且两个text控件分别读取了newList[0]和newList[1]中的数据,为什么要如此呢，我们来看下面的js代码，在vue的computed当中，我们找到了名为<code>newList</code>的计算属性，代码有点长，我分别截了两张图<br/>
<img src="https://pic.mylonly.com/2018-03-30-093720.png" alt=""/><img src="https://pic.mylonly.com/2018-03-30-093640.png" alt=""/></p>

<p>如果你不想看上面的代码，可以直接看下面的结论：</p>

<p><code>这个newList中，就是将configLis中有的text文本内容切割成了两段文本分别放进两个text控件当中（依据前面已有的icon或者tag控件来计算第一行可以塞下的字符长度，其余字符就是第二行的文本内容）</code></p>

<p>好了，这下知道为什么<code>wxc-special-rich-text.vue</code>只能显示不超过两行的的富文本了吧，而且该富文本还必须要是<code>icon+text</code>或者<code>tag+text</code>的格式。</p>

<h2 id="toc_2">我的多行富文本实现思路</h2>

<p>既然<code>wxc-special-rich-text</code>通过将文本内容切割成两段文本来实现两行富文本的功能，那我能否通过将文本切割成粒度更小的的内容来解决多行富文本呢，这个粒度又是多少才最合适呢，我想，作为程序员，粒度为1应该是很容易想出来的一个数字，我也是如此。在粒度为1的情况下是不会存在<code>wxc-rich-text</code>中出现的因为第一行排列不下而将自己移至第二行从而导致第一行末端出现大量空白的情况。</p>

<p>那我们就在这个思路下实现我们自己的多行富文本</p>

<p>首先第一步，在<code>wxc-rich-text.vue</code>当中设置flex-wrap为wrap值，如上面文章所示。</p>

<p>如果你是用npm包的方式引入的weex-ui，那你可能就需要将wxc-rich-text.vue的代码拷贝一份，重新起个别的名字的控件了，然后再将flex-wrap属性设置成wrap。</p>

<p>第二步，将你的富文本内容切割成最小粒度的config，塞入<code>wxc-rich-text</code>的configList当中，我的实现如下:</p>

<pre><code class="language-javascript"> addNormalMessage:function(msg_id,sender,sender_level,title,message){
        console.log(&quot;normal message:&quot;,msg_id,sender,sender_level,title,message)
        var configList = []
        var config = this.addGrade(sender_level,title)
        if (config != {}){
          configList.push(config)
        }
        configList.push( {
          type: &#39;text&#39;,
          value: sender+&quot;:&quot;,
          theme: &#39;blue&#39;
        })

        var message_list = this.addMessage(message)

        message_list.forEach(config =&gt; {
          configList.push(config)
        })     
        this.messages.push(configList)
        this.messagesDict[msg_id] = this.messages.length - 1
        this.scroolToEnd()
    },
</code></pre>

<p>代码当中有三个部分的内容，addGrade是增加徽标的config，此处代码有点啰嗦，因为当时应该是直接复制拷贝的，可以通过设置value，color等等一些变量让代码精简很多。</p>

<pre><code class="language-javascript">addGrade(sender_level,title){
          console.log(&quot;grade:&quot;,sender_level,title)
          var config = {}
            if (sender_level == 500){
              config = {
                  type: &#39;tag&#39;,
                  value: &#39;讲师&#39;,
                  style: {
                    fontSize: 30,
                    color: &#39;#ffffff&#39;,
                    borderColor: &#39;#2d9b3a&#39;,
                    backgroundColor: &#39;#2d9b3a&#39;,
                    height: 40
                  }
              }
            }else if(sender_level == 900){
              config = {
                type:&#39;tag&#39;,
                value:&#39;管理&#39;,
                style: {
                  fontSize: 30,
                  color: &#39;#ffffff&#39;,
                  borderColor: &#39;#ec24dd&#39;,
                  backgroundColor: &#39;#ec24dd&#39;,
                  height: 40
                }
              }
            }else if(sender_level == 2000){
              config = {
                type:&#39;tag&#39;,
                value:&#39;室主&#39;,
                style:{
                  fontSize:30,
                  color: &#39;#ffffff&#39;,
                  borderColor: &#39;#e80c19&#39;,
                  backgroundColor: &#39;#e80c19&#39;,
                  height: 40
                }
              }
            }else{
              if(title &gt;= 0){
                config = {
                  type:&#39;icon&#39;,
                  src:&quot;bmlocal://assets/grade/Grade_&quot;+title+&quot;.png&quot;,
                  style: {
                    width: 90,
                    height: 40
                  }
                }
              }
            }
            return config
        },
</code></pre>

<p>然后加上发送人的姓名，最后加上聊天的内容addMessage,在addMessage当中我实现了QQ表情的匹配已经文本内容的切割</p>

<pre><code class="language-text">addMessage(message){
      var char_list = []
      var chars = message.split(&#39;&#39;)
      var startIndex = -1
      chars.forEach((char,index,array) =&gt; {
        if(char == &#39;[&#39;){
          startIndex = index
        }else if(char == &#39;]&#39;){
          if(startIndex != -1){
            var emotionStr = array.slice(startIndex,index+1).join(&quot;&quot;)
    
            if (emotionStr.indexOf(&quot;http&quot;) &gt; -1){  //图文消息
              var imageUrl = emotionStr.slice(1,index)
              char_list.push(this.oneImageConfig(imageUrl))
            }else{//表情文字
              char_list.push(this.oneEmojConfig(emotionStr))
            }
    
            startIndex = -1
        
          }else{
            char_list.push(this.oneCharConfig(char))
          }
        }else{
          if (startIndex == -1){
            char_list.push(this.oneCharConfig(char))
          }
        }
      });
      return char_list
    },
</code></pre>

<p>我们的业务当中，和大部分公司类似，我们的QQ表情是类似于[微笑][害羞]这种格式的。其中在查找表情的代码中，也许你们会疑问，为什么我不使用正则去匹配QQ表情，其实在我之前未重构之前的项目中（使用swift的作为开发语言），此处业务逻辑就是通过正则将QQ表情替换出来，但是在此处，由于要切割字符串为每个字符串返回一个配置，字符串的逐个遍历已经不可避免，如果在加上正则匹配，时间负责度反而会增加一倍，所以我直接在遍历循环中加入了查找QQ表情的代码，希望能减轻少许我这种投机取巧的方法实现多行富文本样式带来的性能损耗。</p>

<pre><code class="language-javascript">oneEmojConfig(emojName){
  var localPath = emotion.emojLocalPath(emojName)
  console.log(&quot;localpath:&quot;,localPath)
  if( localPath != null){
    return {
      type:&#39;icon&#39;,
      src:localPath,
      style:{
        width:40,
        height:40
      }
    }
  }else{
    return {
      type:&#39;text&#39;,
      value:emojName,
      theme:&quot;yellow&quot;
    }
  }
},
</code></pre>

<p>在设置QQ表情的config当中，我写了一个emotion.js的工具函数，用来返回QQ表情名字所对应的本地图片的路径</p>

<p><code>emotion.js</code></p>

<pre><code class="language-javascript">/**
 * 表情转换工具类
 * @authors Root (root@mylonly.com)
 * @date    2018-03-30 08:48:19
 * @version 1.0.0
 */



let emotionFunc = {
    emotionArray : [&quot;[微笑]&quot;,&quot;[撇嘴]&quot;,&quot;[色]&quot;,&quot;[发呆]&quot;,&quot;[得意]&quot;,&quot;[流泪]&quot;,&quot;[害羞]&quot;,&quot;[闭嘴]&quot;,&quot;[睡]&quot;,&quot;[大哭]&quot;,&quot;[尴尬]&quot;,&quot;[发怒]&quot;,&quot;[调皮]&quot;,&quot;[呲牙]&quot;,&quot;[惊讶]&quot;,&quot;[难过]&quot;,&quot;[酷]&quot;,&quot;[冷汗]&quot;,&quot;[抓狂]&quot;,&quot;[吐]&quot;,&quot;[偷笑]&quot;,&quot;[可爱]&quot;,
&quot;[白眼]&quot;,&quot;[傲慢]&quot;,&quot;[饥饿]&quot;,&quot;[困]&quot;,&quot;[惊恐]&quot;,&quot;[流汗]&quot;,&quot;[憨笑]&quot;,&quot;[大兵]&quot;,&quot;[奋斗]&quot;,&quot;[咒骂]&quot;,&quot;[疑问]&quot;,&quot;[嘘]&quot;,&quot;[晕]&quot;,&quot;[折磨]&quot;,&quot;[衰]&quot;,&quot;[骷髅]&quot;,&quot;[敲打]&quot;,&quot;[再见]&quot;,&quot;[擦汗]&quot;,&quot;[抠鼻]&quot;,&quot;[鼓掌]&quot;,&quot;[糗大了]&quot;,&quot;[坏笑]&quot;,&quot;[左哼哼]&quot;,&quot;[右哼哼]&quot;,&quot;[哈欠]&quot;,
&quot;[鄙视]&quot;,&quot;[委屈]&quot;,&quot;[快哭了]&quot;,&quot;[阴险]&quot;,&quot;[亲亲]&quot;,&quot;[吓]&quot;,&quot;[可怜]&quot;,&quot;[菜刀]&quot;,&quot;[西瓜]&quot;,&quot;[啤酒]&quot;,&quot;[篮球]&quot;,&quot;[乒乓]&quot;,&quot;[咖啡]&quot;,&quot;[饭]&quot;,&quot;[猪头]&quot;,&quot;[玫瑰]&quot;,&quot;[凋谢]&quot;,&quot;[示爱]&quot;,&quot;[爱心]&quot;,&quot;[心碎]&quot;,&quot;[蛋糕]&quot;,&quot;[闪电]&quot;,&quot;[炸弹]&quot;,&quot;[刀]&quot;,&quot;[握手]&quot;,&quot;[胜利]&quot;,
&quot;[便便]&quot;,&quot;[NO]&quot;,&quot;[OK]&quot;,&quot;[抱拳]&quot;,&quot;[弱]&quot;,&quot;[强]&quot;],

    emojLocalPath:function(emoj_name){
      var index = this.emotionArray.indexOf(emoj_name)
      if (index &gt; -1){
          return &quot;bmlocal://assets/emotions/&quot;+index+&quot;@2x.png&quot;
      }
      return null
    },
    emojName:function(index){
        if (index &lt; this.emotionArray.length){
            return this.emotionArray[index]
        }
        return null
    }
}

export default emotionFunc;

</code></pre>

<p>至此，我的整个实现已经完全呈现出来了，如果你能看懂我上面所写，正好你也正好有和我类似的需求，相信你也能在我的思路下实现自己的业务代码。</p>

<p>最后，愿世界和平！！！</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ubuntu 安装uwsgi]]></title>
    <link href="https://blog.mylonly.com/15126534156721.html"/>
    <updated>2017-12-07T21:30:15+08:00</updated>
    <id>https://blog.mylonly.com/15126534156721.html</id>
    <content type="html"><![CDATA[
<p>先源码安装python3.6（记得先用apt-get 安装好一些系统库zlib_dev,openssl_dev）<br/>
然后利用pip3 安装uwsgi</p>

<pre><code class="language-shell">wget https://www.python.org/ftp/python/3.6.3/Python-3.6.3.tar.xz
tar -xvf Python-3.6.3.tar.xz
cd Python-3.6.3/

apt-get install zlib1g-dev
apt-get install libssl-dev
apt-get install sqlite3


./configure --enable-loadable-sqlite-extensions

make &amp;&amp; sudo make install


pip3 install uwsgi

uwsgi -i /data/web/xxxx.ini
</code></pre>

<p>uwsgi 配置侦测文件改动自动自动重启，在ini中加入</p>

<pre><code class="language-text">py-autoreload = 1
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Django 发送html格式的邮件]]></title>
    <link href="https://blog.mylonly.com/15119665687635.html"/>
    <updated>2017-11-29T22:42:48+08:00</updated>
    <id>https://blog.mylonly.com/15119665687635.html</id>
    <content type="html"><![CDATA[
<p>django中默认提供了发送邮件的库<code>mail</code>，通过这个库我们可以很方便的通过django发送一份电子邮件</p>

<h5 id="toc_0">1. 在setting 中指定邮件服务器的基本信息</h5>

<pre><code class="language-python">EMAIL_USE_SSL = True    
EMAIL_HOST = &#39;smtp.exmail.qq.com&#39;  # 如果是 163 改成 smtp.163.com
EMAIL_PORT = 465
EMAIL_HOST_USER = &#39;***@domain.com&#39; # 帐号
EMAIL_HOST_PASSWORD = &#39;password&#39;  # 密码
DEFAULT_FROM_EMAIL = EMAIL_HOST_USER
</code></pre>

<h5 id="toc_1">2. 引用mail库发送邮件</h5>

<pre><code class="language-python">from django.core.mail import EmailMessage
from Across.settings import EMAIL_HOST_USER

msg = EmailMessage(&quot;邮件标题&quot;,&quot;邮件内容&quot;,EMAIL_HOST_USER,[接受邮件列表])
msg.send()
</code></pre>

<h5 id="toc_2">3.发送html模板邮件</h5>

<p>可以利用Django的template库读取指定的html模板，然后将参数代入，首选需要在settings中设置template的目录</p>

<pre><code class="language-python">TEMPLATES = [
    {
        &#39;BACKEND&#39;: &#39;django.template.backends.django.DjangoTemplates&#39;,
        &#39;DIRS&#39;: [
            os.path.join(BASE_DIR,&#39;templates&#39;)  ##你的模板目录
        ],
        &#39;APP_DIRS&#39;: True,
        &#39;OPTIONS&#39;: {
            &#39;context_processors&#39;: [
                &#39;django.template.context_processors.debug&#39;,
                &#39;django.template.context_processors.request&#39;,
                &#39;django.contrib.auth.context_processors.auth&#39;,
                &#39;django.contrib.messages.context_processors.messages&#39;,
            ],
        },
    },
]
</code></pre>

<p>然后将你准备好的html模板放入该目录，然后就是利用temlate的loader函数加载模板传入指定参数</p>

<pre><code class="language-python">html_content = loader.render_to_string(&#39;email.html&#39;,{&#39;authcode&#39;:random,&#39;title&#39;:&quot;标题标题&quot;,&#39;operation&#39;:operation_str})
msg = EmailMessage(&quot;您的验证码&quot;,html_content,EMAIL_HOST_USER,[email])
msg.content_subtype = &quot;html&quot; # Main content is now text/html
msg.send()
</code></pre>

]]></content>
  </entry>
  
</feed>
