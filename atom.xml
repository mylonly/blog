<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[独自一人]]></title>
  <link href="https://blog.xgtian.com/atom.xml" rel="self"/>
  <link href="https://blog.xgtian.com/"/>
  <updated>2020-02-24T14:02:51+08:00</updated>
  <id>https://blog.xgtian.com/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[k8s-1.16高可用集群部署]]></title>
    <link href="https://blog.xgtian.com/15786401810709.html"/>
    <updated>2020-01-10T15:09:41+08:00</updated>
    <id>https://blog.xgtian.com/15786401810709.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">机器要求</h2>

<p>高可用集群一般用于生产环境，官方推荐至少需要3台master节点机器，4台node节点机器</p>

<table>
<thead>
<tr>
<th>Hostname</th>
<th>IP</th>
<th>Role</th>
</tr>
</thead>

<tbody>
<tr>
<td>ucloud-bj-k8s-master-01</td>
<td>10.9.142.180</td>
<td>Master Node</td>
</tr>
<tr>
<td>ucloud-bj-k8s-node-01</td>
<td>10.9.165.222</td>
<td>Worker Node</td>
</tr>
<tr>
<td>ucloud-bj-k8s-node-02</td>
<td>10.9.127.58</td>
<td>Worker Node</td>
</tr>
<tr>
<td>ucloud-bj-K8s-node-03</td>
<td>10.9.57.4</td>
<td>Worker Node</td>
</tr>
<tr>
<td>ucloud-bj-K8s-node-04</td>
<td>10.9.174.192</td>
<td>Worker Node</td>
</tr>
</tbody>
</table>

<h2 id="toc_1">安装准备</h2>

<ol>
<li><p>禁用Swap<br/>
k8s为了使容器的调度更符合机器的实际资源情况，k8s建议关闭内存交换   </p>
<pre><code class="language-text">swapoff -a
</code></pre>
<p>同时删除<code>/etc/fstab</code>中swap那条记录</p>
<p>当然，如果你的机器资源确实不多，需要利用swap，那么你可以不关闭swap交换空间，通过如下参数告诉k8s开启swap</p>
<pre><code class="language-text">kubelet --fail-swap-on=false ...
</code></pre></li>
<li><p>端口开放(生产环境)</p>
<p><img src="https://pic.mylonly.com/2020-01-10-070912.jpg" alt=""/></p></li>
</ol>

<h2 id="toc_2">Docker安装(可选)</h2>

<p>由于k8s需要安装指定docker18.06版本，所以如果你的版本不对，可以先卸载重新安装</p>

<pre><code class="language-text">sudo apt-get remove docker docker-engine docker-ce docker.io
</code></pre>

<p>安装docker</p>

<pre><code class="language-text"># 从 Ubuntu 的存储库安装 Docker：
apt-get update
apt-get install -y docker.io

# 或者从 Docker 的 Ubuntu 或 Debian 镜像仓库中安装 Docker CE 18.06：

## 安装环境准备。
apt-get update &amp;&amp; apt-get install apt-transport-https ca-certificates curl software-properties-common

## 下载 GPG 密钥。
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -

## 添加 docker apt 镜像仓库。
add-apt-repository \
   &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable&quot;

## 安装 docker。
apt-get update &amp;&amp; apt-get install docker-ce=18.06.0~ce~3-0~ubuntu

# 设置守护进程。
cat &gt; /etc/docker/daemon.json &lt;&lt;EOF
{
  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],
  &quot;log-driver&quot;: &quot;json-file&quot;,
  &quot;log-opts&quot;: {
    &quot;max-size&quot;: &quot;100m&quot;
  },
  &quot;storage-driver&quot;: &quot;overlay2&quot;
}
EOF

mkdir -p /etc/systemd/system/docker.service.d

# 重启 docker。
systemctl daemon-reload
systemctl restart docker
</code></pre>

<h2 id="toc_3">安装 <code>kubelet</code>,<code>kubeadm</code>,<code>kubectl</code></h2>

<p>添加阿里源密钥</p>

<pre><code class="language-text">curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - 
</code></pre>

<p>国内源(阿里)</p>

<pre><code class="language-text">cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF
</code></pre>

<p>安装</p>

<pre><code class="language-text">apt-get update
apt-get install -y kubelet=1.16.2-00 kubeadm=1.16.2-00 kubectl=1.16.2-00
apt-mark hold kubelet kubeadm kubectl
    
</code></pre>

<h2 id="toc_4">安装Master节点</h2>

<pre><code class="language-text">kubeadm init --control-plane-endpoint &quot;k8s-api.youxuetong.com:6443&quot; --image-repository registry.aliyuncs.com/google_containers --pod-network-cidr=10.244.0.0/16 --upload-certs
</code></pre>

<p>如果安装完成，最后后输出如下内容</p>

<pre><code class="language-text">Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join k8s-master:6443 --token vxszss.bboqeevhypvt0sxl \
    --discovery-token-ca-cert-hash sha256:56205646be3a53103e175d544dcd27cc82317c93042763cab20745334d8cb782 \
    --control-plane

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join k8s-master:6443 --token vxszss.bboqeevhypvt0sxl \
    --discovery-token-ca-cert-hash sha256:56205646be3a53103e175d544dcd27cc82317c93042763cab20745334d8cb782
</code></pre>

<h2 id="toc_5">部署CNI网络</h2>

<pre><code class="language-text">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml
</code></pre>

<h2 id="toc_6">接入其他Master节点</h2>

<p>在其他master节点机器上执行</p>

<pre><code class="language-text">  kubeadm join k8s-master:6443 --token vxszss.bboqeevhypvt0sxl \
    --discovery-token-ca-cert-hash sha256:56205646be3a53103e175d544dcd27cc82317c93042763cab20745334d8cb782 \
    --control-plane
</code></pre>

<p>如果提示token失效，请利用下面的命令去第一个master节点重新生成token</p>

<pre><code class="language-text">kubeadm token create --print-join-command
</code></pre>

<h2 id="toc_7">接入其他Node节点</h2>

<pre><code class="language-text">kubeadm join k8s-master:6443 --token vxszss.bboqeevhypvt0sxl \
    --discovery-token-ca-cert-hash sha256:56205646be3a53103e175d544dcd27cc82317c93042763cab20745334d8cb782
</code></pre>

<h2 id="toc_8">安装完成</h2>

<p>查看各节点状态</p>

<pre><code class="language-text">ubuntu@ucloud-bj-k8s-master-01:~$ kubectl get nodes -o wide
NAME                      STATUS   ROLES    AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
ucloud-bj-k8s-master-01   Ready    master   26d   v1.16.2   10.9.142.180   &lt;none&gt;        Ubuntu 16.04.6 LTS   4.4.0-142-generic   docker://18.6.0
ucloud-bj-k8s-master-02   Ready    master   26d   v1.16.2   10.9.175.27    &lt;none&gt;        Ubuntu 16.04.6 LTS   4.4.0-142-generic   docker://18.6.0
ucloud-bj-k8s-master-03   Ready    master   26d   v1.16.2   10.9.91.143    &lt;none&gt;        Ubuntu 16.04.6 LTS   4.4.0-142-generic   docker://18.6.0
ucloud-bj-k8s-node-01     Ready    &lt;none&gt;   26d   v1.16.2   10.9.165.222   &lt;none&gt;        Ubuntu 16.04.6 LTS   4.4.0-142-generic   docker://18.6.0
ucloud-bj-k8s-node-02     Ready    &lt;none&gt;   26d   v1.16.2   10.9.127.58    &lt;none&gt;        Ubuntu 16.04.6 LTS   4.4.0-142-generic   docker://18.6.0
ucloud-bj-k8s-node-03     Ready    &lt;none&gt;   26d   v1.16.2   10.9.57.4      &lt;none&gt;        Ubuntu 16.04.6 LTS   4.4.0-142-generic   docker://18.6.0
ucloud-bj-k8s-node-04     Ready    &lt;none&gt;   26d   v1.16.2   10.9.174.192   &lt;none&gt;        Ubuntu 16.04.6 LTS   4.4.0-142-generic   docker://18.6.0
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[k8s-1.16单主节点集群部署]]></title>
    <link href="https://blog.xgtian.com/15786388799784.html"/>
    <updated>2020-01-10T14:47:59+08:00</updated>
    <id>https://blog.xgtian.com/15786388799784.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">机器要求</h2>

<p>单主集群一般用于开发环境，至少需要大于2台机器，一台作为主节点，其余的作为工作节点，机器间可以相互访问即可。</p>

<table>
<thead>
<tr>
<th>Hostname</th>
<th>Ip</th>
<th>Role</th>
</tr>
</thead>

<tbody>
<tr>
<td>k8s-master</td>
<td>192.168.122.2</td>
<td>Master Node</td>
</tr>
<tr>
<td>k8s-node-01</td>
<td>192.168.122.101</td>
<td>Worker Node</td>
</tr>
<tr>
<td>k8s-node-02</td>
<td>192.168.122.102</td>
<td>Worker Node</td>
</tr>
<tr>
<td>K8s-node-03</td>
<td>192.168.122.103</td>
<td>Worker Node</td>
</tr>
</tbody>
</table>

<p>需要注意:</p>

<blockquote>
<p>如果你使用的是云服务器，服务器只有一块网卡，外网IP通过弹性IP绑定，请不要使用公网IP，使用内网网卡绑定的IP来通信。原因是，k8s只会监听网卡绑定IP的端口。</p>
</blockquote>

<h2 id="toc_1">安装准备</h2>

<ol>
<li><p>禁用Swap<br/>
k8s为了使容器的调度更符合机器的实际资源情况，k8s建议关闭内存交换   </p>
<pre><code class="language-text">swapoff -a
</code></pre>
<p>同时删除<code>/etc/fstab</code>中swap那条记录</p>
<p>当然，如果你的机器资源确实不多，需要利用swap，那么你可以不关闭swap交换空间，通过如下参数告诉k8s开启swap</p>
<pre><code class="language-text">kubelet --fail-swap-on=false ...
</code></pre></li>
<li><p>关闭防火墙(开发环境)</p>
<p>Ubuntu</p>
<pre><code class="language-text">sudo ufw disable
</code></pre>
<p>CentOS</p>
<pre><code class="language-text">systemctl stop firewalld
systemctl disable firewalld
</code></pre></li>
<li><p>禁用SELinux(开发环境)</p>
<pre><code class="language-text">sudo apt install selinux-utils
setenforce 0
</code></pre></li>
<li><p>确认mac地址以及product uuid唯一<br/>
如果你的服务器是通过虚拟机克隆过来的，请确保这两项唯一</p>
<pre><code class="language-text">ip link #检查mac地址
sudo cat /sys/class/dmi/id/product_uuid ##检查uuid
</code></pre></li>
</ol>

<h2 id="toc_2">Docker安装(可选)</h2>

<p>由于k8s需要安装指定docker18.06版本，所以如果你的版本不对，可以先卸载重新安装</p>

<pre><code class="language-text">sudo apt-get remove docker docker-engine docker-ce docker.io
</code></pre>

<p>安装docker</p>

<pre><code class="language-text"># 从 Ubuntu 的存储库安装 Docker：
apt-get update
apt-get install -y docker.io

# 或者从 Docker 的 Ubuntu 或 Debian 镜像仓库中安装 Docker CE 18.06：

## 安装环境准备。
apt-get update &amp;&amp; apt-get install apt-transport-https ca-certificates curl software-properties-common

## 下载 GPG 密钥。
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -

## 添加 docker apt 镜像仓库。
add-apt-repository \
   &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable&quot;

## 安装 docker。
apt-get update &amp;&amp; apt-get install docker-ce=18.06.0~ce~3-0~ubuntu

# 设置守护进程。
cat &gt; /etc/docker/daemon.json &lt;&lt;EOF
{
  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],
  &quot;log-driver&quot;: &quot;json-file&quot;,
  &quot;log-opts&quot;: {
    &quot;max-size&quot;: &quot;100m&quot;
  },
  &quot;storage-driver&quot;: &quot;overlay2&quot;
}
EOF

mkdir -p /etc/systemd/system/docker.service.d

# 重启 docker。
systemctl daemon-reload
systemctl restart docker
</code></pre>

<h2 id="toc_3">开始安装 （切换到root身份）</h2>

<ol>
<li><p>安装相关工具</p>
<pre><code class="language-text">apt-get update &amp;&amp; apt-get install -y apt-transport-https curl
</code></pre></li>
<li><p>添加k8s软件源</p>
<p>添加阿里源密钥</p>
<pre><code class="language-text">curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - 
</code></pre>
<p>添加k8s阿里源</p>
<pre><code class="language-text">cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main<br/>
EOF
</code></pre></li>
<li><p>安装 <code>kubelet</code>,<code>kubeadm</code>,<code>kubectl</code></p>
<pre><code class="language-text">apt-get update
apt-get install -y kubelet kubeadm kubectl<br/>
apt-mark hold kubelet kubeadm kubectl
</code></pre>
<p>非root用户需要加上sudo</p></li>
</ol>

<h2 id="toc_4">部署master</h2>

<h3 id="toc_5">初始化master节点</h3>

<pre><code class="language-text">kubeadm init --apiserver-advertise-address=192.168.122.2 --image-repository registry.aliyuncs.com/google_containers --pod-network-cidr=10.244.0.0/16
</code></pre>

<p>参数解释：</p>

<ul>
<li><code>--apiserver-advertise-address</code>: k8s 中的主要服务apiserver的部署地址，填自己的管理节点 ip</li>
<li><code>--image-repository</code>: 拉取的 docker 镜像源，因为初始化的时候kubeadm会去拉 k8s 的很多组件来进行部署，所以需要指定国内镜像源，下不然会拉取不到镜像。</li>
<li><code>--pod-network-cidr</code>: 这个是 k8s 采用的节点网络，因为我们将要使用flannel作为 k8s 的网络，所以这里填10.244.0.0/16就好</li>
<li><code>--kubernetes-version</code>: 这个是用来指定你要部署的 k8s 版本的，一般不用填，不过如果初始化过程中出现了因为版本不对导致的安装错误的话，可以用这个参数手动指定。</li>
<li><code>--gnore-preflight-errors</code>: 忽略初始化时遇到的错误，比如说我想忽略 cpu 数量不够 2 核引起的错误，就可以用--ignore-preflight-errors=CpuNum。错误名称在初始化错误时会给出来。</li>
</ul>

<p>执行上面的命令，等待安装完成，知道出现下面的文字表示安装成功</p>

<pre><code class="language-text">Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.122.2:6443 --token 1gyku3.72jgt8prrp2fdhhx \
    --discovery-token-ca-cert-hash sha256:9c5bf462c308be3a69b6766e71f367e48e222c4ee019c4ec438e02e089ab4e12
</code></pre>

<p>复制保存<code>kubeadm join</code>那行的文字，后面添加节点需要使用到，注意，token默认是24小时会过期，可以在master上利用如下命令查看token是否有效</p>

<pre><code class="language-text">kubeadm token list
</code></pre>

<p>如果没有有效token，可以重新创建一个</p>

<pre><code class="language-text">kubeadm token create
</code></pre>

<h3 id="toc_6">初始化kubectl工具</h3>

<p>按照kubeadm的指示操作就行，依次执行如下命令，用普通用户的账号</p>

<pre><code class="language-text">mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>

<p>如果你想在root用户下使用</p>

<pre><code class="language-text">export KUBECONFIG=/etc/kubernetes/admin.conf
</code></pre>

<h3 id="toc_7">部署flannel (Pod Network Adds-on)</h3>

<p>k8s依赖于第三的节点网络，以便各pod节点之间可以相互通信</p>

<pre><code class="language-text">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml

</code></pre>

<p><code>如果遇到timeout错误，请使用代理</code></p>

<p>输出以下内容代表安装完成</p>

<pre><code class="language-text">clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.extensions/kube-flannel-ds-amd64 created
daemonset.extensions/kube-flannel-ds-arm64 created
daemonset.extensions/kube-flannel-ds-arm created
daemonset.extensions/kube-flannel-ds-ppc64le created
daemonset.extensions/kube-flannel-ds-s390x created

</code></pre>

<h3 id="toc_8">添加node节点</h3>

<p>按照最前面的部署安装好k8s相关程序</p>

<pre><code class="language-text">kubeadm join 192.168.122.2:6443 --token 1gyku3.72jgt8prrp2fdhhx \
    --discovery-token-ca-cert-hash sha256:9c5bf462c308be3a69b6766e71f367e48e222c4ee019c4ec438e02e089ab4e12
</code></pre>

<blockquote>
<p>如果join命令忘记了复制，可以去master节点上执行</p>
</blockquote>

<pre><code class="language-text">kubeadm token create --print-join-command
</code></pre>

<h3 id="toc_9">详细安装日志</h3>

<p>可以通过在命令后面增加<code>--v=5</code>来查看详细安装日志，例如</p>

<pre><code class="language-text">kubeadm join 192.168.122.2:6443 --token 1gyku3.72jgt8prrp2fdhhx \
    --discovery-token-ca-cert-hash sha256:9c5bf462c308be3a69b6766e71f367e48e222c4ee019c4ec438e02e089ab4e12 --v=5
</code></pre>

<h3 id="toc_10">安装完成</h3>

<p>主节点执行<code>kubectl get nodes -o wide</code>，检查各节点是否为<code>Ready状态</code></p>

<pre><code class="language-text">ubuntu@k8s-master:~$ kubectl get nodes -o wide
NAME          STATUS   ROLES    AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
k8s-master    Ready    master   23d   v1.16.2   192.168.122.2     &lt;none&gt;        Ubuntu 16.04.6 LTS   4.4.0-142-generic   docker://18.6.0
k8s-node-01   Ready    &lt;none&gt;   23d   v1.16.2   192.168.122.101   &lt;none&gt;        Ubuntu 16.04.6 LTS   4.4.0-142-generic   docker://18.6.0
k8s-node-02   Ready    &lt;none&gt;   23d   v1.16.2   192.168.122.102   &lt;none&gt;        Ubuntu 16.04.6 LTS   4.4.0-142-generic   docker://18.6.0
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kubernets特性介绍]]></title>
    <link href="https://blog.xgtian.com/15779359388855.html"/>
    <updated>2020-01-02T11:32:18+08:00</updated>
    <id>https://blog.xgtian.com/15779359388855.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>Kubernetes 源于希腊语，意为 “舵手” 或 “飞行员”。它是Google开源的一个容器编排引擎，它支持自动化部署、大规模可伸缩、应用容器化管理,由于Kubernetes单词中，k和s之间的单词数量为8个，所以也简称k8s。</p>
</blockquote>

<h2 id="toc_0">系统架构</h2>

<h3 id="toc_1">单主集群</h3>

<p>单主集群，是集群内只包含一个Control Panel节点的集群，一般用作开发环境，下图是单主集群的结构。<br/>
<img src="https://pic.mylonly.com/2020-01-07-061225.png" alt=""/><br/>
由于主节点只有一个，且work节点无法再主节点宕机后选举产生新的主节点，所有单主集群不适用与产品环境。</p>

<h3 id="toc_2">高可用集群</h3>

<p>高可用集群,是至少包含三个Control Panel节点的集群，Work节点不再直接与某一台主节点的api-server联系，而是通过负载均衡均匀分不到master节点，同时由于etcd节点可以使用外部集群替代，高可用集群又分为内部etcd以及外部etcd集群两种。</p>

<h4 id="toc_3">内部etcd节点集群</h4>

<p><img src="https://pic.mylonly.com/2020-01-08-062708.png" alt=""/></p>

<h4 id="toc_4">外部etcd节点集群</h4>

<p><img src="https://pic.mylonly.com/2020-01-08-062855.png" alt=""/></p>

<h2 id="toc_5">Kubernets的优势</h2>

<p>简化应用程序部署和维护工作，同时最大化利用硬件资源</p>

<h3 id="toc_6">自动修复</h3>

<p>k8s 会自动重新启动失败的容器、替换容器、杀死不响应用户定义的运行状况检查的容器，并且这些都是在用户无感知的情况下进行的（副本数量需要大于1且有正常运行的容器）。</p>

<p>要想介绍k8s的自动修复，必须要介绍存活探针，k8s正是利用存活探针来检查容器是否还在运行，如果探测失败，k8s将自动重启该容器。</p>

<p>k8s有三种探针机制:</p>

<ol>
<li><code>HTTP GET</code>: 针对容器的IP地址，端口以及路径，执行HTTP GET请求，如果收到的返回状态码为2xx或者3xx，则认为探测成功</li>
<li><code>TCP</code>: 针对Socket通信进行探测，尝试与指定端口建立TCP连接，如果连接成功则探测成功。</li>
<li><code>Exec</code>: 在容器内执行指定的命令，并检查命令的退出状态码，如果状态码为0，则探测成功。</li>
</ol>

<p>我们先声明一个拥有两个副本的pod，创建deployment.yaml文件，写入下面的内容</p>

<pre><code class="language-text">apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 2
  selector:
    app: kubia
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      containers:
        - name: kubia
          image: luksa/kubia
          livenessProbe:
              httpGet:
                path: /
                port: 8080
          ports:
            - containerPort: 8080
</code></pre>

<p>然后利用kubectl创建pod</p>

<pre><code class="language-text">kubectl create -f deployment.yaml -n default
</code></pre>

<p>查看pod状态</p>

<pre><code class="language-text">ubuntu@k8s-master:~$ kubectl get pods -n default
NAME          READY   STATUS    RESTARTS   AGE
kubia-dvdkl   1/1     Running   0          4m55s
kubia-g7z9g   1/1     Running   0          4m55s
</code></pre>

<p>等两个pod都完全运行起来之后,我们先模拟第一种情况，程序异常退出，我们删除掉其中一个Pod</p>

<pre><code class="language-text">ubuntu@k8s-master:~$ kubectl delete pod kubia-g7z9g -n default
pod &quot;kubia-g7z9g&quot; deleted
ubuntu@k8s-master:~$ kubectl get pods
NAME          READY   STATUS    RESTARTS   AGE
kubia-dvdkl   1/1     Running   0          7m43s
kubia-jtq6c   1/1     Running   0          39s
</code></pre>

<p>可以看到，很快另外一个应用以及重新运行起来了。</p>

<p>再来看另外一种情况，节点故障，我们关闭其中一台node节点</p>

<pre><code class="language-text">ubuntu@k8s-master:~$ kubectl get nodes
NAME          STATUS     ROLES    AGE     VERSION
k8s-master    Ready      master   26d     v1.16.2
k8s-node-01   Ready      &lt;none&gt;   26d     v1.16.2
k8s-node-02   Ready      &lt;none&gt;   26d     v1.16.2
k8s-node-03   Ready      &lt;none&gt;   26d     v1.16.2
k8s-node-04   Ready      &lt;none&gt;   18d     v1.16.2
k8s-node-05   Ready      &lt;none&gt;   16d     v1.16.2
k8s-node-06   Ready      &lt;none&gt;   14d     v1.16.2
k8s-node-07   Ready      &lt;none&gt;   4d20h   v1.16.2
k8s-node-08   NotReady   &lt;none&gt;   4d19h   v1.16.2
</code></pre>

<p>等待几分钟后，查看Pods</p>

<pre><code class="language-text">ubuntu@k8s-master:~$ kubectl get pods -n default -o wide
NAME          READY   STATUS        RESTARTS   AGE     IP            NODE          NOMINATED NODE   READINESS GATES
kubia-cpmbh   0/1     Pending       0          0s      &lt;none&gt;        k8s-node-06   &lt;none&gt;           &lt;none&gt;
kubia-dvdkl   1/1     Terminating   1          4d19h   10.244.8.44   k8s-node-08   &lt;none&gt;           &lt;none&gt;
kubia-jtq6c   1/1     Running       0          4d19h   10.244.7.5    k8s-node-07   &lt;none&gt;           &lt;none&gt;
</code></pre>

<p>针对节点故障，k8s并不会把出问题的节点上的所有Pod都迁移到别的Pod上，而是创建新的Pod,原来的Pod仍然保留，只是状态不再是Ready,如果节点恢复，原来的Pod状态也会恢复。</p>

<h3 id="toc_7">服务发现</h3>

<p>我们先为kubia应用创建一个服务,创建service.yaml,写入下面的内容</p>

<pre><code class="language-text">kind: apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  selector:
    app: kubia
  ports:
  - port: 80
    targetPort: 8080
</code></pre>

<p>执行<code>kubectl create -f service.yaml -n default</code>创建服务</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl create -f kubia/service.yaml -n default
service/kubia created
</code></pre>

<p>查看service状态</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl get svc -n default
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP   26d
kubia        ClusterIP   10.107.59.19   &lt;none&gt;        80/TCP    27s
</code></pre>

<p>k8s的服务发现分为两种，集群内部访问和集群外部访问</p>

<h4 id="toc_8">集群内部</h4>

<p>可以通过内置的DNS用服务名的方式访问集群内部的应用，同时也能利用环境变量获取开放的端口。</p>

<h6 id="toc_9">通过环境变量访问服务</h6>

<p>当pod创建时，k8s会把当前命名空间内已存在的服务已环境变量的方式导入到pod当中，所以只要你的pod晚于service的创建，pod里的进程就可以根据环境变量获得服务的IP和端口号。<br/>
我们先查看pod早于service创建的情况</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl exec kubia-8fffd4bff-k4tk4 env -n default
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=kubia-8fffd4bff-k4tk4
KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
KUBERNETES_SERVICE_HOST=10.96.0.1
KUBERNETES_SERVICE_PORT=443
KUBERNETES_SERVICE_PORT_HTTPS=443
KUBERNETES_PORT=tcp://10.96.0.1:443
KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_PORT=443
NPM_CONFIG_LOGLEVEL=info
NODE_VERSION=7.9.0
YARN_VERSION=0.22.0
HOME=/root
</code></pre>

<p>然后我们删除pod，让他自动修复</p>

<pre><code class="language-text">kubectl delete pod --all -n default
</code></pre>

<p>查看新的pod</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl get pods -n default
NAME                    READY   STATUS    RESTARTS   AGE
kubia-8fffd4bff-622rl   1/1     Running   0          44s
kubia-8fffd4bff-6h64s   1/1     Running   0          44s
</code></pre>

<p>查看新pod的环境变量</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl exec kubia-8fffd4bff-622rl env -n default
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
HOSTNAME=kubia-8fffd4bff-622rl
KUBIA_PORT_80_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_PORT=443
KUBIA_SERVICE_PORT=80
KUBERNETES_SERVICE_PORT=443
KUBERNETES_PORT_443_TCP_PROTO=tcp
KUBERNETES_PORT_443_TCP_ADDR=10.96.0.1
KUBIA_SERVICE_HOST=10.107.59.19
KUBIA_PORT_80_TCP_PORT=80
KUBERNETES_PORT=tcp://10.96.0.1:443
KUBERNETES_PORT_443_TCP=tcp://10.96.0.1:443
KUBIA_PORT=tcp://10.107.59.19:80
KUBIA_PORT_80_TCP=tcp://10.107.59.19:80
KUBIA_PORT_80_TCP_ADDR=10.107.59.19
KUBERNETES_SERVICE_HOST=10.96.0.1
KUBERNETES_SERVICE_PORT_HTTPS=443
NPM_CONFIG_LOGLEVEL=info
NODE_VERSION=7.9.0
YARN_VERSION=0.22.0
HOME=/root
</code></pre>

<p>对比两种情况，可以发现，<code>KUBIA_SERVICE_HOST</code>和<code>KUBIA_SERVICE_PORT</code>已经存在于环境变量当中了。</p>

<h6 id="toc_10">通过DNS访问服务</h6>

<p>在k8s内部，借助CoreDNS,一旦一个服务创建好，我们可以用<code>hostname.namespace.svc.cluster.local</code>这个域名来获取到服务的IP地址，如果应用和服务同一个集群内部，甚至在同一个命名空间下，我就可以直接用hostname访问该服务<br/>
利用service名称访问服务</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl exec kubia-8fffd4bff-622rl curl http://kubia -n default
You&#39;ve hit kubia-8fffd4bff-622rl
</code></pre>

<pre><code class="language-text">kubectl exec kubia-8fffd4bff-622rl curl http://kubia.default.svc.cluster.local -n default
You&#39;ve hit kubia-8fffd4bff-6h64s
</code></pre>

<p>那么问题来了，利用DNS的方式访问服务确实很方便，但是DNS只能解决IP，针对端口问题怎么办？<br/>
针对这种情况我们的处理办法是固定端口，应为在k8s内部大部分应用独享一个IP的所有端口，所以不存在端口冲突的问题，我们可以针对不同的协议约定一个统一的端口，这样应用就不需要知道具体应用的端口了。</p>

<pre><code class="language-text">常用协议默认端口:
http: 80
https: 443
mysql: 3306
redis: 6379
gRPC: 9000
</code></pre>

<h4 id="toc_11">集群外部</h4>

<p>k8s提供了NodePort,LoadBlancer,Ingres三种服务暴露的方式，应用于各种需要对外暴露服务的场景。<br/>
其中<code>LoadBlancer</code>属于云服务商特有的功能，如果你是私有集群，大概率是用不了这个功能了。</p>

<h6 id="toc_12">NodePort</h6>

<p>NodePort的原理时，在所有Node节点上监听一个端口（所有node节点端口相同），并将传入的数据转发到对应的服务上。</p>

<p>我们修改之前的kubia应用的service.yaml</p>

<pre><code class="language-text">kind: apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  type: NodePort
  selector:
    app: kubia
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 20000
</code></pre>

<p>这次我们指定type类型为NodePort，同时注明nodePort的端口号为20000<br/>
我们更新看看效果</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl delete svc kubia -n default
service &quot;kubia&quot; deleted
ubuntu@k8s-master:~/k8s-dev$ kubectl create -f kubia/service-nodeport.yaml -n default
service/kubia created
</code></pre>

<p>查看新的service</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl get svc -n default
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
kubernetes   ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP        26d
kubia        NodePort    10.111.0.106   &lt;none&gt;        80:30123/TCP   38s
</code></pre>

<p>对比之前的service，新的kubia服务多了一个30123端口，我们可以通过集群任意node节点的ip地址访问这个端口</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ curl http://127.0.0.1:30123
You&#39;ve hit kubia-8fffd4bff-622rl
</code></pre>

<h6 id="toc_13">Ingress</h6>

<p>ingress功能在k8s默认集群里没有提供，需要手动安装开启。ingress可以让应用在http层暴露给外部，通过配置不同的host来指向不同的服务</p>

<p>我们先为kubia这个应用创建一个ingress资源ingress.yaml</p>

<pre><code class="language-text">apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
  - host: kubia.dev.youxuetong.com
    http:
      paths:
        - path: /
          backend: 
            serviceName: kubia
            servicePort: 80
</code></pre>

<p>创建ingress资源并查看</p>

<pre><code class="language-text">ubuntu@k8s-master:~$ kubectl create -f k8s-dev/kubia/ingress.yaml -n default
ingress.networking.k8s.io/kubia created
ubuntu@k8s-master:~$ kubectl get ingress -n default
NAME    HOSTS                      ADDRESS   PORTS   AGE
kubia   kubia.dev.youxuetong.com             80      13s
</code></pre>

<p>我们试试通过域名访问这个服务</p>

<pre><code class="language-text">➜  ~ curl http://kubia.dev.youxuetong.com
You&#39;ve hit kubia-8fffd4bff-622rl
</code></pre>

<h3 id="toc_14">负载均衡</h3>

<p>k8s可以将网络流量随机分发(按照一定规则,在最新的1.17中，会选择一个最短路由的pod节点)到pod节点上。</p>

<p>我们可以看到每次访问同一服务，都可能会落到不同的pod节点上</p>

<pre><code class="language-text">➜  ~ curl http://kubia.dev.youxuetong.com
You&#39;ve hit kubia-8fffd4bff-622rl
➜  ~ curl http://kubia.dev.youxuetong.com
You&#39;ve hit kubia-8fffd4bff-622rl
➜  ~ curl http://kubia.dev.youxuetong.com
You&#39;ve hit kubia-8fffd4bff-622rl
➜  ~ curl http://kubia.dev.youxuetong.com
You&#39;ve hit kubia-8fffd4bff-6h64s
➜  ~ curl http://kubia.dev.youxuetong.com
You&#39;ve hit kubia-8fffd4bff-6h64s
➜  ~ curl http://kubia.dev.youxuetong.com
You&#39;ve hit kubia-8fffd4bff-6h64s
</code></pre>

<p>如果有些特殊应用，希望同一个用户的访问能指向同一个pod，比如处理socket长连接的应用，可以通过更改service的会话亲和性来达到效果</p>

<pre><code class="language-text">apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  sessionAffinity: ClientIP
  selector:
    app: kubia
  ports:
  - port: 80
    targetPort: 8080
</code></pre>

<p>重新创建service后我们来看看效果</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl delete svc kubia -n default
service &quot;kubia&quot; deleted
ubuntu@k8s-master:~/k8s-dev$ kubectl create -f kubia/service-session.yaml -n default
service/kubia created
</code></pre>

<p>利用curl访问</p>

<pre><code class="language-text">ubuntu@k8s-master:~$ curl http://127.0.0.1:30123
You&#39;ve hit kubia-8fffd4bff-6h64s
ubuntu@k8s-master:~$ curl http://127.0.0.1:30123
You&#39;ve hit kubia-8fffd4bff-6h64s
ubuntu@k8s-master:~$ curl http://127.0.0.1:30123
You&#39;ve hit kubia-8fffd4bff-6h64s
ubuntu@k8s-master:~$ curl http://127.0.0.1:30123
You&#39;ve hit kubia-8fffd4bff-6h64s
ubuntu@k8s-master:~$ curl http://127.0.0.1:30123
You&#39;ve hit kubia-8fffd4bff-6h64s
ubuntu@k8s-master:~$ curl http://127.0.0.1:30123
You&#39;ve hit kubia-8fffd4bff-6h64s    
</code></pre>

<h3 id="toc_15">配置文件</h3>

<p>k8s支持将一些敏感信息以及相关应用的配置文件单独存放，做到和应用无关。</p>

<p>创建ConfigMap,configmap.yaml</p>

<pre><code class="language-text">apiVersion: v1
kind: ConfigMap
metadata:
  name: kubia
  labels:
    app: kubia
   
data:
  config.json: |-
    {
      &quot;service&quot;:{
        &quot;host&quot;:&quot;http://kubia.dev.youxuetong.com&quot;,
        &quot;ip&quot;:&quot;10.9.22.1&quot;,
        &quot;port&quot;:80
      }    
    }
  config.yaml: |-
    service:
      host: &quot;http://kubia.dev.youxuetong.com&quot;
      ip: &quot;10.9.22.1&quot;
      port: 80
</code></pre>

<p>修改deployment.yaml</p>

<pre><code class="language-text">apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      volumes:
        - name: config
          configMap:
            name: kubia
      containers:
        - name: kubia
          image: luksa/kubia
          livenessProbe:
              httpGet:
                path: /
                port: 8080
          volumeMounts:
          - name: config
            mountPath: /opt/config
            readOnly: true
          ports:
            - containerPort: 8080
</code></pre>

<p>生成configmap，删除之前的deployment，创建新的</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl create -f kubia/configmap.yaml -n default
configmap/kubia created
ubuntu@k8s-master:~/k8s-dev$ kubectl delete deployment kubia -n default
deployment.apps/kubia deleted
ubuntu@k8s-master:~/k8s-dev$ kubectl create -f kubia/deployment-config.yaml -n default
deployment.apps/kubia created
</code></pre>

<p>我们进入pod中查看配置文件是否挂载成功</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl exec kubia-65cbb6d475-c5pqk ls /opt/config -n default
config.json
config.yaml
ubuntu@k8s-master:~/k8s-dev$ kubectl exec kubia-65cbb6d475-c5pqk cat /opt/config/config.json -n default
{
  &quot;service&quot;:{
    &quot;host&quot;:&quot;http://kubia.dev.youxuetong.com&quot;,
    &quot;ip&quot;:&quot;10.9.22.1&quot;,
    &quot;port&quot;:80
  }
}
ubuntu@k8s-master:~/k8s-dev$ kubectl exec kubia-65cbb6d475-c5pqk cat /opt/config/config.yaml -n default
service:
  host: &quot;http://kubia.dev.youxuetong.com&quot;
  ip: &quot;10.9.22.1&quot;
  port: 80
</code></pre>

<h3 id="toc_16">密钥管理</h3>

<p>密钥管理和配置文件类似，只是密钥当中存储的的是一些敏感信息而已。<br/>
我们以https证书举例，当我们在k8s当中，想给我们的域名绑上https证书的时候<br/>
先创建证书secret，需要实现准备好证书的key和证书文件</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev/ssl-dev.youxuetong.com$ kubectl create secret tls dev.youxuetong.com --cert=server.crt --key=server.key -n default
secret/dev.youxuetong.com created
</code></pre>

<p>这时我们如果想给之前通过ingress暴露的域名<a href="http://kubia.dev.youxuetong.com%E5%8A%A0%E4%B8%8Ahttps%E8%AF%81%E4%B9%A6%EF%BC%8C%E5%8F%AA%E9%9C%80%E8%A6%81%E4%BF%AE%E6%94%B9ingress.yaml">http://kubia.dev.youxuetong.com加上https证书，只需要修改ingress.yaml</a></p>

<pre><code class="language-text">apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: kubia
spec:
  rules:
  - host: kubia.dev.youxuetong.com
    http:
      paths:
        - path: /
          backend: 
            serviceName: kubia
            servicePort: 80
  tls:
  - hosts:
    - kubia.dev.youxuetong.com
    secretName: dev.youxuetong.com
</code></pre>

<p>增加tls字段，为kubia.dev.youxuetong.com 绑上之前创建好的secret<br/>
我们利用curl查看https证书有没有生效</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ curl -k -v https://kubia.dev.youxuetong.com/
*   Trying 125.46.60.5...
* Connected to kubia.dev.youxuetong.com (125.46.60.5) port 443 (#0)
* found 149 certificates in /etc/ssl/certs/ca-certificates.crt
* found 596 certificates in /etc/ssl/certs
* ALPN, offering http/1.1
* SSL connection using TLS1.2 / ECDHE_RSA_AES_256_GCM_SHA384
*    server certificate verification SKIPPED
*    server certificate status verification SKIPPED
*    common name: *.dev.youxuetong.com (matched)
*    server certificate expiration date OK
*    server certificate activation date OK
*    certificate public key: RSA
*    certificate version: #3
*    subject: C=CN,ST=HeNan,L=ZhengZhou,O=JiangShan Technology Co.Ltd,OU=JiangShan Technology Co.Ltd,CN=*.dev.youxuetong.com,EMAIL=yxt@youxuetong.com
*    start date: Fri, 03 Jan 2020 02:27:09 GMT
*    expire date: Mon, 17 May 2021 02:27:09 GMT
*    issuer: C=CN,ST=henan,L=zhengzhou,O=Jiangshan Co.Ltd,OU=Jiangshan Co.Ltd,CN=Jiangshan Co.Ltd,EMAIL=yxt@youxuetong.com
*    compression: NULL
* ALPN, server accepted to use http/1.1
&gt; GET / HTTP/1.1
&gt; Host: kubia.dev.youxuetong.com
&gt; User-Agent: curl/7.47.0
&gt; Accept: */*
&gt;
&lt; HTTP/1.1 200 OK
&lt; Server: openresty/1.15.8.2
&lt; Date: Tue, 14 Jan 2020 06:26:32 GMT
&lt; Transfer-Encoding: chunked
&lt; Connection: keep-alive
&lt; Strict-Transport-Security: max-age=15724800; includeSubDomains
&lt;
You&#39;ve hit kubia-65cbb6d475-c5pqk
</code></pre>

<h3 id="toc_17">统一存储</h3>

<p>k8s允许您自由选择的适合自己存储系统，无论是本地存储、远程目录共享还是云服务商提供的网络磁盘，并提供统一的分配和挂载方案。<br/>
存储分为静态存储和动态存储</p>

<h4 id="toc_18">静态存储</h4>

<p>静态存储需要事先在node上创建目录，分配空间，然后将目录挂载到pod上，和docker的目录挂载原理一样。这样的缺点就是每创建一个应用之前还需要手动创建目录，同时这样生成的pod的流动性就很差了，所以大部分情况下我们不会使用。</p>

<h4 id="toc_19">动态存储</h4>

<p>无需实现绑定，应用程序只需创建一个存储卷声明，声明所需磁盘空间大小和类型，即可自动完成创建和绑定。<br/>
我们先创建一个持久卷声明pvc.yaml</p>

<pre><code class="language-text">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: kubia-pvc
  labels:
    app: kubia
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
</code></pre>

<p>然后在deployment上绑定已经创建的pvc</p>

<pre><code class="language-text">apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubia
spec:
  replicas: 2
  selector:
    matchLabels:
      app: kubia
  template:
    metadata:
      name: kubia
      labels:
        app: kubia
    spec:
      volumes:
        - name: kubia-data
          persistentVolumeClaim:
            claimName: kubia-pvc
      containers:
        - name: kubia
          image: luksa/kubia
          livenessProbe:
              httpGet:
                path: /
                port: 8080
          volumeMounts:
          - name: kubia-data
            mountPath: /data
          ports:
            - containerPort: 8080
</code></pre>

<p>重新创建各项资源</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl create -f kubia/pvc.yaml -n default
persistentvolumeclaim/kubia-pvc created
ubuntu@k8s-master:~/k8s-dev$ kubectl get pvc -n default
NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
kubia-pvc   Bound    pvc-bb91d0ac-5076-41ba-b1fa-beb26473a0dc   10Gi       RWO            nfs-client     29s
ubuntu@k8s-master:~/k8s-dev$ kubectl delete deployment kubia -n default
deployment.apps &quot;kubia&quot; deleted
ubuntu@k8s-master:~/k8s-dev$ kubectl create -f kubia/deployment-pvc.yaml -n default
deployment.apps/kubia created
</code></pre>

<p>我们进入目录查看目录是否挂载成功,并创建一个文件</p>

<pre><code class="language-text">ubuntu@k8s-master:~/k8s-dev$ kubectl exec kubia-5bb69778fc-n2v6k touch /data/readme.md -n default
</code></pre>

<p>去NFS服务器查看文件是否成功创建</p>

<pre><code class="language-text">ubuntu@K8S-NFS:/data$ cd default-kubia-pvc-pvc-bb91d0ac-5076-41ba-b1fa-beb26473a0dc/
ubuntu@K8S-NFS:/data/default-kubia-pvc-pvc-bb91d0ac-5076-41ba-b1fa-beb26473a0dc$ ll
total 8
drwxrwxrwx  2 root root 4096 Jan 14 00:54 ./
drwxrwxrwx 21 root root 4096 Jan 14 00:51 ../
-rw-r--r--  1 root root    0 Jan 14 00:54 readme.md
</code></pre>

<h3 id="toc_20">统一日志</h3>

<p>利用EFK（elasticsearch,fluentd,kibaba）很容易做到对这个集群的日志跟踪。</p>

<p>在k8s里，Fluentd会被安装在每个Node节点上，同时会挂载当前机器的/var/lib/docker/containers目录，由于docker所有的标准输出都会在该目录下记录日志，所以Fluentd可以很容易的做到将整个集群上所允许的应用的日志统一发送到后端。</p>

<h3 id="toc_21">统一监控</h3>

<p>利用Metric-Server可以获取pod集群的资源占用情况的数据，在加上Prometheus和Grafana很容搭建一套集群的监控报警系统。</p>

<h3 id="toc_22">滚动升级&amp;回滚</h3>

<p>k8s提供逐步更新应用程序的能力，并提供不同的升级策略，让用户无感知情况下升级或者回滚应用程序。</p>

<h3 id="toc_23">自动伸缩</h3>

<p>k8s不仅提供手动的应用程序扩容机制，同时还可以通过监视应用程序的CPU使用率或其他度量增长时自动对应用程序进行扩容，已应用短期的高并发请求。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[K8S日志系统EFK]]></title>
    <link href="https://blog.xgtian.com/15774160287586.html"/>
    <updated>2019-12-27T11:07:08+08:00</updated>
    <id>https://blog.xgtian.com/15774160287586.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">elasticsearch</h2>

<pre><code class="language-text">helm install elasticsearch stable/elasticsearch -n share
</code></pre>

<h2 id="toc_1">fluentd</h2>

<pre><code class="language-text">helm repo add kiwigrid https://kiwigrid.github.io
helm repo update
helm install fluentd  kiwigrid/fluentd-elasticsearch --set elasticsearch.bufferChunkLimit=&quot;20M&quot;,elasticsearch.bufferQueueLimit=20 -n share
</code></pre>

<p><code>bufferChunkLimit</code> 一次发送数据的最大限制<br/>
<code>bufferQueueLimit</code> 待发送数据的最大数量限制</p>

<p>bufferChunkLimit * bufferQueueLimit 应小于PC内存</p>

<h2 id="toc_2">kibana</h2>

<pre><code class="language-text">helm install kibana stable/kibana --set ingress.enabled=true,ingress.hosts[0]=kibana.dev.youxuetong.com -n share
</code></pre>

<p>需要手动修改kibana的configmap，修改其中的elasticsearch.hosts</p>

<pre><code class="language-text">data:
  kibana.yml: |
    elasticsearch.hosts: http://elasticsearch-client:9200
    server.host: &quot;0&quot;
    server.name: kibana
</code></pre>

<p>删除正在运行的kibana的pod</p>

<pre><code class="language-text">kubectl delete pod kibana-557d4dc6b9-7dx5j -n share
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[WebDashboard UI部署]]></title>
    <link href="https://blog.xgtian.com/15773284865375.html"/>
    <updated>2019-12-26T10:48:06+08:00</updated>
    <id>https://blog.xgtian.com/15773284865375.html</id>
    <content type="html"><![CDATA[
<p>下载官方文件</p>

<pre><code class="language-text">wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta4/aio/deploy/recommended.yaml
</code></pre>

<p>修改文件,去除自建的secret</p>

<pre><code class="language-text">#---
## 由于证书问题，只能firefox浏览器才能打开，通过修改证书的方式，使得所有浏览器都能打开
#apiVersion: v1
#kind: Secret
#metadata:
#  labels:
#    k8s-app: kubernetes-dashboard
#  name: kubernetes-dashboard-certs  #生成证书会用到该名字
#  namespace: kubernetes-dashboard  #生成证书使用该命名空间
#type: Opaque
</code></pre>

<p>修改service为NodePort方式</p>

<pre><code class="language-text">kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  type: NodePort
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 32000
  selector:
    k8s-app: kubernetes-dashboard
</code></pre>

<p>创建自签名证书</p>

<pre><code class="language-text"># 创建目录使用证书
mkdir key &amp;&amp; cd key
# 查看是否存在namespace为kubernetes-dashboard
kubectl get namespaces
# 不存在namespace为创建kubernetes-dashboard创建namespace
kubectl create namespace kubernetes-dashboard
# 生成 key
openssl genrsa -out dashboard.key 2048
# 生成证书请求
openssl req -days 36000   -new -out dashboard.csr    -key dashboard.key   -subj &#39;/CN=**192.168.100.10**&#39;
# 生成自签证书
openssl x509 -req -in dashboard.csr -signkey dashboard.key -out dashboard.crt
# 目录结构
[root@k8smaster key]# ll
total 12
-rw-r--r-- 1 root root 1001 Oct 23 22:21 dashboard.crt
-rw-r--r-- 1 root root  903 Oct 23 22:20 dashboard.csr
-rw-r--r-- 1 root root 1679 Oct 23 22:20 dashboard.key
# 使用自签证书创建secret
kubectl create secret generic kubernetes-dashboard-certs     --from-file=dashboard.key     --from-file=dashboard.crt      -n kubernetes-dashboard

</code></pre>

<p>应用配置文件</p>

<pre><code class="language-text">kubectl create -f recommend.yaml
</code></pre>

<p>添加管理员并绑定管理员权限</p>

<pre><code class="language-text"># 创建sa
kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard
# 绑定集群管理员
kubectl create clusterrolebinding  dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin
</code></pre>

<p>获取登录token</p>

<pre><code class="language-text">kubectl describe secrets  $(kubectl  get secrets -n kubernetes-dashboard | awk  &#39;/dashboard-admin-token/{print $1}&#39; ) -n kubernetes-dashboard |sed -n &#39;/token:.*/p&#39;
</code></pre>

<p>浏览器访问<a href="https://IP:32000">https://IP:32000</a><br/>
如果Chrome浏览器仍然显示非安全连接，且详细信息中没有继续前往按钮，请用safari打开该连接，点击详细信息中的继续前往，按照系统只是操作，最后发现该自签名证书会被添加到钥匙串中，之后在用chrome浏览器打卡就不会出现无法访问的情况了。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ubuntu安装NFS-Server提供NFS服务]]></title>
    <link href="https://blog.xgtian.com/15773244836718.html"/>
    <updated>2019-12-26T09:41:23+08:00</updated>
    <id>https://blog.xgtian.com/15773244836718.html</id>
    <content type="html"><![CDATA[
<p>如果没有现成的NFS服务,可以利用现有服务器部署一套</p>

<p>安装NFS服务端</p>

<pre><code class="language-text">    sudo apt-get install nfs-kernel-server
</code></pre>

<p>编辑/etc/exports文件,增加你要提供nfs服务的路径</p>

<pre><code class="language-text"># /etc/exports: the access control list for filesystems which may be exported
#       to NFS clients.  See exports(5).
#
# Example for NFSv2 and NFSv3:
# /srv/homes       hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check)
#
# Example for NFSv4:
# /srv/nfs4        gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)
# /srv/nfs4/homes  gss/krb5i(rw,sync,no_subtree_check)
#

/data/charts *(rw,sync,no_subtree_check,no_root_squash)
/data/mysql *(rw,sync,no_subtree_check,no_root_squash)
/data/redis *(rw,sync,no_subtree_check,no_root_squash)
</code></pre>

<p>更改共享目录权限，修改为777 (目前还不清楚最低要求权限是多少)</p>

<pre><code class="language-text">sudo chown 777  /data/charts
</code></pre>

<p>重启NFS服务</p>

<pre><code class="language-text">sudo /etc/init.d/nfs-kernel-server restart
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[基础服务组件安装]]></title>
    <link href="https://blog.xgtian.com/15768388508712.html"/>
    <updated>2019-12-20T18:47:30+08:00</updated>
    <id>https://blog.xgtian.com/15768388508712.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">nginx-ingress</h2>

<p>以NodePort的形式负载均衡，每个node上都会绑定80、443端口</p>

<pre><code class="language-text">helm install nginx-ingress aliyun/nginx-ingress --set controller.service.type=NodePort,controller.service.externalTrafficPolicy=Local,controller.kind=DaemonSet,controller.daemonset.useHostPort=true -n kube-system
</code></pre>

<h2 id="toc_1">chartmuseum</h2>

<pre><code class="language-text">helm install chartmuseum aliyun/chartmuseum  --set persistence.pv.enabled=true,persistence.pv.nfs.server=10.9.122.86,persistence.pv.nfs.path=/data/charts/,persistence.pv.pvname=charts --set ingress.enabled=true   --set ingress.hosts[0].name=charts.youxuetong.com   --set ingress.hosts[0].path=/ --set ingress.hosts[0].tls=true --set ingress.hosts[0].tlsSecret=chart-secret,env.open.DISABLE_API=false,env.open.ALLOW_OVERWRITE=true,env.secret.BASIC_AUTH_USER=yxt,env.secret.BASIC_AUTH_PASS=9cWcXrHPZiDj7jwL
</code></pre>

<h2 id="toc_2">NFS动态存储</h2>

<pre><code class="language-text">helm install nfs-client-provisioner aliyun/nfs-client-provisioner --set nfs.server=192.168.122.9 --set nfs.path=/data -n share
</code></pre>

<p>如果需要将nfs-client当做默认storageclass，需要加上storageClass.defaultClass=true参数</p>

<pre><code class="language-text">helm install nfs-client-provisioner aliyun/nfs-client-provisioner --set nfs.server=192.168.122.9,nfs.path=/data,storageClass.defaultClass=true -n share
</code></pre>

<blockquote>
<p>需要所有node节点都安装nfs组件，否则挂载不上</p>

<pre><code class="language-text">sudo apt-get install nfs-common
</code></pre>
</blockquote>

<h2 id="toc_3">MySQL</h2>

<pre><code class="language-text">helm install mysql aliyun/mysql --set root.password=4zxsM3XcTgVtydCK,db.user=dev,db.password=QLdTNpi6QQqiMp7B,db.name=passport_dev -n share
</code></pre>

<h2 id="toc_4">Redis</h2>

<pre><code class="language-text">helm install redis aliyun/redis --set password=X2eH6BvahNqAuQkg,master.service.type=NodePort,master.service.nodePort=32765,slave.service.type=NodePort,slave.service.nodePort=32764 -n share
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HAProxy]]></title>
    <link href="https://blog.xgtian.com/15761310581680.html"/>
    <updated>2019-12-12T14:10:58+08:00</updated>
    <id>https://blog.xgtian.com/15761310581680.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0"> 验证配置文件</h2>

<pre><code class="language-text">    haproxy -c -f /etc/haproxy/haproxy.cfg
</code></pre>

<h2 id="toc_1">负载均衡配置</h2>

<pre><code class="language-text">frontend k8s-api

  bind 0.0.0.0:6443

  log global

  mode tcp

  default_backend k8s-api-server

backend k8s-api-server

  mode tcp

  log global

  balance roundrobin

  server ucloud-bj-k8s-master-01 10.9.142.180:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3

  server ucloud-bj-k8s-master-02 10.9.175.27:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3

  server ucloud-bj-k8s-master-03 10.9.91.143:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kubernets 卸载清理]]></title>
    <link href="https://blog.xgtian.com/15761212808457.html"/>
    <updated>2019-12-12T11:28:00+08:00</updated>
    <id>https://blog.xgtian.com/15761212808457.html</id>
    <content type="html"><![CDATA[
<pre><code class="language-text">kubeadm reset -f
modprobe -r ipip
lsmod
rm -rf ~/.kube/
rm -rf /etc/kubernetes/
rm -rf /etc/systemd/system/kubelet.service.d
rm -rf /etc/systemd/system/kubelet.service
rm -rf /usr/bin/kube*
rm -rf /etc/cni
rm -rf /opt/cni
rm -rf /var/lib/etcd
rm -rf /var/etcd


</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[k8s实用指令]]></title>
    <link href="https://blog.xgtian.com/15750110628081.html"/>
    <updated>2019-11-29T15:04:22+08:00</updated>
    <id>https://blog.xgtian.com/15750110628081.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">创建Docker私有仓库密钥</h2>

<pre><code class="language-text">kubectl create secret docker-registry regsecret --docker-server=registry.cn-hangzhou.aliyuncs.com --docker-username=yin32167@aliyun.com --docker-password=xxxxxx --docker-email=yin32167@aliyun.com
</code></pre>

<h2 id="toc_1">k8s简单端口转发</h2>

<p>转发至pod</p>

<pre><code class="language-text">kubectl port-forward --address 0.0.0.0 pod/mypod 8888:5000
</code></pre>

<p>转发至service</p>

<pre><code class="language-text">kubectl port-forward --address 0.0.0.0 services/myservice 8888:5000
</code></pre>

<h2 id="toc_2">移除node</h2>

<p>移除node需要先将node上的pod转移</p>

<pre><code class="language-text">kubectl drain k8s-node-storage --delete-local-data --force --ignore-daemonsets
</code></pre>

<p>然后利用delete命令删除node</p>

<pre><code class="language-text">kubectl delete node node-01
</code></pre>

<h2 id="toc_3">k8s创建tls secret</h2>

<pre><code class="language-text">kubectl create secret tls dev.youxuetong.com --cert=server.crt --key=server.key -n share
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[helm基础介绍]]></title>
    <link href="https://blog.xgtian.com/15743903332872.html"/>
    <updated>2019-11-22T10:38:53+08:00</updated>
    <id>https://blog.xgtian.com/15743903332872.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">helm安装</h2>

<h3 id="toc_1">3.0版本</h3>

<pre><code class="language-text">curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
</code></pre>

<h2 id="toc_2">helm仓库</h2>

<pre><code class="language-text">http://mirror.azure.cn/kubernetes/charts/
</code></pre>

<p>使用方法</p>

<pre><code class="language-text">helm repo add azure http://mirror.azure.cn/kubernetes/charts/
</code></pre>

<p>阿里云</p>

<pre><code class="language-text">helm repo add aliyun https://apphub.aliyuncs.com/
</code></pre>

<h2 id="toc_3">搜索公共仓库</h2>

<p><a href="https://hub.helm.sh/">仓库地址</a></p>

<pre><code class="language-text">helm search hub wordpress
</code></pre>

<h2 id="toc_4">搜索私有仓库</h2>

<pre><code class="language-text">helm search repo wordpress
</code></pre>

<h2 id="toc_5">Helm 如何管理多环境下 (Test、Staging、Production) 的业务配置？</h2>

<p>Chart 是支持参数替换的，可以把业务配置相关的参数设置为模板变量。使用 helm install 命令部署的时候指定一个参数值文件，这样就可以把业务参数从 Chart 中剥离了。例如： helm install --values=values-production.yaml wordpress。</p>

<h2 id="toc_6">helm push 插件</h2>

<pre><code class="language-text">helm plugin install https://github.com/chartmuseum/helm-push
</code></pre>

<p>上传包</p>

<pre><code class="language-text">helm push . http://192.168.1.24:8080
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux 测试磁盘性能]]></title>
    <link href="https://blog.xgtian.com/15595559159421.html"/>
    <updated>2019-06-03T17:58:35+08:00</updated>
    <id>https://blog.xgtian.com/15595559159421.html</id>
    <content type="html"><![CDATA[
<p>测试场景：</p>

<p>100%随机，100%读， 4K</p>

<pre><code class="language-text">fio -filename=/dev/emcpowerb -direct=1 -iodepth 1 -thread -rw=randread -ioengine=psync -bs=4k -size=1000G -numjobs=50 -runtime=180 -group_reporting -name=rand_100read_4k
</code></pre>

<p>100%随机，100%写， 4K</p>

<pre><code class="language-text">fio -filename=/dev/emcpowerb -direct=1 -iodepth 1 -thread -rw=randwrite -ioengine=psync -bs=4k -size=1000G -numjobs=50 -runtime=180 -group_reporting -name=rand_100write_4k
</code></pre>

<p>100%顺序，100%读 ，4K</p>

<pre><code class="language-text">fio -filename=/dev/emcpowerb -direct=1 -iodepth 1 -thread -rw=read -ioengine=psync -bs=4k -size=1000G -numjobs=50 -runtime=180 -group_reporting -name=sqe_100read_4k
</code></pre>

<p>100%顺序，100%写 ，4K</p>

<pre><code class="language-text">fio -filename=/dev/emcpowerb -direct=1 -iodepth 1 -thread -rw=write -ioengine=psync -bs=4k -size=1000G -numjobs=50 -runtime=180 -group_reporting -name=sqe_100write_4k
</code></pre>

<p>100%随机，70%读，30%写 4K</p>

<pre><code class="language-text">fio -filename=/dev/emcpowerb -direct=1 -iodepth 1 -thread -rw=randrw -rwmixread=70 -ioengine=psync -bs=4k -size=1000G -numjobs=50 -runtime=180 -group_reporting -name=randrw_70read_4k
</code></pre>

<p>io=执行了多少M的IO</p>

<p>bw=平均IO带宽<br/>
iops=IOPS<br/>
runt=线程运行时间<br/>
slat=提交延迟<br/>
clat=完成延迟<br/>
lat=响应时间<br/>
bw=带宽<br/>
cpu=利用率<br/>
IO depths=io队列<br/>
IO submit=单个IO提交要提交的IO数<br/>
IO complete=Like the above submit number, but for completions instead.<br/>
IO issued=The number of read/write requests issued, and how many of them were short.<br/>
IO latencies=IO完延迟的分布</p>

<p>io=总共执行了多少size的IO<br/>
aggrb=group总带宽<br/>
minb=最小.平均带宽.<br/>
maxb=最大平均带宽.<br/>
mint=group中线程的最短运行时间.<br/>
maxt=group中线程的最长运行时间.</p>

<p>ios=所有group总共执行的IO数.<br/>
merge=总共发生的IO合并数.<br/>
ticks=Number of ticks we kept the disk busy.<br/>
io_queue=花费在队列上的总共时间.<br/>
util=磁盘利用率</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Confluence安装]]></title>
    <link href="https://blog.xgtian.com/15535769948107.html"/>
    <updated>2019-03-26T13:09:54+08:00</updated>
    <id>https://blog.xgtian.com/15535769948107.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>启动镜像</p>
<pre><code class="language-text">docker run -d --restart always -v /home/ubuntu/confluence:/var/atlassian/application-data/confluence --name=&quot;confluence&quot; -d -p 80:8090 -p 8091:8091 atlassian/confluence-server:6.12
</code></pre></li>
</ol>

<h2 id="toc_0">下载</h2>

<p>官网下载confluence的tar包(v6.12.2)</p>

<h2 id="toc_1">安装</h2>

<p>解压缩（这里解压到/server/confluence目录），打开该目录下confluence/WEB-INF/classes/confluence-init.properties文件，添加行：</p>

<pre><code class="language-text">confluence.home=/server/confluence
</code></pre>

<h2 id="toc_2">启动</h2>

<blockquote>
<p>\( cd /server/confluence<br/>
\) ./bin/start-confluence.sh </p>
</blockquote>

<h2 id="toc_3">浏览器访问</h2>

<p>输入 <a href="http://localhost:8090/">http://localhost:8090/</a> 进行安装，期间会要求输入license，导向到官网申请之；进入数据库步骤，如果选择mysql，要依次逆行：</p>

<ol>
<li>下载mysql驱动放到WEB-INF/lib目录，重新启动confluence服务器</li>
<li><p>创建数据库时，编码COLLATE 应选：utf8_bin，如：</p>
<blockquote>
<p>CREATE DATABASE IF NOT EXISTS confluence DEFAULT CHARSET utf8 COLLATE utf8_bin; <br/>
如果没有创建数据用户，可以创建：<br/>
GRANT ALL ON <em>.</em> TO banyuan@localhost IDENTIFIED BY &quot;Banyuan2019&quot;; </p>
</blockquote></li>
<li><p>事务默认级别应设置为：READ-COMMITTED。具体操作过程可能如下：</p>
<ul>
<li>a.查看mysql的conf文件所在位置：<br/>
&gt; $ mysqld --verbose --help|grep -A 1 &#39;Default options&#39;<br/>
可能返回结果：/etc/my.cnf /etc/mysql/my.cnf /usr/local/etc/my.cnf ~/.my.cnf </li>
<li>b. 编辑my.conf，如：/usr/local/etc/my.cnf ，在 [mysqld]节增加行：
<code>
[mysqld]<br/>
...<br/>
transaction-isolation=READ-COMMITTED
</code>
然后重启服务。可以进入mysql控制台，执行下面的命令确认是否修改过了。<br/>
&gt; show variables like &#39;transaction_isolation&#39;;</li>
</ul></li>
</ol>

<p>以上这些步骤，在实际安装过程中都是有提示的，所以不必特别担心。</p>

<p>安装完毕后，仍会检查mysql的一些设置，比如可能提示如下两个变量设置值：</p>

<blockquote>
<p>成功最大允许数据包 - max_allowed_packet：一般不小于34m<br/>
InnoDB 日志文件大小 - innodb_log_file_size：一般不小于256M</p>

<p>character-set-server = utf8<br/>
innodb_log_file_size = 256M<br/>
max_allowed_packet = 34M</p>
</blockquote>

<h2 id="toc_4">破解</h2>

<p>破解需要在安装过程中进行，安装进入到授权码页面时，开始破解：</p>

<ol>
<li>使用【附件】目录confluence-crack.zip解压缩，比如解到/cracked目录。</li>
<li>把confluence的WEB-INF/lib/atlassian-extras-decoder-v2-3.4.1.jar 文件复制到/cracked目录，重命名为atlassian-extras-2.4.jar</li>
<li>运行/cracked目录下的confluence_keygen.jar，点.patch!，依提示选择atlassian-extras-2.4.jar，就可以在/cracked目录看到atlassian-extras-2.4.jar和atlassian-extras-2.4.bak两个文件，这里atlassian-extras-2.4.jar已经是破解好的了；</li>
<li>然后在patch软件里输入安装Web页上的Server Id，点击.gen!产生key</li>
<li>将atlassian-extras-2.4.jar名字改回来：atlassian-extras-decoder-v2-3.4.1，复制回confluence的WEB-INF/lib/目录，重新启动confluence</li>
<li>复制key到Web页，进行下一步<br/>
## 邮件服务器配置<br/>
直接填写地址、协议死活不工作，不得已配置了JNDI：</li>
</ol>

<p>A. 在{confluence-install}/conf/server.xml中<Context>的末尾加代码：</p>

<pre><code class="language-text">&lt;Context path=&quot;&quot; docBase=&quot;../confluence&quot; debug=&quot;0&quot; reloadable=&quot;false&quot; useHttpOnly=&quot;true&quot;&gt;
....
&lt;Resource name=&quot;mail/BanyuanSMTPServer&quot;
    auth=&quot;Container&quot;
    type=&quot;javax.mail.Session&quot;
    mail.smtp.host=&quot;smtp.banyuan.club&quot;
    mail.smtp.port=&quot;465&quot;
    mail.smtp.auth=&quot;true&quot;
    mail.smtp.user=&quot;admin@banyuan.club&quot;
    password=&quot;SemiCircle2019&quot;
    mail.smtp.starttls.enable=&quot;true&quot;
    mail.transport.protocol=&quot;smtps&quot;
    mail.smtp.socketFactory.class=&quot;javax.net.ssl.SSLSocketFactory&quot;
/&gt;
&lt;/Context&gt;
</code></pre>

<p>B. 然后移动{confluence-install}/confluence/WEB-INF/lib/mail-x.x.x.jar到{confluence-install}/lib目录（移动不是拷贝）<br/>
C. 重启服务器，<br/>
D. 进入后台邮件服务器配置JNDI服务名为：java:comp/env/mail/BanyuanSMTPServer，注意：SMTP单独配置选项都要为空（可以只能配置用户名，同mail.smtp.user），然后发送测试邮件，通过。</p>

<h2 id="toc_5">RestAPI调用</h2>

<p>官方文档： <a href="https://developer.atlassian.com/server/confluence/confluence-server-rest-api/">https://developer.atlassian.com/server/confluence/confluence-server-rest-api/</a></p>

<p>例：<br/>
获取文章内容：<a href="http://localhost:8090/rest/api/content/327696?expand=body.storage">http://localhost:8090/rest/api/content/327696?expand=body.storage</a><br/>
获取文章里宏：<a href="http://localhost:8090/rest/api/content/327696/history/2/macro/id/0efe0e6f-e80e-4528-81d8-968f47a87907">http://localhost:8090/rest/api/content/327696/history/2/macro/id/0efe0e6f-e80e-4528-81d8-968f47a87907</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mac 安装MiniKube]]></title>
    <link href="https://blog.xgtian.com/15526315084700.html"/>
    <updated>2019-03-15T14:31:48+08:00</updated>
    <id>https://blog.xgtian.com/15526315084700.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>安装kubectl</p>
<pre><code class="language-text">brew install kubernetes-cli
</code></pre></li>
<li><p>安装minikube</p>
<p>官方原版，需要科学上网</p>
<pre><code class="language-text">brew cask install minikube
</code></pre>
<p>阿里云的minikube</p>
<pre><code class="language-text">curl -Lo minikube http://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/releases/v0.30.0/minikube-darwin-amd64 &amp;&amp; chmod +xminikube &amp;&amp; sudo mv minikube /usr/local/bin/
</code></pre></li>
<li><p>minikube 启动命令</p>
<pre><code class="language-text">minikube start --docker-env HTTP_PROXY=192.168.2.114:31210 --docker-env HTTPS_PROXY=192.168.2.114:31210
</code></pre></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python 单元测试以及单元测试覆盖率]]></title>
    <link href="https://blog.xgtian.com/15451201223625.html"/>
    <updated>2018-12-18T16:02:02+08:00</updated>
    <id>https://blog.xgtian.com/15451201223625.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">单元测试</h2>

<ol>
<li><p>安装nosetest</p>
<pre><code class="language-text">pip install nose
</code></pre></li>
<li><p>执行测试并输出xunit格式xml文件</p>
<pre><code class="language-text">nosetest --with-xunit -v
</code></pre>
<p>如果你想指定输出的文件名</p>
<pre><code class="language-text">nosetest --with-xunit --xunit-file=xunittest.xml -v
</code></pre></li>
</ol>

<h2 id="toc_1">测试覆盖率</h2>

<ol>
<li><p>安装coverage</p>
<pre><code class="language-text">pip install coverage
</code></pre></li>
<li><p>测试指定的测试代码或者模块</p>
<pre><code class="language-text">coverage run test.py
</code></pre>
<p>以上操作会在当前目录下生成<code>.coverage</code>目录</p></li>
<li><p>生成xml报告或者html报告</p>
<pre><code class="language-text">coverage html
coverage xml
</code></pre></li>
</ol>

<h2 id="toc_2">在Django 中使用nose和coverage</h2>

<p>如果仅仅是使用coverage</p>

<pre><code class="language-text">coverage run manage.py test #执行django的单元测试
coverage xml #生成xml报告
</code></pre>

<p>如果需要同时使用nosetest和coverage</p>

<ol>
<li><p>下载安装django-nose</p>
<pre><code class="language-text">pip install django-nose
</code></pre></li>
<li><p>将django-nose添加到项目的setting.py文件当中</p>
<pre><code class="language-text">INSTALLED_APPS = (
...<br/>
&#39;django_nose&#39;, # Append to INSTALLED_APPS<br/>
...<br/>
)<br/>
TEST_RUNNER = &#39;django_nose.NoseTestSuiteRunner&#39;<br/>
NOSE_ARGS = [<br/>
   &#39;--with-coverage&#39;,<br/>
    &#39;--with-xunit&#39;,<br/>
    &#39;--xunit-file=xunittest.xml&#39;<br/>
]
</code></pre></li>
<li><p>执行单元测试</p>
<pre><code class="language-text">python manage.py test
</code></pre></li>
<li><p>生成单元测试覆盖率</p>
<pre><code class="language-text">coverage xml
</code></pre></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Redmine Docker部署]]></title>
    <link href="https://blog.xgtian.com/15446089404193.html"/>
    <updated>2018-12-12T18:02:20+08:00</updated>
    <id>https://blog.xgtian.com/15446089404193.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>拉取镜像</p>
<pre><code class="language-text">docker pull redmine
</code></pre></li>
<li><p>启动docker镜像</p>
<pre><code class="language-text">docker run -d --name redmine \
-e REDMINE_DB_MYSQL=192.168.2.191 \<br/>
-e REDMINE_DB_USERNAME=root \<br/>
-e REDMINE_DB_PASSWORD=www.1234TV.com \<br/>
-e REDMINE_DB_DATABASE=redmine \<br/>
-p 80:3000 redmine   
</code></pre></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GitLab和Redmine深度集成]]></title>
    <link href="https://blog.xgtian.com/15445994167623.html"/>
    <updated>2018-12-12T15:23:36+08:00</updated>
    <id>https://blog.xgtian.com/15445994167623.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">Redmine Issue Tracker</h2>

<ol>
<li><p>设置Redmine Issue Tracker<br/>
在GitLab的项目setting里面找到integrations，找到redmine<br/>
修改如下:<br/>
<img src="https://pic.mylonly.com/2018-12-12-072609.png" alt=""/></p></li>
<li><p>关闭内置issue</p>
<p>setting-&gt;general-&gt;Permissions</p>
<p><img src="https://pic.mylonly.com/2018-12-12-072807.png" alt=""/></p></li>
</ol>

<h2 id="toc_1">Redmine 启用版本库</h2>

<ol>
<li><p>进入redmine容器</p>
<pre><code class="language-text">docker exec -it redmine /bin/bash
</code></pre></li>
<li><p>安装插件<code>redmine_gitlab_hook</code></p>
<p>进入<code>/usr/src/redmine/plugins</code>目录</p>
<pre><code class="language-text">git clone https://github.com/phlegx/redmine_gitlab_hook.git
</code></pre>
<p>登录redmine管理员，管理-&gt;插件，找到<code>Redmine GitLab Hook plugin</code>,进入配置</p>
<p><img src="https://pic.mylonly.com/2018-12-12-101517.png" alt=""/></p></li>
<li><p>创建本地git仓库并拉取</p>
<pre><code class="language-text">mkdir -p /home/redmine/git-repo #创建本地git仓库
cd /home/redmine/git-repo/<br/>
git clone --mirror http://username:password@gitlab.1234tv.lan:/awesome/python-test.git #此处最好采用http加上用户名和密码的方式拉取git仓库<br/>
chmod -R redmine:redmine python-test.git ##记得修改用户组为redmine:redmine
</code></pre></li>
<li><p>登录redmine管理员，启用版本库</p>
<p><img src="https://pic.mylonly.com/2018-12-12-073502.png" alt=""/></p></li>
<li><p>配置版本库</p>
<p>进入到和gitlab项目对应的项目中，进入设置页面<br/>
git clone时加了-mirror参数,使用下面参数<br/>
<img src="https://pic.mylonly.com/2018-12-12-101134.png" alt=""/><br/>
如果git clone 时没有使用-mirror参数，使用下面的配置<br/>
<img src="https://pic.mylonly.com/2018-12-12-073658.png" alt=""/></p></li>
</ol>

<h2 id="toc_2">配置GitLab Webhook</h2>

<p>进入GitLab的项目中，在setting-&gt;Integrations中添加如下webhooks地址</p>

<p>webhook url格式:</p>

<pre><code class="language-text">{redmine_installation_url}/gitlab_hook?key={redmine_repository_API_key}&amp;project_id={redmine_project_identifier}
</code></pre>

<p><img src="https://pic.mylonly.com/2018-12-12-101954.png" alt=""/></p>

<p>如果redmine的项目名称和版本库的标识不一致，需要带上<code>repository_name</code>参数，手动指定redmine上的版本库</p>

<p>如果需要gitlab webhook插件自动创建版本库,还需要加上<code>repository_git_url</code>、<code>repository_namespace</code>、<code>repository_name</code>这几个参数,</p>

<p>其中<code>repository_git_url</code>为需要克隆的远程仓库的地址<br/>
而<code>repository_namespace</code>和<code>repository_name</code>自由填写，会在redmine的版本库里生成类似 <code>{repository_namespace}_{repository_name}</code>样式的版本库标识</p>

<h2 id="toc_3">验证</h2>

<ol>
<li>在redmine里创建一个issue，状态为新建</li>
<li>本地clone项目，少许修改，commit日志填写&quot;bugfix #<id>&quot; (<code>id为redmine上的issue id</code>),然后push 到服务器上</li>
<li>查看redmine上该issue的状态是否改变</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GitLabRunner配合SonarScanner针对每次commit做代码检查]]></title>
    <link href="https://blog.xgtian.com/15445172404972.html"/>
    <updated>2018-12-11T16:34:00+08:00</updated>
    <id>https://blog.xgtian.com/15445172404972.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">Sonar GitLab-Plugin插件以及配置</h2>

<ol>
<li><p>下载安装插件</p>
<p>admin登录SonarQube，点击 配置 —&gt; 系统 —&gt; 更新中心 —&gt; Available —&gt; Search，输入 GitLab，在列表中点击 install 安装，安装完毕后重启 SonarQube 即可<br/>
<img src="https://pic.mylonly.com/2018-12-11-093357.png" alt=""/></p></li>
<li><p>在GitLab上注册一个Sonarqube账号，获取Private Token</p></li>
</ol>

<p><img src="https://pic.mylonly.com/2018-12-11-093529.png" alt=""/></p>

<ol>
<li>回到SonarQube 配置插件
<img src="https://pic.mylonly.com/2018-12-11-093642.png" alt=""/><br/></li>
</ol>

<h2 id="toc_1">Sonar Scanner 相关配置</h2>

<p>依然采用docker方式</p>

<ol>
<li><p>拉取镜像</p>
<pre><code class="language-text">docker pull ciricihq/gitlab-sonar-scanner
</code></pre></li>
<li><p>在sonarqube上新建要测试的项目<br/>
按照向导创建完项目之后记住所设置的<code>key</code><br/>
<img src="https://pic.mylonly.com/2018-12-11-094008.png" alt=""/></p></li>
<li><p>回到GitLab,在需要进行质量检测的项目中加入<code>.gitlab-ci.yml</code>,在末尾添加如下代码：</p>
<pre><code class="language-text">stages:
- analysis<br/>
sonarqube:<br/>
  stage: analysis<br/>
  image: ciricihq/gitlab-sonar-scanner<br/>
  variables:<br/>
    SONAR_URL: http://sonarqube.domain.lan #你的sonarqube地址<br/>
    SONAR_ANALYSIS_MODE: issues<br/>
  script:<br/>
  - gitlab-sonar-scanner<br/>
sonarqube-reports:<br/>
  stage: analysis<br/>
  image: ciricihq/gitlab-sonar-scanner<br/>
  variables:<br/>
    SONAR_URL: http://sonarqube.domain.lan #你的sonarqube地址<br/>
    SONAR_ANALYSIS_MODE: publish<br/>
  script:<br/>
  - gitlab-sonar-scanner
</code></pre></li>
<li><p>在项目根目录添加sonar scanner 配置文件 <code>sonar-project.properties</code></p>
<pre><code class="language-text">sonar.projectKey=&lt;sonarqube上创建项目时填写的key&gt;
sonar.sources=.<br/>
sonar.gitlab.project_id=git@gitlab.domain.lan/awesome/xxx.git ##你的git仓库地址
</code></pre></li>
</ol>

<h2 id="toc_2">GitLab Runner注册</h2>

<ol>
<li><p>拉取镜像</p>
<pre><code class="language-text">docker pull gitlab/gitlab-runner
</code></pre></li>
<li><p>利用docker注册镜像</p>
<pre><code class="language-text">docker run --rm -t -i -v /path/to/config:/etc/gitlab-runner --name  gitlab-runner gitlab/gitlab-runner register \
  --non-interactive \<br/>
  --executor &quot;docker&quot; \<br/>
  --docker-image alpine:3 \<br/>
  --url &quot;https://gitlab.com/&quot; \<br/>
      --registration-token &quot;PROJECT_REGISTRATION_TOKEN&quot; \<br/>
  --description &quot;docker-runner&quot; \<br/>
  --tag-list &quot;docker,aws&quot; \<br/>
  --run-untagged \<br/>
  --locked=&quot;false&quot;
</code></pre></li>
<li><p>启动GitLab Runner</p>
<pre><code class="language-text">docker run -d --name gitlab-runner --restart always \
 -v /srv/gitlab-runner/config:/etc/gitlab-runner \<br/>
 -v /var/run/docker.sock:/var/run/docker.sock \<br/>
 gitlab/gitlab-runner
</code></pre></li>
</ol>

<h2 id="toc_3">测试验证</h2>

<p>修改代码提交到gitlab，观察gitlab的pipeline的状态，以及sonarqube上新建项目的状态。</p>

<p><img src="https://pic.mylonly.com/2018-12-11-094744.png" alt="GitLab Pipeline"/><br/>
<img src="https://pic.mylonly.com/2018-12-11-094806.png" alt="Sonarqube"/></p>

<h2 id="toc_4">问题</h2>

<p>观察redmine的日志，如果出现权限问题，请注意仓库的用户组需要为redmine:redmine</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SonarQube部署]]></title>
    <link href="https://blog.xgtian.com/15444340190835.html"/>
    <updated>2018-12-10T17:26:59+08:00</updated>
    <id>https://blog.xgtian.com/15444340190835.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">部署postgres</h2>

<ol>
<li><p>拉取镜像</p>
<pre><code class="language-text">docker pull postgres
</code></pre></li>
<li><p>启动镜像</p>
<pre><code class="language-text">docker run --name postgres \
-e POSTGRES_USER=&quot;sonar&quot; \<br/>
-e POSTGRES_PASSWORD=****** \<br/>
-d postgres
</code></pre></li>
</ol>

<h2 id="toc_1">部署SonarQube</h2>

<ol>
<li><p>拉取镜像</p>
<pre><code class="language-text">docker pull sonarqube
</code></pre></li>
<li><p>启动镜像</p>
<pre><code class="language-text">docker run -d --name sonarqube\
    -p 80:9000     \<br/>
    -e sonar.jdbc.username=sonar \<br/>
    -e sonar.jdbc.password=****** \    <br/>
    -e sonar.jdbc.url=jdbc:postgresql://192.168.2.191/sonar sonarqube
</code></pre></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[利用SoftetherVPN和Bind9穿透内网，共享局域网内部资源]]></title>
    <link href="https://blog.xgtian.com/15440788666202.html"/>
    <updated>2018-12-06T14:47:46+08:00</updated>
    <id>https://blog.xgtian.com/15440788666202.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">写在前面</h2>

<p>我想很多公司由于安全考虑，一些内部系统(Git仓库、OA之类）的一定是放在公司内部的服务器上，只有在公司的网络下才能够访问，但是一些公司的随着业务的发展，肯定会出现一些外地的同事（外地的办事处、研发中心、销售团队等等）这种情况，这样一来，在内部服务不迁移到外网的前提下，如何让外地的同事能访问到内网这些资源就成了一个必须要解决的问题了。<br/>
其实内网穿透有很很多种解决方案，本文只是利用SoftetherVPN以及Bind9这两个工具提供另一种解决思路。<br/>
本文所提到的解决方案有如下优点:</p>

<pre><code class="language-text">1. 原来的内部资源不需要额外的配置改动
2. 需要身份认证，只有拥有权限的外部人员才能访问内部资源
3. 只有加入了共享网络的内部资源才能被外部访问，没有开放共享的内部资源的无法访问
4. 可以做到通过相同的域名访问同一个内部资源
5. 通过LPTP,LPSec协议支持OS X以及iOS设备访问
</code></pre>

<p>当然也有如下不足:<br/>
    1. 需要一台拥有公网IP的服务器作为中心服务器<br/>
    2. 由于本文的解决方案是基于虚拟局域网来实现的，因此拥有局域网内一些无法回避的缺点。</p>

<h2 id="toc_1">原理</h2>

<p><img src="https://pic.mylonly.com/2018-12-06-%E7%BD%91%E7%BB%9C%E5%9B%BE%20-1-.png" alt=""/><br/>
本解决方案利用了SoftetherVPN提供的Ad-Hoc VPN将内网服务器和外部用户通过VLAN连接在同一个局域网内，然后通过支持智能解析的DNS服务器，将域名解析到对应的虚拟局域网IP。<br/>
如果是内网用户，则直接通过DNS服务器将域名解析到内部资源对应的内网IP。</p>

<h2 id="toc_2">部署</h2>

<p>本次部署大部分采用Docker方式部署，如果需要手动安装的，可以在<a href="https://www.softether-download.com/cn.aspx?product=softether">这个网站</a>下载对应的安装包</p>

<h3 id="toc_3">Docker 环境</h3>

<p>略，不再本文讨论范围之内</p>

<h3 id="toc_4">安装Softether Server</h3>

<ol>
<li><p>拉取镜像</p>
<pre><code class="language-text">siomiz/softethervpn
</code></pre></li>
<li><p>启动vpnserver</p>
<pre><code class="language-text">docker run -d --cap-add NET_ADMIN \
 -p 500:500/udp \<br/>
 -p 4500:4500/udp \<br/>
 -p 1701:1701/tcp \<br/>
 -p 1194:1194/udp \<br/>
 -p 5555:5555/tcp \<br/>
 -p 443:443/tcp \<br/>
 -p 992:992 \<br/>
 siomiz/softethervpn
</code></pre>
<p>记得要开放对应的端口</p></li>
</ol>

<h3 id="toc_5">安装Softether ServerManger</h3>

<p>虽然Softether 提供了mac版的安装包，但本人装完之后发现其实是套了一个wine的环境，在mac上体验极差，所以此处推荐找一个windows电脑安装vpn server manager，<a href="https://www.softether-download.com/cn.aspx?product=softether">下载地址</a>在此<br/>
由于是windows上的安装教程，大部分都是图片，为了不占文章篇幅，就不放在文章里介绍了，各位可以参考<a href="https://www.softether.org/4-docs/2-howto/1.VPN_for_On-premise/1.Ad-hoc_VPN">官网</a>或者这篇<a href="http://www.leqii.com/download/yulan/d77/d77.html">博客</a></p>

<p>其中需要注意的是，在<code>虚拟HUB管理页面</code>中的<code>虚拟NAT和虚拟DHCP服务器</code>，点击<code>SecureNAT</code>,然后设置DHCP服务器的网段，以及DNS服务器为下面将要搭建的DNS服务器地址<br/>
<img src="https://pic.mylonly.com/2018-12-06-101214.png" alt=""/></p>

<h3 id="toc_6">部署DNS服务器</h3>

<ol>
<li><p>安装bind9</p>
<pre><code class="language-text">sudo apt-get install bind9
</code></pre></li>
<li><p>增加两个视图文件,在/etc/bind/目录下<br/>
name.2.conf</p>
<pre><code class="language-text">acl &quot;2&quot; {
    192.168.2.0/24;<br/>
};   
</code></pre>
<p>name.111.conf </p>
<pre><code class="language-text">acl &quot;111&quot; {
    192.168.111.0/24;<br/>
};
</code></pre>
<p>两个视图文件主要是为了区分来源IP所在的网段，然后根据不同的网段返回不同的解析结果，在本解决方案当中，192.168.2.0/24网段为内网网段，   192.168.111.0/24为VPN生成的虚拟局域网的网段</p></li>
<li><p>新建不同网段对应的不同的解析记录文件，也在/etc/bind/目录下</p>
<p>1234tv.lan-2.zone</p>
<pre><code class="language-text">$TTL    604800
@       IN      SOA      xxxx.xx. root.xxxx.xx. (<br/>
                              2         ; Serial<br/>
                         604800         ; Refresh<br/>
                          86400         ; Retry<br/>
                        2419200         ; Expire<br/>
                         604800 )       ; Negative Cache TTL<br/>
;<br/>
@       IN      NS      xxxx.xx.<br/>
@       IN      A      192.168.2.222<br/>
gitlab  IN      A       192.168.2.84
</code></pre>
<p>1234tv.lan-111.zone</p>
<pre><code class="language-text">$TTL    604800
@       IN      SOA     xxxx.xx. root.xxxx.xx. (<br/>
                              2         ; Serial<br/>
                         604800         ; Refresh<br/>
                          86400         ; Retry<br/>
                        2419200         ; Expire<br/>
                         604800 )       ; Negative Cache TTL<br/>
;<br/>
@       IN      NS      xxxx.xx.<br/>
@       IN      A       192.168.111.222<br/>
gitlab  IN      A       192.168.111.84
</code></pre></li>
<li><p>将视图文件和解析文件写入Bind9主配置文件</p>
<pre><code class="language-text">sudo vim /etc/bind/name.conf
</code></pre>
<p>增加如下内容，声明两个视图，view_2,view_11,可以注释掉默认带的配置文件，如果你没有其他的用途的话,注意每一行后面的分号，不可缺少。</p>
<pre><code class="language-text">#include &quot;/etc/bind/named.conf.options&quot;;
#include &quot;/etc/bind/named.conf.local&quot;;<br/>
#include &quot;/etc/bind/named.conf.default-zones&quot;;<br/>
include &quot;/etc/bind/name.2.conf&quot;;<br/>
view &quot;View_2&quot; {<br/>
        match-clients {&quot;2&quot;;};<br/>
        zone &quot;xxxx.xxx&quot; IN {<br/>
                type master;<br/>
                file &quot;xxxx.xxx-2.zone&quot;;<br/>
                allow-update {none;};<br/>
        };<br/>
};<br/>
include &quot;/etc/bind/name.111.conf&quot;;<br/>
view &quot;View_111&quot; {<br/>
        match-clients {&quot;111&quot;;};<br/>
        zone &quot;xxxx.xxx&quot; IN {<br/>
                type master;<br/>
                file &quot;xxxx.xxx-111.zone&quot;;<br/>
                allow-update {none;};<br/>
        };<br/>
};
</code></pre></li>
<li><p>然后启动Bind9 服务</p>
<pre><code class="language-text">sudo /etc/init.d/bind9 start
</code></pre></li>
<li><p>验证解析</p>
<pre><code class="language-text">dig gitlab.xxx.xx @192.168.2.222
</code></pre>
<p><img src="media/15440788666202/15440920519061.jpg" alt="" style="width:460px;"/></p></li>
</ol>

<h4 id="toc_7">安装Vpn Client</h4>

<p>所有需要共享给外部访问的内部服务器都需要安装vpn client连接上vpn server，下面的方法为linux 下docker方式部署vpn_client方法，windows端请参考这篇<a href="http://www.leqii.com/download/yulan/d77/d77.html">文章</a></p>

<ol>
<li><p>拉取镜像</p>
<pre><code class="language-text">sudo docker pull mitsutaka/softether-vpnclient
</code></pre></li>
<li><p>启动镜像连接之前创建的vpn server</p>
<pre><code class="language-text">docker run -d --name=softether-vpnclient \
--net=host --privileged \<br/>
-e VPN_SERVER=&lt;Softether VPN server&gt; \<br/>
-e VPN_PORT=&lt;Softether VPN port&gt; \<br/>
-e ACCOUNT_USER=&lt;Registered username&gt; \<br/>
-e ACCOUNT_PASS=&lt;Registered password&gt; \<br/>
-e VIRTUAL_HUB=&lt;Virtual Hub name&gt; \<br/>
-e TAP_IPADDR=&lt;IP address/netmask&gt; \<br/>
mitsutaka/softether-vpnclient<br/>
```<br/>
服务器端，VIRTUAL_HUB端填入之前在manager上新建的虚拟HUB名称 ，TAP_IPADDR，最好采用指定IP的方式，尤其是DNS服务器，需要固定IP地址
</code></pre></li>
</ol>

<h2 id="toc_8">结束语</h2>

<p>至此，这个简单的内网穿透方案算是部署完成，内网用户可以直接通过设置内网DNS服务器来通过域名访问内部服务器，外网用户在使用vpn clinet连接上虚拟局域网之后也能通过相同的域名访问内部服务器资源。<br/>
写这篇文章，一方面是觉得自己以后可能还会有此需求，怕自己遗忘，权且当做记录。另外也是想也可能有别的同学有这方面的需要，写出来共享一下。</p>

]]></content>
  </entry>
  
</feed>
