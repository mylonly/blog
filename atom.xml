<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[独自一人]]></title>
  <link href="https://blog.xgtian.com/atom.xml" rel="self"/>
  <link href="https://blog.xgtian.com/"/>
  <updated>2020-01-10T14:41:32+08:00</updated>
  <id>https://blog.xgtian.com/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[k8s-1.16高可用集群部署]]></title>
    <link href="https://blog.xgtian.com/15786401810709.html"/>
    <updated>2020-01-10T15:09:41+08:00</updated>
    <id>https://blog.xgtian.com/15786401810709.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">机器要求</h2>

<p>高可用集群一般用于生产环境，官方推荐至少需要3台master节点机器，4台node节点机器</p>

<table>
<thead>
<tr>
<th>Hostname</th>
<th>IP</th>
<th>Role</th>
</tr>
</thead>

<tbody>
<tr>
<td>ucloud-bj-k8s-master-01</td>
<td>10.9.142.180</td>
<td>Master Node</td>
</tr>
<tr>
<td>ucloud-bj-k8s-node-01</td>
<td>10.9.165.222</td>
<td>Worker Node</td>
</tr>
<tr>
<td>ucloud-bj-k8s-node-02</td>
<td>10.9.127.58</td>
<td>Worker Node</td>
</tr>
<tr>
<td>ucloud-bj-K8s-node-03</td>
<td>10.9.57.4</td>
<td>Worker Node</td>
</tr>
<tr>
<td>ucloud-bj-K8s-node-04</td>
<td>10.9.174.192</td>
<td>Worker Node</td>
</tr>
</tbody>
</table>

<h2 id="toc_1">安装准备</h2>

<ol>
<li><p>禁用Swap<br/>
k8s为了使容器的调度更符合机器的实际资源情况，k8s建议关闭内存交换   </p>
<pre><code class="language-text">swapoff -a
</code></pre>
<p>同时删除<code>/etc/fstab</code>中swap那条记录</p>
<p>当然，如果你的机器资源确实不多，需要利用swap，那么你可以不关闭swap交换空间，通过如下参数告诉k8s开启swap</p>
<pre><code class="language-text">kubelet --fail-swap-on=false ...
</code></pre></li>
<li><p>端口开放(生产环境)</p>
<p><img src="https://pic.mylonly.com/2020-01-10-070912.jpg" alt=""/></p></li>
</ol>

<h2 id="toc_2">Docker安装(可选)</h2>

<p>由于k8s需要安装指定docker18.06版本，所以如果你的版本不对，可以先卸载重新安装</p>

<pre><code class="language-text">sudo apt-get remove docker docker-engine docker-ce docker.io
</code></pre>

<p>安装docker</p>

<pre><code class="language-text"># 从 Ubuntu 的存储库安装 Docker：
apt-get update
apt-get install -y docker.io

# 或者从 Docker 的 Ubuntu 或 Debian 镜像仓库中安装 Docker CE 18.06：

## 安装环境准备。
apt-get update &amp;&amp; apt-get install apt-transport-https ca-certificates curl software-properties-common

## 下载 GPG 密钥。
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -

## 添加 docker apt 镜像仓库。
add-apt-repository \
   &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable&quot;

## 安装 docker。
apt-get update &amp;&amp; apt-get install docker-ce=18.06.0~ce~3-0~ubuntu

# 设置守护进程。
cat &gt; /etc/docker/daemon.json &lt;&lt;EOF
{
  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],
  &quot;log-driver&quot;: &quot;json-file&quot;,
  &quot;log-opts&quot;: {
    &quot;max-size&quot;: &quot;100m&quot;
  },
  &quot;storage-driver&quot;: &quot;overlay2&quot;
}
EOF

mkdir -p /etc/systemd/system/docker.service.d

# 重启 docker。
systemctl daemon-reload
systemctl restart docker
</code></pre>

<h2 id="toc_3">安装 <code>kubelet</code>,<code>kubeadm</code>,<code>kubectl</code></h2>

<p>添加阿里源密钥</p>

<pre><code class="language-text">curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - 
</code></pre>

<p>国内源(阿里)</p>

<pre><code class="language-text">cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF
</code></pre>

<p>安装</p>

<pre><code class="language-text">apt-get update
apt-get install -y kubelet=1.16.2-00 kubeadm=1.16.2-00 kubectl=1.16.2-00
apt-mark hold kubelet kubeadm kubectl
    
</code></pre>

<h2 id="toc_4">安装Master节点</h2>

<pre><code class="language-text">kubeadm init --control-plane-endpoint &quot;k8s-api.youxuetong.com:6443&quot; --image-repository registry.aliyuncs.com/google_containers --pod-network-cidr=10.244.0.0/16 --upload-certs
</code></pre>

<p>如果安装完成，最后后输出如下内容</p>

<pre><code class="language-text">Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of control-plane nodes by copying certificate authorities
and service account keys on each node and then running the following as root:

  kubeadm join k8s-master:6443 --token vxszss.bboqeevhypvt0sxl \
    --discovery-token-ca-cert-hash sha256:56205646be3a53103e175d544dcd27cc82317c93042763cab20745334d8cb782 \
    --control-plane

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join k8s-master:6443 --token vxszss.bboqeevhypvt0sxl \
    --discovery-token-ca-cert-hash sha256:56205646be3a53103e175d544dcd27cc82317c93042763cab20745334d8cb782
</code></pre>

<h2 id="toc_5">部署CNI网络</h2>

<pre><code class="language-text">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml
</code></pre>

<h2 id="toc_6">接入其他Master节点</h2>

<p>在其他master节点机器上执行</p>

<pre><code class="language-text">  kubeadm join k8s-master:6443 --token vxszss.bboqeevhypvt0sxl \
    --discovery-token-ca-cert-hash sha256:56205646be3a53103e175d544dcd27cc82317c93042763cab20745334d8cb782 \
    --control-plane
</code></pre>

<p>如果提示token失效，请利用下面的命令去第一个master节点重新生成token</p>

<pre><code class="language-text">kubeadm token create --print-join-command
</code></pre>

<h2 id="toc_7">接入其他Node节点</h2>

<pre><code class="language-text">kubeadm join k8s-master:6443 --token vxszss.bboqeevhypvt0sxl \
    --discovery-token-ca-cert-hash sha256:56205646be3a53103e175d544dcd27cc82317c93042763cab20745334d8cb782
</code></pre>

<h2 id="toc_8">安装完成</h2>

<p>查看各节点状态</p>

<pre><code class="language-text">ubuntu@ucloud-bj-k8s-master-01:~$ kubectl get nodes -o wide
NAME                      STATUS   ROLES    AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
ucloud-bj-k8s-master-01   Ready    master   26d   v1.16.2   10.9.142.180   &lt;none&gt;        Ubuntu 16.04.6 LTS   4.4.0-142-generic   docker://18.6.0
ucloud-bj-k8s-master-02   Ready    master   26d   v1.16.2   10.9.175.27    &lt;none&gt;        Ubuntu 16.04.6 LTS   4.4.0-142-generic   docker://18.6.0
ucloud-bj-k8s-master-03   Ready    master   26d   v1.16.2   10.9.91.143    &lt;none&gt;        Ubuntu 16.04.6 LTS   4.4.0-142-generic   docker://18.6.0
ucloud-bj-k8s-node-01     Ready    &lt;none&gt;   26d   v1.16.2   10.9.165.222   &lt;none&gt;        Ubuntu 16.04.6 LTS   4.4.0-142-generic   docker://18.6.0
ucloud-bj-k8s-node-02     Ready    &lt;none&gt;   26d   v1.16.2   10.9.127.58    &lt;none&gt;        Ubuntu 16.04.6 LTS   4.4.0-142-generic   docker://18.6.0
ucloud-bj-k8s-node-03     Ready    &lt;none&gt;   26d   v1.16.2   10.9.57.4      &lt;none&gt;        Ubuntu 16.04.6 LTS   4.4.0-142-generic   docker://18.6.0
ucloud-bj-k8s-node-04     Ready    &lt;none&gt;   26d   v1.16.2   10.9.174.192   &lt;none&gt;        Ubuntu 16.04.6 LTS   4.4.0-142-generic   docker://18.6.0
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[k8s-1.16单主节点集群部署]]></title>
    <link href="https://blog.xgtian.com/15786388799784.html"/>
    <updated>2020-01-10T14:47:59+08:00</updated>
    <id>https://blog.xgtian.com/15786388799784.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">机器要求</h2>

<p>单主集群一般用于开发环境，至少需要大于2台机器，一台作为主节点，其余的作为工作节点，机器间可以相互访问即可。</p>

<table>
<thead>
<tr>
<th>Hostname</th>
<th>Ip</th>
<th>Role</th>
</tr>
</thead>

<tbody>
<tr>
<td>k8s-master</td>
<td>192.168.122.2</td>
<td>Master Node</td>
</tr>
<tr>
<td>k8s-node-01</td>
<td>192.168.122.101</td>
<td>Worker Node</td>
</tr>
<tr>
<td>k8s-node-02</td>
<td>192.168.122.102</td>
<td>Worker Node</td>
</tr>
<tr>
<td>K8s-node-03</td>
<td>192.168.122.103</td>
<td>Worker Node</td>
</tr>
</tbody>
</table>

<p>需要注意:</p>

<blockquote>
<p>如果你使用的是云服务器，服务器只有一块网卡，外网IP通过弹性IP绑定，请不要使用公网IP，使用内网网卡绑定的IP来通信。原因是，k8s只会监听网卡绑定IP的端口。</p>
</blockquote>

<h2 id="toc_1">安装准备</h2>

<ol>
<li><p>禁用Swap<br/>
k8s为了使容器的调度更符合机器的实际资源情况，k8s建议关闭内存交换   </p>
<pre><code class="language-text">swapoff -a
</code></pre>
<p>同时删除<code>/etc/fstab</code>中swap那条记录</p>
<p>当然，如果你的机器资源确实不多，需要利用swap，那么你可以不关闭swap交换空间，通过如下参数告诉k8s开启swap</p>
<pre><code class="language-text">kubelet --fail-swap-on=false ...
</code></pre></li>
<li><p>关闭防火墙(开发环境)</p>
<pre><code class="language-text">systemctl stop firewalld
systemctl disable firewalld
</code></pre></li>
<li><p>禁用SELinux(开发环境)</p>
<pre><code class="language-text">sudo apt install selinux-utils
setenforce 0
</code></pre></li>
<li><p>确认mac地址以及product uuid唯一<br/>
如果你的服务器是通过虚拟机克隆过来的，请确保这两项唯一</p>
<pre><code class="language-text">ip link #检查mac地址
sudo cat /sys/class/dmi/id/product_uuid ##检查uuid
</code></pre></li>
</ol>

<h2 id="toc_2">Docker安装(可选)</h2>

<p>由于k8s需要安装指定docker18.06版本，所以如果你的版本不对，可以先卸载重新安装</p>

<pre><code class="language-text">sudo apt-get remove docker docker-engine docker-ce docker.io
</code></pre>

<p>安装docker</p>

<pre><code class="language-text"># 从 Ubuntu 的存储库安装 Docker：
apt-get update
apt-get install -y docker.io

# 或者从 Docker 的 Ubuntu 或 Debian 镜像仓库中安装 Docker CE 18.06：

## 安装环境准备。
apt-get update &amp;&amp; apt-get install apt-transport-https ca-certificates curl software-properties-common

## 下载 GPG 密钥。
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -

## 添加 docker apt 镜像仓库。
add-apt-repository \
   &quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu \
   $(lsb_release -cs) \
   stable&quot;

## 安装 docker。
apt-get update &amp;&amp; apt-get install docker-ce=18.06.0~ce~3-0~ubuntu

# 设置守护进程。
cat &gt; /etc/docker/daemon.json &lt;&lt;EOF
{
  &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;],
  &quot;log-driver&quot;: &quot;json-file&quot;,
  &quot;log-opts&quot;: {
    &quot;max-size&quot;: &quot;100m&quot;
  },
  &quot;storage-driver&quot;: &quot;overlay2&quot;
}
EOF

mkdir -p /etc/systemd/system/docker.service.d

# 重启 docker。
systemctl daemon-reload
systemctl restart docker
</code></pre>

<h2 id="toc_3">开始安装 （切换到root身份）</h2>

<ol>
<li><p>安装相关工具</p>
<pre><code class="language-text">apt-get update &amp;&amp; apt-get install -y apt-transport-https curl
</code></pre></li>
<li><p>添加k8s软件源</p>
<p>添加阿里源密钥</p>
<pre><code class="language-text">curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - 
</code></pre>
<p>添加k8s阿里源</p>
<pre><code class="language-text">cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main<br/>
EOF
</code></pre></li>
<li><p>安装 <code>kubelet</code>,<code>kubeadm</code>,<code>kubectl</code></p>
<pre><code class="language-text">apt-get update
apt-get install -y kubelet kubeadm kubectl<br/>
apt-mark hold kubelet kubeadm kubectl
</code></pre>
<p>非root用户需要加上sudo</p></li>
</ol>

<h2 id="toc_4">部署master</h2>

<h3 id="toc_5">初始化master节点</h3>

<pre><code class="language-text">kubeadm init --apiserver-advertise-address=192.168.122.2 --image-repository registry.aliyuncs.com/google_containers --pod-network-cidr=10.244.0.0/16
</code></pre>

<p>参数解释：</p>

<ul>
<li><code>--apiserver-advertise-address</code>: k8s 中的主要服务apiserver的部署地址，填自己的管理节点 ip</li>
<li><code>--image-repository</code>: 拉取的 docker 镜像源，因为初始化的时候kubeadm会去拉 k8s 的很多组件来进行部署，所以需要指定国内镜像源，下不然会拉取不到镜像。</li>
<li><code>--pod-network-cidr</code>: 这个是 k8s 采用的节点网络，因为我们将要使用flannel作为 k8s 的网络，所以这里填10.244.0.0/16就好</li>
<li><code>--kubernetes-version</code>: 这个是用来指定你要部署的 k8s 版本的，一般不用填，不过如果初始化过程中出现了因为版本不对导致的安装错误的话，可以用这个参数手动指定。</li>
<li><code>--gnore-preflight-errors</code>: 忽略初始化时遇到的错误，比如说我想忽略 cpu 数量不够 2 核引起的错误，就可以用--ignore-preflight-errors=CpuNum。错误名称在初始化错误时会给出来。</li>
</ul>

<p>执行上面的命令，等待安装完成，知道出现下面的文字表示安装成功</p>

<pre><code class="language-text">Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

You should now deploy a pod network to the cluster.
Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 192.168.122.2:6443 --token 1gyku3.72jgt8prrp2fdhhx \
    --discovery-token-ca-cert-hash sha256:9c5bf462c308be3a69b6766e71f367e48e222c4ee019c4ec438e02e089ab4e12
</code></pre>

<p>复制保存<code>kubeadm join</code>那行的文字，后面添加节点需要使用到，注意，token默认是24小时会过期，可以在master上利用如下命令查看token是否有效</p>

<pre><code class="language-text">kubeadm token list
</code></pre>

<p>如果没有有效token，可以重新创建一个</p>

<pre><code class="language-text">kubeadm token create
</code></pre>

<h3 id="toc_6">初始化kubectl工具</h3>

<p>按照kubeadm的指示操作就行，依次执行如下命令，用普通用户的账号</p>

<pre><code class="language-text">mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
</code></pre>

<p>如果你想在root用户下使用</p>

<pre><code class="language-text">export KUBECONFIG=/etc/kubernetes/admin.conf
</code></pre>

<h3 id="toc_7">部署flannel (Pod Network Adds-on)</h3>

<p>k8s依赖于第三的节点网络，以便各pod节点之间可以相互通信</p>

<pre><code class="language-text">kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml

</code></pre>

<p><code>如果遇到timeout错误，请使用代理</code></p>

<p>输出以下内容代表安装完成</p>

<pre><code class="language-text">clusterrole.rbac.authorization.k8s.io/flannel created
clusterrolebinding.rbac.authorization.k8s.io/flannel created
serviceaccount/flannel created
configmap/kube-flannel-cfg created
daemonset.extensions/kube-flannel-ds-amd64 created
daemonset.extensions/kube-flannel-ds-arm64 created
daemonset.extensions/kube-flannel-ds-arm created
daemonset.extensions/kube-flannel-ds-ppc64le created
daemonset.extensions/kube-flannel-ds-s390x created

</code></pre>

<h3 id="toc_8">添加node节点</h3>

<p>按照最前面的部署安装好k8s相关程序</p>

<pre><code class="language-text">kubeadm join 192.168.122.2:6443 --token 1gyku3.72jgt8prrp2fdhhx \
    --discovery-token-ca-cert-hash sha256:9c5bf462c308be3a69b6766e71f367e48e222c4ee019c4ec438e02e089ab4e12
</code></pre>

<blockquote>
<p>如果join命令忘记了复制，可以去master节点上执行</p>
</blockquote>

<pre><code class="language-text">kubeadm token create --print-join-command
</code></pre>

<h3 id="toc_9">详细安装日志</h3>

<p>可以通过在命令后面增加<code>--v=5</code>来查看详细安装日志，例如</p>

<pre><code class="language-text">kubeadm join 192.168.122.2:6443 --token 1gyku3.72jgt8prrp2fdhhx \
    --discovery-token-ca-cert-hash sha256:9c5bf462c308be3a69b6766e71f367e48e222c4ee019c4ec438e02e089ab4e12 --v=5
</code></pre>

<h3 id="toc_10">安装完成</h3>

<p>主节点执行<code>kubectl get nodes -o wide</code>，检查各节点是否为<code>Ready状态</code></p>

<pre><code class="language-text">ubuntu@k8s-master:~$ kubectl get nodes -o wide
NAME          STATUS   ROLES    AGE   VERSION   INTERNAL-IP       EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
k8s-master    Ready    master   23d   v1.16.2   192.168.122.2     &lt;none&gt;        Ubuntu 16.04.6 LTS   4.4.0-142-generic   docker://18.6.0
k8s-node-01   Ready    &lt;none&gt;   23d   v1.16.2   192.168.122.101   &lt;none&gt;        Ubuntu 16.04.6 LTS   4.4.0-142-generic   docker://18.6.0
k8s-node-02   Ready    &lt;none&gt;   23d   v1.16.2   192.168.122.102   &lt;none&gt;        Ubuntu 16.04.6 LTS   4.4.0-142-generic   docker://18.6.0
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[K8S日志系统EFK]]></title>
    <link href="https://blog.xgtian.com/15774160287586.html"/>
    <updated>2019-12-27T11:07:08+08:00</updated>
    <id>https://blog.xgtian.com/15774160287586.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">elasticsearch</h2>

<pre><code class="language-text">helm install elasticsearch stable/elasticsearch -n share
</code></pre>

<h2 id="toc_1">fluentd</h2>

<pre><code class="language-text">helm repo add kiwigrid https://kiwigrid.github.io
helm repo update
helm install fluentd  kiwigrid/fluentd-elasticsearch --set elasticsearch.bufferChunkLimit=&quot;20M&quot;,elasticsearch.bufferQueueLimit=20 -n share
</code></pre>

<p><code>bufferChunkLimit</code> 一次发送数据的最大限制<br/>
<code>bufferQueueLimit</code> 待发送数据的最大数量限制</p>

<p>bufferChunkLimit * bufferQueueLimit 应小于PC内存</p>

<h2 id="toc_2">kibana</h2>

<pre><code class="language-text">helm install kibana stable/kibana --set ingress.enabled=true,ingress.hosts[0]=kibana.dev.youxuetong.com -n share
</code></pre>

<p>需要手动修改kibana的configmap，修改其中的elasticsearch.hosts</p>

<pre><code class="language-text">data:
  kibana.yml: |
    elasticsearch.hosts: http://elasticsearch-client:9200
    server.host: &quot;0&quot;
    server.name: kibana
</code></pre>

<p>删除正在运行的kibana的pod</p>

<pre><code class="language-text">kubectl delete pod kibana-557d4dc6b9-7dx5j -n share
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[WebDashboard UI部署]]></title>
    <link href="https://blog.xgtian.com/15773284865375.html"/>
    <updated>2019-12-26T10:48:06+08:00</updated>
    <id>https://blog.xgtian.com/15773284865375.html</id>
    <content type="html"><![CDATA[
<p>下载官方文件</p>

<pre><code class="language-text">wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta4/aio/deploy/recommended.yaml
</code></pre>

<p>修改文件,去除自建的secret</p>

<pre><code class="language-text">#---
## 由于证书问题，只能firefox浏览器才能打开，通过修改证书的方式，使得所有浏览器都能打开
#apiVersion: v1
#kind: Secret
#metadata:
#  labels:
#    k8s-app: kubernetes-dashboard
#  name: kubernetes-dashboard-certs  #生成证书会用到该名字
#  namespace: kubernetes-dashboard  #生成证书使用该命名空间
#type: Opaque
</code></pre>

<p>修改service为NodePort方式</p>

<pre><code class="language-text">kind: Service
apiVersion: v1
metadata:
  labels:
    k8s-app: kubernetes-dashboard
  name: kubernetes-dashboard
  namespace: kubernetes-dashboard
spec:
  type: NodePort
  ports:
    - port: 443
      targetPort: 8443
      nodePort: 32000
  selector:
    k8s-app: kubernetes-dashboard
</code></pre>

<p>创建自签名证书</p>

<pre><code class="language-text"># 创建目录使用证书
mkdir key &amp;&amp; cd key
# 查看是否存在namespace为kubernetes-dashboard
kubectl get namespaces
# 不存在namespace为创建kubernetes-dashboard创建namespace
kubectl create namespace kubernetes-dashboard
# 生成 key
openssl genrsa -out dashboard.key 2048
# 生成证书请求
openssl req -days 36000   -new -out dashboard.csr    -key dashboard.key   -subj &#39;/CN=**192.168.100.10**&#39;
# 生成自签证书
openssl x509 -req -in dashboard.csr -signkey dashboard.key -out dashboard.crt
# 目录结构
[root@k8smaster key]# ll
total 12
-rw-r--r-- 1 root root 1001 Oct 23 22:21 dashboard.crt
-rw-r--r-- 1 root root  903 Oct 23 22:20 dashboard.csr
-rw-r--r-- 1 root root 1679 Oct 23 22:20 dashboard.key
# 使用自签证书创建secret
kubectl create secret generic kubernetes-dashboard-certs     --from-file=dashboard.key     --from-file=dashboard.crt      -n kubernetes-dashboard

</code></pre>

<p>应用配置文件</p>

<pre><code class="language-text">kubectl create -f recommend.yaml
</code></pre>

<p>添加管理员并绑定管理员权限</p>

<pre><code class="language-text"># 创建sa
kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard
# 绑定集群管理员
kubectl create clusterrolebinding  dashboard-cluster-admin --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin
</code></pre>

<p>获取登录token</p>

<pre><code class="language-text">kubectl describe secrets  $(kubectl  get secrets -n kubernetes-dashboard | awk  &#39;/dashboard-admin-token/{print $1}&#39; ) -n kubernetes-dashboard |sed -n &#39;/token:.*/p&#39;
</code></pre>

<p>浏览器访问<a href="https://IP:32000">https://IP:32000</a><br/>
如果Chrome浏览器仍然显示非安全连接，且详细信息中没有继续前往按钮，请用safari打开该连接，点击详细信息中的继续前往，按照系统只是操作，最后发现该自签名证书会被添加到钥匙串中，之后在用chrome浏览器打卡就不会出现无法访问的情况了。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[ubuntu安装NFS-Server提供NFS服务]]></title>
    <link href="https://blog.xgtian.com/15773244836718.html"/>
    <updated>2019-12-26T09:41:23+08:00</updated>
    <id>https://blog.xgtian.com/15773244836718.html</id>
    <content type="html"><![CDATA[
<p>如果没有现成的NFS服务,可以利用现有服务器部署一套</p>

<p>安装NFS服务端</p>

<pre><code class="language-text">    sudo apt-get install nfs-kernel-server
</code></pre>

<p>编辑/etc/exports文件,增加你要提供nfs服务的路径</p>

<pre><code class="language-text"># /etc/exports: the access control list for filesystems which may be exported
#       to NFS clients.  See exports(5).
#
# Example for NFSv2 and NFSv3:
# /srv/homes       hostname1(rw,sync,no_subtree_check) hostname2(ro,sync,no_subtree_check)
#
# Example for NFSv4:
# /srv/nfs4        gss/krb5i(rw,sync,fsid=0,crossmnt,no_subtree_check)
# /srv/nfs4/homes  gss/krb5i(rw,sync,no_subtree_check)
#

/data/charts *(rw,sync,no_subtree_check,no_root_squash)
/data/mysql *(rw,sync,no_subtree_check,no_root_squash)
/data/redis *(rw,sync,no_subtree_check,no_root_squash)
</code></pre>

<p>更改共享目录权限，修改为777 (目前还不清楚最低要求权限是多少)</p>

<pre><code class="language-text">sudo chown 777  /data/charts
</code></pre>

<p>重启NFS服务</p>

<pre><code class="language-text">sudo /etc/init.d/nfs-kernel-server restart
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[HAProxy]]></title>
    <link href="https://blog.xgtian.com/15761310581680.html"/>
    <updated>2019-12-12T14:10:58+08:00</updated>
    <id>https://blog.xgtian.com/15761310581680.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0"> 验证配置文件</h2>

<pre><code class="language-text">    haproxy -c -f /etc/haproxy/haproxy.cfg
</code></pre>

<h2 id="toc_1">负载均衡配置</h2>

<pre><code class="language-text">frontend k8s-api

  bind 0.0.0.0:6443

  log global

  mode tcp

  default_backend k8s-api-server

backend k8s-api-server

  mode tcp

  log global

  balance roundrobin

  server ucloud-bj-k8s-master-01 10.9.142.180:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3

  server ucloud-bj-k8s-master-02 10.9.175.27:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3

  server ucloud-bj-k8s-master-03 10.9.91.143:6443 weight 1 maxconn 1000 check inter 2000 rise 2 fall 3
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kubernets 卸载清理]]></title>
    <link href="https://blog.xgtian.com/15761212808457.html"/>
    <updated>2019-12-12T11:28:00+08:00</updated>
    <id>https://blog.xgtian.com/15761212808457.html</id>
    <content type="html"><![CDATA[
<pre><code class="language-text">kubeadm reset -f
modprobe -r ipip
lsmod
rm -rf ~/.kube/
rm -rf /etc/kubernetes/
rm -rf /etc/systemd/system/kubelet.service.d
rm -rf /etc/systemd/system/kubelet.service
rm -rf /usr/bin/kube*
rm -rf /etc/cni
rm -rf /opt/cni
rm -rf /var/lib/etcd
rm -rf /var/etcd


</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[k8s实用指令]]></title>
    <link href="https://blog.xgtian.com/15750110628081.html"/>
    <updated>2019-11-29T15:04:22+08:00</updated>
    <id>https://blog.xgtian.com/15750110628081.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">创建Docker私有仓库密钥</h2>

<pre><code class="language-text">kubectl create secret docker-registry regsecret --docker-server=registry.cn-hangzhou.aliyuncs.com --docker-username=yin32167@aliyun.com --docker-password=xxxxxx --docker-email=yin32167@aliyun.com
</code></pre>

<h2 id="toc_1">k8s简单端口转发</h2>

<p>转发至pod</p>

<pre><code class="language-text">kubectl port-forward --address 0.0.0.0 pod/mypod 8888:5000
</code></pre>

<p>转发至service</p>

<pre><code class="language-text">kubectl port-forward --address 0.0.0.0 services/myservice 8888:5000
</code></pre>

<h2 id="toc_2">移除node</h2>

<p>移除node需要先将node上的pod转移</p>

<pre><code class="language-text">kubectl drain k8s-node-storage --delete-local-data --force --ignore-daemonsets
</code></pre>

<p>然后利用delete命令删除node</p>

<pre><code class="language-text">kubectl delete node node-01
</code></pre>

<h2 id="toc_3">k8s创建tls secret</h2>

<pre><code class="language-text">kubectl create secret tls dev.youxuetong.com --cert=server.crt --key=server.key -n share
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[helm基础介绍]]></title>
    <link href="https://blog.xgtian.com/15743903332872.html"/>
    <updated>2019-11-22T10:38:53+08:00</updated>
    <id>https://blog.xgtian.com/15743903332872.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">helm安装</h2>

<h3 id="toc_1">3.0版本</h3>

<pre><code class="language-text">curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
</code></pre>

<h2 id="toc_2">helm仓库</h2>

<pre><code class="language-text">http://mirror.azure.cn/kubernetes/charts/
</code></pre>

<p>使用方法</p>

<pre><code class="language-text">helm repo add azure http://mirror.azure.cn/kubernetes/charts/
</code></pre>

<p>阿里云</p>

<pre><code class="language-text">helm repo add aliyun https://apphub.aliyuncs.com/
</code></pre>

<h2 id="toc_3">搜索公共仓库</h2>

<p><a href="https://hub.helm.sh/">仓库地址</a></p>

<pre><code class="language-text">helm search hub wordpress
</code></pre>

<h2 id="toc_4">搜索私有仓库</h2>

<pre><code class="language-text">helm search repo wordpress
</code></pre>

<h2 id="toc_5">Helm 如何管理多环境下 (Test、Staging、Production) 的业务配置？</h2>

<p>Chart 是支持参数替换的，可以把业务配置相关的参数设置为模板变量。使用 helm install 命令部署的时候指定一个参数值文件，这样就可以把业务参数从 Chart 中剥离了。例如： helm install --values=values-production.yaml wordpress。</p>

<h2 id="toc_6">helm push 插件</h2>

<pre><code class="language-text">helm plugin install https://github.com/chartmuseum/helm-push
</code></pre>

<p>上传包</p>

<pre><code class="language-text">helm push . http://192.168.1.24:8080
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux 测试磁盘性能]]></title>
    <link href="https://blog.xgtian.com/15595559159421.html"/>
    <updated>2019-06-03T17:58:35+08:00</updated>
    <id>https://blog.xgtian.com/15595559159421.html</id>
    <content type="html"><![CDATA[
<p>测试场景：</p>

<p>100%随机，100%读， 4K</p>

<pre><code class="language-text">fio -filename=/dev/emcpowerb -direct=1 -iodepth 1 -thread -rw=randread -ioengine=psync -bs=4k -size=1000G -numjobs=50 -runtime=180 -group_reporting -name=rand_100read_4k
</code></pre>

<p>100%随机，100%写， 4K</p>

<pre><code class="language-text">fio -filename=/dev/emcpowerb -direct=1 -iodepth 1 -thread -rw=randwrite -ioengine=psync -bs=4k -size=1000G -numjobs=50 -runtime=180 -group_reporting -name=rand_100write_4k
</code></pre>

<p>100%顺序，100%读 ，4K</p>

<pre><code class="language-text">fio -filename=/dev/emcpowerb -direct=1 -iodepth 1 -thread -rw=read -ioengine=psync -bs=4k -size=1000G -numjobs=50 -runtime=180 -group_reporting -name=sqe_100read_4k
</code></pre>

<p>100%顺序，100%写 ，4K</p>

<pre><code class="language-text">fio -filename=/dev/emcpowerb -direct=1 -iodepth 1 -thread -rw=write -ioengine=psync -bs=4k -size=1000G -numjobs=50 -runtime=180 -group_reporting -name=sqe_100write_4k
</code></pre>

<p>100%随机，70%读，30%写 4K</p>

<pre><code class="language-text">fio -filename=/dev/emcpowerb -direct=1 -iodepth 1 -thread -rw=randrw -rwmixread=70 -ioengine=psync -bs=4k -size=1000G -numjobs=50 -runtime=180 -group_reporting -name=randrw_70read_4k
</code></pre>

<p>io=执行了多少M的IO</p>

<p>bw=平均IO带宽<br/>
iops=IOPS<br/>
runt=线程运行时间<br/>
slat=提交延迟<br/>
clat=完成延迟<br/>
lat=响应时间<br/>
bw=带宽<br/>
cpu=利用率<br/>
IO depths=io队列<br/>
IO submit=单个IO提交要提交的IO数<br/>
IO complete=Like the above submit number, but for completions instead.<br/>
IO issued=The number of read/write requests issued, and how many of them were short.<br/>
IO latencies=IO完延迟的分布</p>

<p>io=总共执行了多少size的IO<br/>
aggrb=group总带宽<br/>
minb=最小.平均带宽.<br/>
maxb=最大平均带宽.<br/>
mint=group中线程的最短运行时间.<br/>
maxt=group中线程的最长运行时间.</p>

<p>ios=所有group总共执行的IO数.<br/>
merge=总共发生的IO合并数.<br/>
ticks=Number of ticks we kept the disk busy.<br/>
io_queue=花费在队列上的总共时间.<br/>
util=磁盘利用率</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Confluence安装]]></title>
    <link href="https://blog.xgtian.com/15535769948107.html"/>
    <updated>2019-03-26T13:09:54+08:00</updated>
    <id>https://blog.xgtian.com/15535769948107.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>启动镜像</p>
<pre><code class="language-text">docker run -d --restart always -v /home/ubuntu/confluence:/var/atlassian/application-data/confluence --name=&quot;confluence&quot; -d -p 80:8090 -p 8091:8091 atlassian/confluence-server:6.12
</code></pre></li>
</ol>

<h2 id="toc_0">下载</h2>

<p>官网下载confluence的tar包(v6.12.2)</p>

<h2 id="toc_1">安装</h2>

<p>解压缩（这里解压到/server/confluence目录），打开该目录下confluence/WEB-INF/classes/confluence-init.properties文件，添加行：</p>

<pre><code class="language-text">confluence.home=/server/confluence
</code></pre>

<h2 id="toc_2">启动</h2>

<blockquote>
<p>\( cd /server/confluence<br/>
\) ./bin/start-confluence.sh </p>
</blockquote>

<h2 id="toc_3">浏览器访问</h2>

<p>输入 <a href="http://localhost:8090/">http://localhost:8090/</a> 进行安装，期间会要求输入license，导向到官网申请之；进入数据库步骤，如果选择mysql，要依次逆行：</p>

<ol>
<li>下载mysql驱动放到WEB-INF/lib目录，重新启动confluence服务器</li>
<li><p>创建数据库时，编码COLLATE 应选：utf8_bin，如：</p>
<blockquote>
<p>CREATE DATABASE IF NOT EXISTS confluence DEFAULT CHARSET utf8 COLLATE utf8_bin; <br/>
如果没有创建数据用户，可以创建：<br/>
GRANT ALL ON <em>.</em> TO banyuan@localhost IDENTIFIED BY &quot;Banyuan2019&quot;; </p>
</blockquote></li>
<li><p>事务默认级别应设置为：READ-COMMITTED。具体操作过程可能如下：</p>
<ul>
<li>a.查看mysql的conf文件所在位置：<br/>
&gt; $ mysqld --verbose --help|grep -A 1 &#39;Default options&#39;<br/>
可能返回结果：/etc/my.cnf /etc/mysql/my.cnf /usr/local/etc/my.cnf ~/.my.cnf </li>
<li>b. 编辑my.conf，如：/usr/local/etc/my.cnf ，在 [mysqld]节增加行：
<code>
[mysqld]<br/>
...<br/>
transaction-isolation=READ-COMMITTED
</code>
然后重启服务。可以进入mysql控制台，执行下面的命令确认是否修改过了。<br/>
&gt; show variables like &#39;transaction_isolation&#39;;</li>
</ul></li>
</ol>

<p>以上这些步骤，在实际安装过程中都是有提示的，所以不必特别担心。</p>

<p>安装完毕后，仍会检查mysql的一些设置，比如可能提示如下两个变量设置值：</p>

<blockquote>
<p>成功最大允许数据包 - max_allowed_packet：一般不小于34m<br/>
InnoDB 日志文件大小 - innodb_log_file_size：一般不小于256M</p>

<p>character-set-server = utf8<br/>
innodb_log_file_size = 256M<br/>
max_allowed_packet = 34M</p>
</blockquote>

<h2 id="toc_4">破解</h2>

<p>破解需要在安装过程中进行，安装进入到授权码页面时，开始破解：</p>

<ol>
<li>使用【附件】目录confluence-crack.zip解压缩，比如解到/cracked目录。</li>
<li>把confluence的WEB-INF/lib/atlassian-extras-decoder-v2-3.4.1.jar 文件复制到/cracked目录，重命名为atlassian-extras-2.4.jar</li>
<li>运行/cracked目录下的confluence_keygen.jar，点.patch!，依提示选择atlassian-extras-2.4.jar，就可以在/cracked目录看到atlassian-extras-2.4.jar和atlassian-extras-2.4.bak两个文件，这里atlassian-extras-2.4.jar已经是破解好的了；</li>
<li>然后在patch软件里输入安装Web页上的Server Id，点击.gen!产生key</li>
<li>将atlassian-extras-2.4.jar名字改回来：atlassian-extras-decoder-v2-3.4.1，复制回confluence的WEB-INF/lib/目录，重新启动confluence</li>
<li>复制key到Web页，进行下一步<br/>
## 邮件服务器配置<br/>
直接填写地址、协议死活不工作，不得已配置了JNDI：</li>
</ol>

<p>A. 在{confluence-install}/conf/server.xml中<Context>的末尾加代码：</p>

<pre><code class="language-text">&lt;Context path=&quot;&quot; docBase=&quot;../confluence&quot; debug=&quot;0&quot; reloadable=&quot;false&quot; useHttpOnly=&quot;true&quot;&gt;
....
&lt;Resource name=&quot;mail/BanyuanSMTPServer&quot;
    auth=&quot;Container&quot;
    type=&quot;javax.mail.Session&quot;
    mail.smtp.host=&quot;smtp.banyuan.club&quot;
    mail.smtp.port=&quot;465&quot;
    mail.smtp.auth=&quot;true&quot;
    mail.smtp.user=&quot;admin@banyuan.club&quot;
    password=&quot;SemiCircle2019&quot;
    mail.smtp.starttls.enable=&quot;true&quot;
    mail.transport.protocol=&quot;smtps&quot;
    mail.smtp.socketFactory.class=&quot;javax.net.ssl.SSLSocketFactory&quot;
/&gt;
&lt;/Context&gt;
</code></pre>

<p>B. 然后移动{confluence-install}/confluence/WEB-INF/lib/mail-x.x.x.jar到{confluence-install}/lib目录（移动不是拷贝）<br/>
C. 重启服务器，<br/>
D. 进入后台邮件服务器配置JNDI服务名为：java:comp/env/mail/BanyuanSMTPServer，注意：SMTP单独配置选项都要为空（可以只能配置用户名，同mail.smtp.user），然后发送测试邮件，通过。</p>

<h2 id="toc_5">RestAPI调用</h2>

<p>官方文档： <a href="https://developer.atlassian.com/server/confluence/confluence-server-rest-api/">https://developer.atlassian.com/server/confluence/confluence-server-rest-api/</a></p>

<p>例：<br/>
获取文章内容：<a href="http://localhost:8090/rest/api/content/327696?expand=body.storage">http://localhost:8090/rest/api/content/327696?expand=body.storage</a><br/>
获取文章里宏：<a href="http://localhost:8090/rest/api/content/327696/history/2/macro/id/0efe0e6f-e80e-4528-81d8-968f47a87907">http://localhost:8090/rest/api/content/327696/history/2/macro/id/0efe0e6f-e80e-4528-81d8-968f47a87907</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Mac 安装MiniKube]]></title>
    <link href="https://blog.xgtian.com/15526315084700.html"/>
    <updated>2019-03-15T14:31:48+08:00</updated>
    <id>https://blog.xgtian.com/15526315084700.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>安装kubectl</p>
<pre><code class="language-text">brew install kubernetes-cli
</code></pre></li>
<li><p>安装minikube</p>
<p>官方原版，需要科学上网</p>
<pre><code class="language-text">brew cask install minikube
</code></pre>
<p>阿里云的minikube</p>
<pre><code class="language-text">curl -Lo minikube http://kubernetes.oss-cn-hangzhou.aliyuncs.com/minikube/releases/v0.30.0/minikube-darwin-amd64 &amp;&amp; chmod +xminikube &amp;&amp; sudo mv minikube /usr/local/bin/
</code></pre></li>
<li><p>minikube 启动命令</p>
<pre><code class="language-text">minikube start --docker-env HTTP_PROXY=192.168.2.114:31210 --docker-env HTTPS_PROXY=192.168.2.114:31210
</code></pre></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python 单元测试以及单元测试覆盖率]]></title>
    <link href="https://blog.xgtian.com/15451201223625.html"/>
    <updated>2018-12-18T16:02:02+08:00</updated>
    <id>https://blog.xgtian.com/15451201223625.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">单元测试</h2>

<ol>
<li><p>安装nosetest</p>
<pre><code class="language-text">pip install nose
</code></pre></li>
<li><p>执行测试并输出xunit格式xml文件</p>
<pre><code class="language-text">nosetest --with-xunit -v
</code></pre>
<p>如果你想指定输出的文件名</p>
<pre><code class="language-text">nosetest --with-xunit --xunit-file=xunittest.xml -v
</code></pre></li>
</ol>

<h2 id="toc_1">测试覆盖率</h2>

<ol>
<li><p>安装coverage</p>
<pre><code class="language-text">pip install coverage
</code></pre></li>
<li><p>测试指定的测试代码或者模块</p>
<pre><code class="language-text">coverage run test.py
</code></pre>
<p>以上操作会在当前目录下生成<code>.coverage</code>目录</p></li>
<li><p>生成xml报告或者html报告</p>
<pre><code class="language-text">coverage html
coverage xml
</code></pre></li>
</ol>

<h2 id="toc_2">在Django 中使用nose和coverage</h2>

<p>如果仅仅是使用coverage</p>

<pre><code class="language-text">coverage run manage.py test #执行django的单元测试
coverage xml #生成xml报告
</code></pre>

<p>如果需要同时使用nosetest和coverage</p>

<ol>
<li><p>下载安装django-nose</p>
<pre><code class="language-text">pip install django-nose
</code></pre></li>
<li><p>将django-nose添加到项目的setting.py文件当中</p>
<pre><code class="language-text">INSTALLED_APPS = (
...<br/>
&#39;django_nose&#39;, # Append to INSTALLED_APPS<br/>
...<br/>
)<br/>
TEST_RUNNER = &#39;django_nose.NoseTestSuiteRunner&#39;<br/>
NOSE_ARGS = [<br/>
   &#39;--with-coverage&#39;,<br/>
    &#39;--with-xunit&#39;,<br/>
    &#39;--xunit-file=xunittest.xml&#39;<br/>
]
</code></pre></li>
<li><p>执行单元测试</p>
<pre><code class="language-text">python manage.py test
</code></pre></li>
<li><p>生成单元测试覆盖率</p>
<pre><code class="language-text">coverage xml
</code></pre></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Redmine Docker部署]]></title>
    <link href="https://blog.xgtian.com/15446089404193.html"/>
    <updated>2018-12-12T18:02:20+08:00</updated>
    <id>https://blog.xgtian.com/15446089404193.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>拉取镜像</p>
<pre><code class="language-text">docker pull redmine
</code></pre></li>
<li><p>启动docker镜像</p>
<pre><code class="language-text">docker run -d --name redmine \
-e REDMINE_DB_MYSQL=192.168.2.191 \<br/>
-e REDMINE_DB_USERNAME=root \<br/>
-e REDMINE_DB_PASSWORD=www.1234TV.com \<br/>
-e REDMINE_DB_DATABASE=redmine \<br/>
-p 80:3000 redmine   
</code></pre></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GitLab和Redmine深度集成]]></title>
    <link href="https://blog.xgtian.com/15445994167623.html"/>
    <updated>2018-12-12T15:23:36+08:00</updated>
    <id>https://blog.xgtian.com/15445994167623.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">Redmine Issue Tracker</h2>

<ol>
<li><p>设置Redmine Issue Tracker<br/>
在GitLab的项目setting里面找到integrations，找到redmine<br/>
修改如下:<br/>
<img src="https://pic.mylonly.com/2018-12-12-072609.png" alt=""/></p></li>
<li><p>关闭内置issue</p>
<p>setting-&gt;general-&gt;Permissions</p>
<p><img src="https://pic.mylonly.com/2018-12-12-072807.png" alt=""/></p></li>
</ol>

<h2 id="toc_1">Redmine 启用版本库</h2>

<ol>
<li><p>进入redmine容器</p>
<pre><code class="language-text">docker exec -it redmine /bin/bash
</code></pre></li>
<li><p>安装插件<code>redmine_gitlab_hook</code></p>
<p>进入<code>/usr/src/redmine/plugins</code>目录</p>
<pre><code class="language-text">git clone https://github.com/phlegx/redmine_gitlab_hook.git
</code></pre>
<p>登录redmine管理员，管理-&gt;插件，找到<code>Redmine GitLab Hook plugin</code>,进入配置</p>
<p><img src="https://pic.mylonly.com/2018-12-12-101517.png" alt=""/></p></li>
<li><p>创建本地git仓库并拉取</p>
<pre><code class="language-text">mkdir -p /home/redmine/git-repo #创建本地git仓库
cd /home/redmine/git-repo/<br/>
git clone --mirror http://username:password@gitlab.1234tv.lan:/awesome/python-test.git #此处最好采用http加上用户名和密码的方式拉取git仓库<br/>
chmod -R redmine:redmine python-test.git ##记得修改用户组为redmine:redmine
</code></pre></li>
<li><p>登录redmine管理员，启用版本库</p>
<p><img src="https://pic.mylonly.com/2018-12-12-073502.png" alt=""/></p></li>
<li><p>配置版本库</p>
<p>进入到和gitlab项目对应的项目中，进入设置页面<br/>
git clone时加了-mirror参数,使用下面参数<br/>
<img src="https://pic.mylonly.com/2018-12-12-101134.png" alt=""/><br/>
如果git clone 时没有使用-mirror参数，使用下面的配置<br/>
<img src="https://pic.mylonly.com/2018-12-12-073658.png" alt=""/></p></li>
</ol>

<h2 id="toc_2">配置GitLab Webhook</h2>

<p>进入GitLab的项目中，在setting-&gt;Integrations中添加如下webhooks地址</p>

<p>webhook url格式:</p>

<pre><code class="language-text">{redmine_installation_url}/gitlab_hook?key={redmine_repository_API_key}&amp;project_id={redmine_project_identifier}
</code></pre>

<p><img src="https://pic.mylonly.com/2018-12-12-101954.png" alt=""/></p>

<p>如果redmine的项目名称和版本库的标识不一致，需要带上<code>repository_name</code>参数，手动指定redmine上的版本库</p>

<p>如果需要gitlab webhook插件自动创建版本库,还需要加上<code>repository_git_url</code>、<code>repository_namespace</code>、<code>repository_name</code>这几个参数,</p>

<p>其中<code>repository_git_url</code>为需要克隆的远程仓库的地址<br/>
而<code>repository_namespace</code>和<code>repository_name</code>自由填写，会在redmine的版本库里生成类似 <code>{repository_namespace}_{repository_name}</code>样式的版本库标识</p>

<h2 id="toc_3">验证</h2>

<ol>
<li>在redmine里创建一个issue，状态为新建</li>
<li>本地clone项目，少许修改，commit日志填写&quot;bugfix #<id>&quot; (<code>id为redmine上的issue id</code>),然后push 到服务器上</li>
<li>查看redmine上该issue的状态是否改变</li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GitLabRunner配合SonarScanner针对每次commit做代码检查]]></title>
    <link href="https://blog.xgtian.com/15445172404972.html"/>
    <updated>2018-12-11T16:34:00+08:00</updated>
    <id>https://blog.xgtian.com/15445172404972.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">Sonar GitLab-Plugin插件以及配置</h2>

<ol>
<li><p>下载安装插件</p>
<p>admin登录SonarQube，点击 配置 —&gt; 系统 —&gt; 更新中心 —&gt; Available —&gt; Search，输入 GitLab，在列表中点击 install 安装，安装完毕后重启 SonarQube 即可<br/>
<img src="https://pic.mylonly.com/2018-12-11-093357.png" alt=""/></p></li>
<li><p>在GitLab上注册一个Sonarqube账号，获取Private Token</p></li>
</ol>

<p><img src="https://pic.mylonly.com/2018-12-11-093529.png" alt=""/></p>

<ol>
<li>回到SonarQube 配置插件
<img src="https://pic.mylonly.com/2018-12-11-093642.png" alt=""/><br/></li>
</ol>

<h2 id="toc_1">Sonar Scanner 相关配置</h2>

<p>依然采用docker方式</p>

<ol>
<li><p>拉取镜像</p>
<pre><code class="language-text">docker pull ciricihq/gitlab-sonar-scanner
</code></pre></li>
<li><p>在sonarqube上新建要测试的项目<br/>
按照向导创建完项目之后记住所设置的<code>key</code><br/>
<img src="https://pic.mylonly.com/2018-12-11-094008.png" alt=""/></p></li>
<li><p>回到GitLab,在需要进行质量检测的项目中加入<code>.gitlab-ci.yml</code>,在末尾添加如下代码：</p>
<pre><code class="language-text">stages:
- analysis<br/>
sonarqube:<br/>
  stage: analysis<br/>
  image: ciricihq/gitlab-sonar-scanner<br/>
  variables:<br/>
    SONAR_URL: http://sonarqube.domain.lan #你的sonarqube地址<br/>
    SONAR_ANALYSIS_MODE: issues<br/>
  script:<br/>
  - gitlab-sonar-scanner<br/>
sonarqube-reports:<br/>
  stage: analysis<br/>
  image: ciricihq/gitlab-sonar-scanner<br/>
  variables:<br/>
    SONAR_URL: http://sonarqube.domain.lan #你的sonarqube地址<br/>
    SONAR_ANALYSIS_MODE: publish<br/>
  script:<br/>
  - gitlab-sonar-scanner
</code></pre></li>
<li><p>在项目根目录添加sonar scanner 配置文件 <code>sonar-project.properties</code></p>
<pre><code class="language-text">sonar.projectKey=&lt;sonarqube上创建项目时填写的key&gt;
sonar.sources=.<br/>
sonar.gitlab.project_id=git@gitlab.domain.lan/awesome/xxx.git ##你的git仓库地址
</code></pre></li>
</ol>

<h2 id="toc_2">GitLab Runner注册</h2>

<ol>
<li><p>拉取镜像</p>
<pre><code class="language-text">docker pull gitlab/gitlab-runner
</code></pre></li>
<li><p>利用docker注册镜像</p>
<pre><code class="language-text">docker run --rm -t -i -v /path/to/config:/etc/gitlab-runner --name  gitlab-runner gitlab/gitlab-runner register \
  --non-interactive \<br/>
  --executor &quot;docker&quot; \<br/>
  --docker-image alpine:3 \<br/>
  --url &quot;https://gitlab.com/&quot; \<br/>
      --registration-token &quot;PROJECT_REGISTRATION_TOKEN&quot; \<br/>
  --description &quot;docker-runner&quot; \<br/>
  --tag-list &quot;docker,aws&quot; \<br/>
  --run-untagged \<br/>
  --locked=&quot;false&quot;
</code></pre></li>
<li><p>启动GitLab Runner</p>
<pre><code class="language-text">docker run -d --name gitlab-runner --restart always \
 -v /srv/gitlab-runner/config:/etc/gitlab-runner \<br/>
 -v /var/run/docker.sock:/var/run/docker.sock \<br/>
 gitlab/gitlab-runner
</code></pre></li>
</ol>

<h2 id="toc_3">测试验证</h2>

<p>修改代码提交到gitlab，观察gitlab的pipeline的状态，以及sonarqube上新建项目的状态。</p>

<p><img src="https://pic.mylonly.com/2018-12-11-094744.png" alt="GitLab Pipeline"/><br/>
<img src="https://pic.mylonly.com/2018-12-11-094806.png" alt="Sonarqube"/></p>

<h2 id="toc_4">问题</h2>

<p>观察redmine的日志，如果出现权限问题，请注意仓库的用户组需要为redmine:redmine</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SonarQube部署]]></title>
    <link href="https://blog.xgtian.com/15444340190835.html"/>
    <updated>2018-12-10T17:26:59+08:00</updated>
    <id>https://blog.xgtian.com/15444340190835.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">部署postgres</h2>

<ol>
<li><p>拉取镜像</p>
<pre><code class="language-text">docker pull postgres
</code></pre></li>
<li><p>启动镜像</p>
<pre><code class="language-text">docker run --name postgres \
-e POSTGRES_USER=&quot;sonar&quot; \<br/>
-e POSTGRES_PASSWORD=****** \<br/>
-d postgres
</code></pre></li>
</ol>

<h2 id="toc_1">部署SonarQube</h2>

<ol>
<li><p>拉取镜像</p>
<pre><code class="language-text">docker pull sonarqube
</code></pre></li>
<li><p>启动镜像</p>
<pre><code class="language-text">docker run -d --name sonarqube\
    -p 80:9000     \<br/>
    -e sonar.jdbc.username=sonar \<br/>
    -e sonar.jdbc.password=****** \    <br/>
    -e sonar.jdbc.url=jdbc:postgresql://192.168.2.191/sonar sonarqube
</code></pre></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[利用SoftetherVPN和Bind9穿透内网，共享局域网内部资源]]></title>
    <link href="https://blog.xgtian.com/15440788666202.html"/>
    <updated>2018-12-06T14:47:46+08:00</updated>
    <id>https://blog.xgtian.com/15440788666202.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">写在前面</h2>

<p>我想很多公司由于安全考虑，一些内部系统(Git仓库、OA之类）的一定是放在公司内部的服务器上，只有在公司的网络下才能够访问，但是一些公司的随着业务的发展，肯定会出现一些外地的同事（外地的办事处、研发中心、销售团队等等）这种情况，这样一来，在内部服务不迁移到外网的前提下，如何让外地的同事能访问到内网这些资源就成了一个必须要解决的问题了。<br/>
其实内网穿透有很很多种解决方案，本文只是利用SoftetherVPN以及Bind9这两个工具提供另一种解决思路。<br/>
本文所提到的解决方案有如下优点:</p>

<pre><code class="language-text">1. 原来的内部资源不需要额外的配置改动
2. 需要身份认证，只有拥有权限的外部人员才能访问内部资源
3. 只有加入了共享网络的内部资源才能被外部访问，没有开放共享的内部资源的无法访问
4. 可以做到通过相同的域名访问同一个内部资源
5. 通过LPTP,LPSec协议支持OS X以及iOS设备访问
</code></pre>

<p>当然也有如下不足:<br/>
    1. 需要一台拥有公网IP的服务器作为中心服务器<br/>
    2. 由于本文的解决方案是基于虚拟局域网来实现的，因此拥有局域网内一些无法回避的缺点。</p>

<h2 id="toc_1">原理</h2>

<p><img src="https://pic.mylonly.com/2018-12-06-%E7%BD%91%E7%BB%9C%E5%9B%BE%20-1-.png" alt=""/><br/>
本解决方案利用了SoftetherVPN提供的Ad-Hoc VPN将内网服务器和外部用户通过VLAN连接在同一个局域网内，然后通过支持智能解析的DNS服务器，将域名解析到对应的虚拟局域网IP。<br/>
如果是内网用户，则直接通过DNS服务器将域名解析到内部资源对应的内网IP。</p>

<h2 id="toc_2">部署</h2>

<p>本次部署大部分采用Docker方式部署，如果需要手动安装的，可以在<a href="https://www.softether-download.com/cn.aspx?product=softether">这个网站</a>下载对应的安装包</p>

<h3 id="toc_3">Docker 环境</h3>

<p>略，不再本文讨论范围之内</p>

<h3 id="toc_4">安装Softether Server</h3>

<ol>
<li><p>拉取镜像</p>
<pre><code class="language-text">siomiz/softethervpn
</code></pre></li>
<li><p>启动vpnserver</p>
<pre><code class="language-text">docker run -d --cap-add NET_ADMIN \
 -p 500:500/udp \<br/>
 -p 4500:4500/udp \<br/>
 -p 1701:1701/tcp \<br/>
 -p 1194:1194/udp \<br/>
 -p 5555:5555/tcp \<br/>
 -p 443:443/tcp \<br/>
 -p 992:992 \<br/>
 siomiz/softethervpn
</code></pre>
<p>记得要开放对应的端口</p></li>
</ol>

<h3 id="toc_5">安装Softether ServerManger</h3>

<p>虽然Softether 提供了mac版的安装包，但本人装完之后发现其实是套了一个wine的环境，在mac上体验极差，所以此处推荐找一个windows电脑安装vpn server manager，<a href="https://www.softether-download.com/cn.aspx?product=softether">下载地址</a>在此<br/>
由于是windows上的安装教程，大部分都是图片，为了不占文章篇幅，就不放在文章里介绍了，各位可以参考<a href="https://www.softether.org/4-docs/2-howto/1.VPN_for_On-premise/1.Ad-hoc_VPN">官网</a>或者这篇<a href="http://www.leqii.com/download/yulan/d77/d77.html">博客</a></p>

<p>其中需要注意的是，在<code>虚拟HUB管理页面</code>中的<code>虚拟NAT和虚拟DHCP服务器</code>，点击<code>SecureNAT</code>,然后设置DHCP服务器的网段，以及DNS服务器为下面将要搭建的DNS服务器地址<br/>
<img src="https://pic.mylonly.com/2018-12-06-101214.png" alt=""/></p>

<h3 id="toc_6">部署DNS服务器</h3>

<ol>
<li><p>安装bind9</p>
<pre><code class="language-text">sudo apt-get install bind9
</code></pre></li>
<li><p>增加两个视图文件,在/etc/bind/目录下<br/>
name.2.conf</p>
<pre><code class="language-text">acl &quot;2&quot; {
    192.168.2.0/24;<br/>
};   
</code></pre>
<p>name.111.conf </p>
<pre><code class="language-text">acl &quot;111&quot; {
    192.168.111.0/24;<br/>
};
</code></pre>
<p>两个视图文件主要是为了区分来源IP所在的网段，然后根据不同的网段返回不同的解析结果，在本解决方案当中，192.168.2.0/24网段为内网网段，   192.168.111.0/24为VPN生成的虚拟局域网的网段</p></li>
<li><p>新建不同网段对应的不同的解析记录文件，也在/etc/bind/目录下</p>
<p>1234tv.lan-2.zone</p>
<pre><code class="language-text">$TTL    604800
@       IN      SOA      xxxx.xx. root.xxxx.xx. (<br/>
                              2         ; Serial<br/>
                         604800         ; Refresh<br/>
                          86400         ; Retry<br/>
                        2419200         ; Expire<br/>
                         604800 )       ; Negative Cache TTL<br/>
;<br/>
@       IN      NS      xxxx.xx.<br/>
@       IN      A      192.168.2.222<br/>
gitlab  IN      A       192.168.2.84
</code></pre>
<p>1234tv.lan-111.zone</p>
<pre><code class="language-text">$TTL    604800
@       IN      SOA     xxxx.xx. root.xxxx.xx. (<br/>
                              2         ; Serial<br/>
                         604800         ; Refresh<br/>
                          86400         ; Retry<br/>
                        2419200         ; Expire<br/>
                         604800 )       ; Negative Cache TTL<br/>
;<br/>
@       IN      NS      xxxx.xx.<br/>
@       IN      A       192.168.111.222<br/>
gitlab  IN      A       192.168.111.84
</code></pre></li>
<li><p>将视图文件和解析文件写入Bind9主配置文件</p>
<pre><code class="language-text">sudo vim /etc/bind/name.conf
</code></pre>
<p>增加如下内容，声明两个视图，view_2,view_11,可以注释掉默认带的配置文件，如果你没有其他的用途的话,注意每一行后面的分号，不可缺少。</p>
<pre><code class="language-text">#include &quot;/etc/bind/named.conf.options&quot;;
#include &quot;/etc/bind/named.conf.local&quot;;<br/>
#include &quot;/etc/bind/named.conf.default-zones&quot;;<br/>
include &quot;/etc/bind/name.2.conf&quot;;<br/>
view &quot;View_2&quot; {<br/>
        match-clients {&quot;2&quot;;};<br/>
        zone &quot;xxxx.xxx&quot; IN {<br/>
                type master;<br/>
                file &quot;xxxx.xxx-2.zone&quot;;<br/>
                allow-update {none;};<br/>
        };<br/>
};<br/>
include &quot;/etc/bind/name.111.conf&quot;;<br/>
view &quot;View_111&quot; {<br/>
        match-clients {&quot;111&quot;;};<br/>
        zone &quot;xxxx.xxx&quot; IN {<br/>
                type master;<br/>
                file &quot;xxxx.xxx-111.zone&quot;;<br/>
                allow-update {none;};<br/>
        };<br/>
};
</code></pre></li>
<li><p>然后启动Bind9 服务</p>
<pre><code class="language-text">sudo /etc/init.d/bind9 start
</code></pre></li>
<li><p>验证解析</p>
<pre><code class="language-text">dig gitlab.xxx.xx @192.168.2.222
</code></pre>
<p><img src="media/15440788666202/15440920519061.jpg" alt="" style="width:460px;"/></p></li>
</ol>

<h4 id="toc_7">安装Vpn Client</h4>

<p>所有需要共享给外部访问的内部服务器都需要安装vpn client连接上vpn server，下面的方法为linux 下docker方式部署vpn_client方法，windows端请参考这篇<a href="http://www.leqii.com/download/yulan/d77/d77.html">文章</a></p>

<ol>
<li><p>拉取镜像</p>
<pre><code class="language-text">sudo docker pull mitsutaka/softether-vpnclient
</code></pre></li>
<li><p>启动镜像连接之前创建的vpn server</p>
<pre><code class="language-text">docker run -d --name=softether-vpnclient \
--net=host --privileged \<br/>
-e VPN_SERVER=&lt;Softether VPN server&gt; \<br/>
-e VPN_PORT=&lt;Softether VPN port&gt; \<br/>
-e ACCOUNT_USER=&lt;Registered username&gt; \<br/>
-e ACCOUNT_PASS=&lt;Registered password&gt; \<br/>
-e VIRTUAL_HUB=&lt;Virtual Hub name&gt; \<br/>
-e TAP_IPADDR=&lt;IP address/netmask&gt; \<br/>
mitsutaka/softether-vpnclient<br/>
```<br/>
服务器端，VIRTUAL_HUB端填入之前在manager上新建的虚拟HUB名称 ，TAP_IPADDR，最好采用指定IP的方式，尤其是DNS服务器，需要固定IP地址
</code></pre></li>
</ol>

<h2 id="toc_8">结束语</h2>

<p>至此，这个简单的内网穿透方案算是部署完成，内网用户可以直接通过设置内网DNS服务器来通过域名访问内部服务器，外网用户在使用vpn clinet连接上虚拟局域网之后也能通过相同的域名访问内部服务器资源。<br/>
写这篇文章，一方面是觉得自己以后可能还会有此需求，怕自己遗忘，权且当做记录。另外也是想也可能有别的同学有这方面的需要，写出来共享一下。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[SoftEther VPN 部署]]></title>
    <link href="https://blog.xgtian.com/15439862777767.html"/>
    <updated>2018-12-05T13:04:37+08:00</updated>
    <id>https://blog.xgtian.com/15439862777767.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">Server安装部署</h2>

<p>点<a href="https://www.softether-download.com/cn.aspx?product=softether">这里</a>找到合适的软件包</p>

<p>或者直接用下面这个链接    </p>

<pre><code class="language-text">wget https://github.com/SoftEtherVPN/SoftEtherVPN_Stable/releases/download/v4.28-9669-beta/softether-vpnserver-v4.28-9669-beta-2018.09.11-linux-x64-64bit.tar.gz
</code></pre>

<ol>
<li><p>解压完拷贝到/usr/local目录,然后make编译</p>
<pre><code class="language-text">tar -zvxf softether-vpnserver-v4.28-9669-beta-2018.09.11-linux-x64-64bit.tar.gz
mv vpnserver /usr/local/<br/>
cd /usr/local/vpnserver<br/>
make
</code></pre></li>
<li><p>启动vpnserver</p>
<pre><code class="language-text">/usr/local/vpnserver start
#/usr/local/vpnserver stop
</code></pre>
<p><em>注意：SoftEther默认监听443端口，记得防火墙打开443端口</em></p></li>
<li><p>安装server manager</p>
<p>点击<a href="http://www.leqii.com/download/yulan/d77/d77.html">这里</a>查看manager设置</p></li>
<li><p>安装vpnclient</p>
<p>拉取vpnclient镜像</p>
<pre><code class="language-text">sudo docker pull mitsutaka/softether-vpnclient
</code></pre>
<p>启动vpnclient</p>
<pre><code class="language-text">docker run -d --name=softether-vpnclient \
--net=host --privileged \<br/>
-e VPN_SERVER=&lt;Softether VPN server&gt; \<br/>
-e VPN_PORT=&lt;Softether VPN port&gt; \<br/>
-e ACCOUNT_USER=&lt;Registered username&gt; \<br/>
-e ACCOUNT_PASS=&lt;Registered password&gt; \<br/>
-e VIRTUAL_HUB=&lt;Virtual Hub name&gt; \<br/>
-e TAP_IPADDR=&lt;IP address/netmask&gt; \<br/>
mitsutaka/softether-vpnclient
</code></pre>
<p>例子:</p>
<pre><code class="language-text">docker run -d --name=softether-vpnclient     --net=host --privileged     -e VPN_SERVER=120.132.55.152     -e VPN_PORT=443     -e ACCOUNT_USER=test     -e ACCOUNT_PASS=1234TV.com     -e VIRTUAL_HUB=1234TV   -e TAP_IPADDR=192.168.111.79/255.255.255.0 mitsutaka/softether-vpnclient
</code></pre></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GitLab Docker部署]]></title>
    <link href="https://blog.xgtian.com/15439814205804.html"/>
    <updated>2018-12-05T11:43:40+08:00</updated>
    <id>https://blog.xgtian.com/15439814205804.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>安装Docker</p>
<pre><code class="language-text">sudo wget -qO- https://get.docker.com/ | sh
</code></pre></li>
<li><p>非root用户需要将用户加到docker用户组中</p>
<pre><code class="language-text">sudo usermod -aG docker ubuntu
</code></pre></li>
<li><p>启动docker</p>
<pre><code class="language-text">sudo service docker start
</code></pre></li>
<li><p>拉取gitlab-ce镜像</p>
<pre><code class="language-text">sudo docker pull gitlab/gitlab-ce
</code></pre></li>
<li><p>启动gitlab-ce docker镜像</p>
<pre><code class="language-text">sudo docker run --detach \
--hostname gitlab.example.com \<br/>
--publish 443:443 --publish 80:80 --publish 22:22 \<br/>
--name gitlab \<br/>
--restart always \<br/>
--volume /srv/gitlab/config:/etc/gitlab \<br/>
--volume /srv/gitlab/logs:/var/log/gitlab \<br/>
--volume /srv/gitlab/data:/var/opt/gitlab \<br/>
gitlab/gitlab-ce:latest
</code></pre>
<p>具体的镜像命令可以参考<a href="https://docs.gitlab.com/omnibus/docker/">gitlab官方文档</a></p>
<p>如果是新安装的Ubuntu服务器，记得用ufw添加端口</p>
<pre><code class="language-text">sudo ufw allow 80/tcp
</code></pre></li>
</ol>

<h2 id="toc_0">设置默认的管理员账号密码</h2>

<ol>
<li><p>进入gitlab容器</p>
<pre><code class="language-text">docker exec -it gitlab /bin/bash
</code></pre></li>
<li><p>进入rails</p>
<pre><code class="language-text">cd /opt/gitlab/bin
gitlab-rails console production
</code></pre></li>
<li><p>设置root用户密码</p>
<pre><code class="language-text">irb(main):001:0&gt; u = User.where(email: &#39;admin@example.com&#39;).first
=&gt; #&lt;User id:1 @root&gt;<br/>
irb(main):002:0&gt; u.password<br/>
=&gt; nil<br/>
irb(main):003:0&gt; u.password=&#39;12345678&#39;<br/>
=&gt; &quot;12345678&quot;<br/>
irb(main):004:0&gt; u.save!<br/>
Enqueued ActionMailer::DeliveryJob (Job ID: b0e971db-4c6b-439c-82ee-86c7c277537e) to Sidekiq(mailers) with arguments: &quot;DeviseMailer&quot;, &quot;password_change&quot;, &quot;deliver_now&quot;, gid://gitlab/User/1<br/>
=&gt; true
</code></pre></li>
</ol>

<h2 id="toc_1">邮件配置</h2>

<ol>
<li><p>进入gitlab容器</p>
<pre><code class="language-text">docker exec -it gitlab /bin/bash
</code></pre></li>
<li><p>修改配置文件 </p>
<pre><code class="language-text">#vim /etc/gitlab/gitlab.rb，添加以下内容
gitlab_rails[&#39;gitlab_email_enabled&#39;] = true<br/>
gitlab_rails[&#39;gitlab_email_from&#39;] = &#39;gitlab@1234tv.com&#39;<br/>
gitlab_rails[&#39;gitlab_email_display_name&#39;] = &#39;GitLab&#39;<br/>
gitlab_rails[&#39;gitlab_email_reply_to&#39;] = &#39;noreply@1234tv.com&#39;<br/>
gitlab_rails[&#39;smtp_enable&#39;] = true<br/>
gitlab_rails[&#39;smtp_address&#39;] = &quot;smtp.exmail.qq.com&quot;<br/>
gitlab_rails[&#39;smtp_port&#39;] = 465<br/>
gitlab_rails[&#39;smtp_user_name&#39;] = &quot;gitlab@1234tv.com&quot;<br/>
gitlab_rails[&#39;smtp_password&#39;] = &quot;******&quot;<br/>
gitlab_rails[&#39;smtp_authentication&#39;] = &quot;login&quot;<br/>
gitlab_rails[&#39;smtp_enable_starttls_auto&#39;] = true<br/>
gitlab_rails[&#39;smtp_tls&#39;] = true
</code></pre></li>
<li><p>重新加载配置文件，并验证是否生效</p>
<pre><code class="language-text">gitlab-ctl reconfigure ##重新加载配置
gitlab-rails console  ##进入rails
</code></pre>
<p>输入 <code>ActionMailer::Base.delivery_method</code>:</p>
<pre><code class="language-text">irb(main):002:0* ActionMailer::Base.delivery_method
</code></pre>
<p>如果出现<code>=&gt;:smtp</code>说明设置成功了</p></li>
</ol>

]]></content>
  </entry>
  
</feed>
