<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[独自一人]]></title>
  <link href="https://blog.mylonly.com/atom.xml" rel="self"/>
  <link href="https://blog.mylonly.com/"/>
  <updated>2018-07-06T21:19:40+08:00</updated>
  <id>https://blog.mylonly.com/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.coderforart.com/">CoderForArt</generator>

  
  <entry>
    <title type="html"><![CDATA[我的豆瓣电影影评抓取之旅]]></title>
    <link href="https://blog.mylonly.com/15308616208129.html"/>
    <updated>2018-07-06T15:20:20+08:00</updated>
    <id>https://blog.mylonly.com/15308616208129.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">前言</h2>

<p>由于最近一直在研究基于机器学习的推荐系统，需要大量的数据来训练AI模型，但是在模型的测试验证过程中，苦于中文数据集的缺失(或者说根本没有，国人在这方面做得实在是太差了)，只能利用国外公开的推荐系统数据集，有著名的<a href="https://grouplens.org/datasets/movielens/">MovieLens电影评分数据集</a>和<a href="http://del.icio.us">Del.icio.us链接推荐数据集</a>，虽然通过计算损失函数也能大致的评估推荐模型的优劣程度从而进行相应的优化，但是由于语言环境、文化等等的不同，国外人对某个电影的评分毕竟跟我们还是有一定差距的，在输出推荐结果时，即使给出的某个电影或者某个网站链接其相似度很高时，我仍然不确定这个推荐结果是否真的如损失函数计算的那样准确。所以，为了能拥有一个可以用于训练的中文的数据集，就有了本文所记录的豆瓣影评的抓取过程。</p>

<h2 id="toc_1">网站分析</h2>

<p>首先还是要分析一下要抓取的网站<a href="https://movie.douban.com/">豆瓣电影</a>,主要是通过搜索引擎或者浏览器的调试工具看看有没有可以利用的API，在没有找到任何api的前提下才开始分析网站的页面结构，找到可以提取的信息。<br/>
通过搜索引擎，我找到了<a href="https://developers.douban.com/wiki/?title=api_v2">豆瓣开发者平台</a>，在<a href="https://developers.douban.com/wiki/?title=movie_v2">豆瓣电影</a>的文档中有获取电影，获取影评等等的详细接口，正当我以为接下来的数据采集将会变得非常简单之时，下面这张图还是让我冷静了下来<br/>
<img src="media/15308616208129/15308648552927.jpg" alt=""/></p>

<blockquote>
<p>如果你在2015年之前注册过豆瓣的开发者，那么恭喜你,你可以通过豆瓣提供的API或者SDK获取你想获得的任何数据</p>
</blockquote>

<h3 id="toc_2">电影信息获取</h3>

<p>虽然APIKey是不可能拿到了，但是通过文档我仍然发现了一些GET请求并不需要AUTH认证，也就是说有没有APIKey并不影响使用。其中，对我们有用的就是获取<code>TOP250电影列表</code>的接口:</p>

<pre><code>http://api.douban.com/v2/movie/top250
</code></pre>

<p>接口返回的格式大概如下:<br/>
<img src="media/15308616208129/15308650382688.jpg" alt=""/></p>

<p>里面包含了电影一些详细信息，对于推荐系统来说，这些数据足够了。</p>

<p>此外，通过利用Chrome的调试工具，在<a href="https://movie.douban.com/tag/#/">豆瓣电影-分类</a>这个页面我发现了他们使用的一个JQuery接口,也是一个GET请求，不需要AUTH。</p>

<p><img src="media/15308616208129/15308662057789.jpg" alt=""/><br/>
<img src="media/15308616208129/15308662242933.jpg" alt=""/></p>

<p>详细接口如下,可以通过更改start来迭代获取所有电影条目</p>

<pre><code>https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=0,10&amp;tags=&amp;start=20
</code></pre>

<p>这个接口可以获取所有豆瓣上收录的电影，经过我的测试，start改为10000时，返回的数据就已经是空的了</p>

<h3 id="toc_3">影评信息获取</h3>

<p>获取电影信息的方法有了，接下来就是要分析如何获取影评信息了。<br/>
在每部电影的详情页面里,如<a href="https://movie.douban.com/subject/1292064/">楚门的世界</a>，我们找到了如下这几个详情页面,分别显示了针对这部电影的影评和短评信息</p>

<pre><code>https://movie.douban.com/subject/1292064/reviews ##影评页面
https://movie.douban.com/subject/1292064/comments ##短评页面
</code></pre>

<p><img src="https://pic.mylonly.com/2018-07-06-085101.png" alt=""/></p>

<p>照例先用调试工具看看有没有可以用的api接口后，发现这次并没有那么好运了，这个影评页面是由服务器渲染完成的。</p>

<p>没有了接口，我们来分析页面，依然通过调试工具:<br/>
<img src="media/15308616208129/15308675859423.jpg" alt=""/><br/>
每条评论都是在一个review-item的div块里面，而所有评论都是在一个review-list的div块里吗，我们通过xpath语法可以很容易的定位到每条评论的详细信息,下面是所有信息的xpath语句，在我们写爬虫时候就靠他提取内容了</p>

<pre><code>评论列表: &quot;//div[contains(@class,&#39;review-list&#39;)]//div[contains(@class,&#39;review-item&#39;)]&quot;

评论ID: &quot;.//div[@class=&#39;main-bd&#39;]//div[@class=&#39;review-short&#39;]/@data-rid&quot;
作者头像: &quot;./header[@class=&#39;main-hd&#39;]//a[@class=&#39;avator&#39;]//img/@src &quot;
作者昵称: &quot;.//header//a[@class=&#39;name&#39;]/text()&quot;
推荐程度(评分): &quot;.//header//span[contains(@class,&#39;main-title-rating&#39;)]/@title&quot;
影评标题: &quot;.//div[@class=&#39;main-bd&#39;]//h2//a/text()&quot;
影评摘要: &quot;.//div[@class=&#39;main-bd&#39;]//div[@class=&#39;short-content&#39;]/text()&quot;
影评详情页链接: &quot;.//div[@class=&#39;main-bd&#39;]//h2//a/@href&quot;
</code></pre>

<p><img src="https://pic.mylonly.com/2018-07-06-090909.png" alt=""/></p>

<p>其中具体的评分我们不能直接拿到，而是只能拿到具体的文字描述,经过我的验证，具体如下对应关系如下:</p>

<pre><code>&#39;力荐&#39;: 5,
&#39;推荐&#39;: 4,
&#39;还行&#39;: 3,
&#39;较差&#39;: 2,
&#39;很差&#39;: 1,  
</code></pre>

<p>在后续的代码编写过程中，我们会根据这个对应关系将其转换为对应的评分信息</p>

<h2 id="toc_4">实现爬虫</h2>

<p>既然已经分析的差不多了，我们所需要的信息基本都有途径可以获得，那么接下来我们就开始具体的爬虫实现，我们采用Scrapy这个Python爬虫框架来帮我们简化爬虫的开发过程。Scrapy的安装以及VirtualEnv环境的搭建就不详细说了，其并不再本文的讨论范围之内，附上<a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/overview.html">Scrapy的中文文档地址</a></p>

<h3 id="toc_5">创建项目工程</h3>

<pre><code>##创建DoubanSpider工程
scrapy startproject Douban
</code></pre>

<p>创建好的工程目录大致如下:<br/>
<img src="media/15308616208129/15308694758600.jpg" alt=""/></p>

<p>其中：</p>

<pre><code>    spiders: 爬虫文件夹,存放具体的爬虫代码，我们待会要编写的两个爬虫(电影信息和影评信息)就需要放在这个文件夹下
    items.py: 模型类，所有需要结构化的数据都要预先在此文件中定义
    middlewares.py: 中间件类，scrapy的核心之一，我们会用到其中的downloadMiddleware,
    pipelines.py: 管道类，数据的输出管理，是存数据库还是存文件在这里决定
    settings.py: 设置类，一些全局的爬虫设置，如果每个爬虫需要有自定义的地方，可以在爬虫中直接设置custom_settings属性
</code></pre>

<h3 id="toc_6">电影信息爬虫</h3>

<blockquote>
<p>由于电影信息的获取有API接口可以使用，所以此处页可以不采用爬虫来处理数据。</p>
</blockquote>

<p>在spiders中新建一个movies.py的文件，定义我们的爬虫</p>

<p>由于我们爬取电影是通过api接口的形式获取，因此并不需要跟进解析，所以我们的爬虫直接继承Spider就可以了</p>

<h4 id="toc_7">定义爬虫</h4>

<pre><code>class MovieSpider(Spider):
    name = &#39;movie&#39; #爬虫名称
    allow_dominas = [&quot;douban.com&quot;] #允许的域名
    
    #自定义的爬虫设置，会覆盖全局setting中的设置
    custom_settings = {
        &quot;ITEM_PIPELINES&quot;: {
            &#39;Douban.pipelines.MoviePipeline&#39;: 300
        },
        &quot;DEFAULT_REQUEST_HEADERS&quot;: {
            &#39;accept&#39;: &#39;application/json, text/javascript, */*; q=0.01&#39;,
            &#39;accept-encoding&#39;: &#39;gzip, deflate&#39;,
            &#39;accept-language&#39;: &#39;zh-CN,zh;q=0.8,en;q=0.6,zh-TW;q=0.4&#39;,
            &#39;referer&#39;: &#39;https://mm.taobao.com/search_tstar_model.htm?spm=719.1001036.1998606017.2.KDdsmP&#39;,
            &#39;user-agent&#39;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.97 Safari/537.36&#39;,
            &#39;x-requested-with&#39;: &#39;XMLHttpRequest&#39;,
        },
        &quot;ROBOTSTXT_OBEY&quot;:False #需要忽略ROBOTS.TXT文件
    }

</code></pre>

<p>custom_setting中，<code>ITEM_PIPELINES</code>指定了获取数据后数据输出时使用的管道接口<br/>
<code>DEFAULT_REQUEST_HEADERS</code>则是让我们的Spider伪装成一个浏览器，防止被豆瓣拦截掉。<br/>
而<code>ROBOTSTXT_OBEY</code>则是让我们的爬虫忽略ROBOTS.txt的警告</p>

<p>接下来通过<code>start_request</code>告诉爬虫要爬取的链接：</p>

<pre><code class="language-Python">
    def start_requests(self):
        url = &#39;&#39;&#39;https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=0,10&amp;tags=&amp;start={start}&#39;&#39;&#39;
        requests = []
        for i in range(500):
            request = Request(url.format(start=i*20), callback=self.parse_movie)
            requests.append(request)
        return requests
</code></pre>

<p>由于我们之前分析网站的时候已经分析过了，start参数到10000时就获取不到数据了，所以此处直接用这个数字循环获得所有链接</p>

<p>接下来解析每个接口返回的内容:</p>

<pre><code class="language-Python">
def parse_movie(self, response):
    jsonBody = json.loads(response.body)
    subjects = jsonBody[&#39;data&#39;]
    movieItems = []
    for subject in subjects:
        item = MovieItem()
        item[&#39;id&#39;] = int(subject[&#39;id&#39;])
        item[&#39;title&#39;] = subject[&#39;title&#39;]
        item[&#39;rating&#39;] = float(subject[&#39;rate&#39;])
        item[&#39;alt&#39;] = subject[&#39;url&#39;]
        item[&#39;image&#39;] = subject[&#39;cover&#39;]
        movieItems.append(item)
    return movieItems
</code></pre>

<p>在request中，我们指定了一个parse_movie的方法来解析返回的内容，此处我们需要使用一个在items.py中定义的Item,具体Item如下:</p>

<h4 id="toc_8">定义Item</h4>

<pre><code class="language-Python">#定义你需要获取的数据
class MovieItem(scrapy.Item):
    id = scrapy.Field()
    title = scrapy.Field()
    rating = scrapy.Field()
    genres = scrapy.Field()
    original_title = scrapy.Field()
    alt = scrapy.Field()
    image = scrapy.Field()
    year = scrapy.Field()
</code></pre>

<p>items返回给Scrapy之后，Scrapy会调用我们之前在custom_setting中指定的<code>Douban.pipelines.MoviePipeline</code>来处理获取到的item，MoviePipeline定义在pipelines.py中，具体内容如下:</p>

<h4 id="toc_9">定义Pipeline</h4>

<pre><code class="language-Python">class MoviePipeline(object):

    movieInsert = &#39;&#39;&#39;insert into movies(id,title,rating,genres,original_title,alt,image,year) values (&#39;{id}&#39;,&#39;{title}&#39;,&#39;{rating}&#39;,&#39;{genres}&#39;,&#39;{original_title}&#39;,&#39;{alt}&#39;,&#39;{image}&#39;,&#39;{year}&#39;)&#39;&#39;&#39;

    def process_item(self, item, spider):

        id = item[&#39;id&#39;]
        sql = &#39;select * from movies where id=%s&#39;% id
        self.cursor.execute(sql)
        results = self.cursor.fetchall()
        if len(results) &gt; 0:
            rating = item[&#39;rating&#39;]
            sql = &#39;update movies set rating=%f&#39; % rating
            self.cursor.execute(sql)
        else:
            sqlinsert = self.movieInsert.format(
                id=item[&#39;id&#39;],
                title=pymysql.escape_string(item[&#39;title&#39;]),
                rating=item[&#39;rating&#39;],
                genres=item.get(&#39;genres&#39;),
                original_title=item.get(&#39;original_title&#39;),
                alt=pymysql.escape_string(item.get(&#39;alt&#39;)),
                image=pymysql.escape_string(item.get(&#39;image&#39;)),
                year=item.get(&#39;year&#39;)
            )
            self.cursor.execute(sqlinsert)
        return item

    def open_spider(self, spider):
        self.connect = pymysql.connect(&#39;localhost&#39;,&#39;root&#39;,&#39;******&#39;,&#39;douban&#39;, charset=&#39;utf8&#39;, use_unicode=True)
        self.cursor = self.connect.cursor()
        self.connect.autocommit(True)


    def close_spider(self, spider):
        self.cursor.close()
        self.connect.close()

</code></pre>

<p>在此Pipeline中，我们通过连接mysql数据库将每次获取到的item插入到具体的数据表中</p>

<h4 id="toc_10">运行爬虫</h4>

<p>在命令行下输入:</p>

<pre><code>scrapy crawl movie
</code></pre>

<p><img src="https://pic.mylonly.com/2018-07-06-101458.png" alt=""/><br/>
<img src="https://pic.mylonly.com/2018-07-06-101353.png" alt=""/></p>

<h3 id="toc_11">影评爬虫</h3>

<p>影评爬虫的难度要大很多了，因为获取电影信息我们是通过接口直接拿到的，这种接口返回的数据格式统一，基本不会出现异常情况，而且电影数量有限，很短时间就能爬取完毕，并不会触发豆瓣的防爬虫机制，而在影评爬虫的编写过程中，这些都会遇到。</p>

<h4 id="toc_12">爬虫逻辑</h4>

<pre><code>class ReviewSpider(Spider):
    name = &quot;review&quot;
    allow_domain = [&#39;douban.com&#39;]
    custom_settings = {
        &quot;ITEM_PIPELINES&quot;: {
            &#39;Douban.pipelines.ReviewPipeline&#39;: 300
        },
        &quot;DEFAULT_REQUEST_HEADERS&quot;: {
            &#39;connection&#39;:&#39;keep-alive&#39;,
            &#39;Upgrade-Insecure-Requests&#39;:&#39;1&#39;,
            &#39;DNT&#39;:1,
            &#39;Accept&#39;:&#39;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&#39;,
            &#39;Accept-Encoding&#39;:&#39;gzip, deflate, br&#39;,
            &#39;Accept-Language&#39;:&#39;zh-CN,zh;q=0.9,en;q=0.8,zh-TW;q=0.7&#39;,
            &#39;Cookie&#39;:&#39;bid=wpnjOBND4DA; ll=&quot;118159&quot;; __utmc=30149280;&#39;,            &#39;user-agent&#39;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) AppleWebKit/537.36 (KHTML, like Gecko) &#39;
                          &#39;Chrome/67.0.3396.87 Safari/537.36&#39;,
        },
        &quot;ROBOTSTXT_OBEY&quot;: False,
        # &quot;DOWNLOAD_DELAY&quot;: 1,
        &quot;RETRY_TIMES&quot;: 9,
        &quot;DOWNLOAD_TIMEOUT&quot;: 10
    }
</code></pre>

<p>对比获取电影信息的爬虫，在custom_setting中多了几个设置：<br/>
<code>RETRY_TIMES</code>：用来控制最大重试次数，因为豆瓣有反爬虫机制，当一个IP访问次数过多时就会限制这个IP访问，所以为了绕过这个机制，我们通过代理IP来爬取对应的页面，每爬取一个页面就更换一次IP，但是由于代理IP的质量参差不齐，收费的可能会好点，但还是会存在，为了避免出现因为代理连接不上导致某个页面被忽略掉，我们设置这个值，当重试次数大于设定的值时仍然没有获取到页面就会pass掉这个连接。如果你的代理IP质量不好，请增大此处的次数。<br/>
<code>DOWNLOAD_TIMEOUT</code>: 下载超时时间，默认是60秒，此处修改为10秒是想让整体的爬取速度加快，因为RETRY_TIMES的缘故，需要RETRY的判定时间为1分钟，如果有很多这种有问题的页面，那么整个爬取的过程会十分漫长。<br/>
<code>DOWNLOAD_DELAY</code>: 下载延迟，如果你使用代理IP之后还是会出现访问返回403的情况，请设置此值，因为某IP太频繁的访问页面会触发豆瓣的防爬虫机制。</p>

<pre><code class="language-Python">  def start_requests(self):
        #从数据库中找到所有的moviesId
        self.connect = pymysql.connect(&#39;localhost&#39;,&#39;root&#39;,&#39;******&#39;,&#39;douban&#39;, charset=&#39;utf8&#39;, use_unicode=True)
        self.cursor = self.connect.cursor()
        self.connect.autocommit(True)
        sql = &quot;select id,current_page,total_page from movies&quot;
        self.cursor.execute(sql)
        results = self.cursor.fetchall()
        url_format = &#39;&#39;&#39;https://movie.douban.com/subject/{movieId}/reviews?start={offset}&#39;&#39;&#39;
        for row in results:
            movieId = row[0]
            current_page = row[1]
            total_page = row[2]
            if current_page != total_page: ##说明评论没有爬完
                url = url_format.format(movieId=movieId, offset=current_page*20)
                request = Request(url, callback=self.parse_review, meta={&#39;movieId&#39;: movieId}, dont_filter=True)
                yield request
</code></pre>

<p>照例，我们在start_request中告诉Scrapy要爬取的起始网址链接，通过我们之前的分析，影评页面的地址格式为:</p>

<pre><code class="language-Python">https://movie.douban.com/subject/{movieId}/reviews?start={offset}
</code></pre>

<p>而movieId,我们之前的爬虫已经将所有电影的信息抓取了下来，所以我们在此先通过查询数据库将所有的已抓取的电影信息获取到，取到其中的movieId，然后构造一个页面链接。</p>

<pre><code class="language-Python">url = url_format.format(movieId=movieId, offset=current_page*20)
</code></pre>

<p>因为抓取豆瓣影评的过程十分漫长，中间会出现各种各样的问题导致爬虫意外退出，因此我们需要一个机制让爬虫能从上次停止的地方继续爬取，current_page和total_page就是为此而服务的，在后面的数据解析过程中，每解析一个页面，就会将当期页面的页数存储下来，防止出现意外情况。</p>

<pre><code class="language-Python">    def parse_review(self, response):
        movieId = response.request.meta[&#39;movieId&#39;]
        review_list = response.xpath(&quot;//div[contains(@class,&#39;review-list&#39;)]//div[contains(@class,&#39;review-item&#39;)]&quot;)
        for review in review_list:
            item = ReviewItem()
            item[&#39;id&#39;] = review.xpath(&quot;.//div[@class=&#39;main-bd&#39;]//div[@class=&#39;review-short&#39;]/@data-rid&quot;).extract()[0]
            avator = review.xpath(&quot;.//header//a[@class=&#39;avator&#39;]/@href&quot;).extract()[0]
            item[&#39;username&#39;] = avator.split(&#39;/&#39;)[-2]
            item[&#39;avatar&#39;] = review.xpath(&quot;./header[@class=&#39;main-hd&#39;]//a[@class=&#39;avator&#39;]//img/@src&quot;).extract()[0]
            item[&#39;nickname&#39;] = review.xpath(&quot;.//header//a[@class=&#39;name&#39;]/text()&quot;).extract()[0]
            item[&#39;movieId&#39;] = movieId
            rate = review.xpath(&quot;.//header//span[contains(@class,&#39;main-title-rating&#39;)]/@title&quot;).extract()
            if len(rate)&gt;0:
                rate = rate[0]
                item[&#39;rating&#39;] = RATING_DICT.get(rate)
                item[&#39;create_time&#39;] = review.xpath(&quot;.//header//span[@class=&#39;main-meta&#39;]/text()&quot;).extract()[0]
                item[&#39;title&#39;] = review.xpath(&quot;.//div[@class=&#39;main-bd&#39;]//h2//a/text()&quot;).extract()[0]
                item[&#39;alt&#39;] = review.xpath(&quot;.//div[@class=&#39;main-bd&#39;]//h2//a/@href&quot;).extract()[0]
                summary = review.xpath(&quot;.//div[@class=&#39;main-bd&#39;]//div[@class=&#39;short-content&#39;]/text()&quot;).extract()[0]
                item[&#39;summary&#39;] = summary.strip().replace(&#39;\n&#39;, &#39;&#39;).replace(&#39;\xa0(&#39;,&#39;&#39;)
                yield item

        current_page = response.xpath(&quot;//span[@class=&#39;thispage&#39;]/text()&quot;).extract()
        total_page = response.xpath(&quot;//span[@class=&#39;thispage&#39;]/@data-total-page&quot;).extract()
        paginator = response.xpath(&quot;//div[@class=&#39;paginator&#39;]&quot;).extract()
        if len(paginator) == 0 and len(review_list): ##不存在导航条，但是评论列表存在，说明评论只有一页

            sql = &quot;update movies set current_page = 1, total_page=1 where id=&#39;%s&#39;&quot; % movieId
            self.cursor.execute(sql)

        elif len(paginator) and len(review_list):
            current_page = int(current_page[0])
            total_page = int(total_page[0])
            sql = &quot;update movies set current_page = %d, total_page=%d where id=&#39;%s&#39;&quot; % (current_page, total_page, movieId)
            self.cursor.execute(sql)
            if current_page != total_page:
                url_format = &#39;&#39;&#39;https://movie.douban.com/subject/{movieId}/reviews?start={offset}&#39;&#39;&#39;
                next_request = Request(url_format.format(movieId=movieId, offset=current_page*20),
                                       callback=self.parse_review,
                                       dont_filter=True, meta={&#39;movieId&#39;: movieId})
                yield next_request

        else:
            yield response.request
</code></pre>

<p>接下来，分析解析函数，DoubanItem的数据获取就不额外介绍了，利用之前分析时用到的xpath语句可以很容易的定义到具体内容。<br/>
其中movieId是起始链接中通过Request中Meta属性传递过来的，当前你可以通过分析网页找到包含movieId的地方。</p>

<pre><code>current_page = response.xpath(&quot;//span[@class=&#39;thispage&#39;]/text()&quot;).extract()
total_page = response.xpath(&quot;//span[@class=&#39;thispage&#39;]/@data-total-page&quot;).extract()
paginator = response.xpath(&quot;//div[@class=&#39;paginator&#39;]&quot;).extract()
</code></pre>

<p>上面基础代码的作用主要是为了获取影评页面的底部导航条</p>

<p><img src="media/15308616208129/15308756078903.jpg" alt=""/></p>

<p>但是这个导航条会有两种情况获取不到:</p>

<pre><code>1. 当某个电影的评论不足20条时，也就是只有一页评论。
2. 当触发了豆瓣的反爬虫的机制时，返回的页面并不是评论页面，而是一个验证页面，自然也找不到导航条
</code></pre>

<p>所以在下面的代码中，我通过这几个变量来判断了以上几种情况：</p>

<pre><code>1. 情况1时，不需要继续爬取剩下的评论，直接将current_page和total_page设置为1保存到movie表即可
2. 情况2时，由于此时触发了反爬虫机制，返回的页面没有我们的数据，如果我们直接忽略掉的话，会损失大量的数据（这种情况很常见），所以我们就干脆再试一次，返回request，让Scrapy重新爬取这个页面，因为每次重新爬取都会换一个新的代理IP，所以我们有很大概率下次抓取就是正常的。此处有一点需要注意：因为Scrapy默认会过滤掉重复请求，所以我们需要在构造Request的时候讲dont_filter参数设置为True,让其不要过滤重复链接。
3. 正常情况时，通过xpath语法获取的下一页评论的链接地址然后构造一个request交给Scrapy继续爬取

####影评下载中间件
上面说过，抓取影评页面时需要通过使用代理IP的方式来达到绕过豆瓣的反爬虫机制，具体代理的设置就需要在DownloadMiddleware中设置

```Python
class DoubanDownloaderMiddleware(object):
# Not all methods need to be defined. If a method is not defined,
# scrapy acts as if the downloader middleware does not modify the
# passed objects.

ip_list = None

@classmethod
def from_crawler(cls, crawler):
    # This method is used by Scrapy to create your spiders.
    s = cls()
    crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
    return s

def process_request(self, request, spider):
    # Called for each request that goes through the downloader
    # middleware.

    # Must either:
    # - return None: continue processing this request
    # - or return a Response object
    # - or return a Request object
    # - or raise IgnoreRequest: process_exception() methods of
    #   installed downloader middleware will be called

    if self.ip_list is None or len(self.ip_list) == 0:
        response = requests.request(&#39;get&#39;,&#39;http://api3.xiguadaili.com/ip/?tid=555688914990728&amp;num=10&amp;protocol=https&#39;).text
        self.ip_list = response.split(&#39;\r\n&#39;)

    ip = random.choice(self.ip_list)
    request.meta[&#39;proxy&#39;] = &quot;https://&quot;+ip
    print(&quot;当前proxy:%s&quot; % ip)
    self.ip_list.remove(ip)
    return None

def process_response(self, request, response, spider):
    # Called with the response returned from the downloader.
    # Must either;
    # - return a Response object
    # - return a Request object
    # # - or raise IgnoreRequest

    if response.status == 403:
        res = parse.urlparse(request.url)
        res = parse.parse_qs(res.query)
        url = res.get(&#39;r&#39;)
        if url and len(url) &gt; 0 :
            request = request.replace(url=res[&#39;r&#39;][0])
        return request

    return response
```    
</code></pre>

<p>其中主要就要实现两个函数，process_request和process_response，前者是每次爬取页面前Scrapy会调用这个函数，后者则是每次爬取完页面之后调用。<br/>
    在前者方法里，我们通过调用一个在线的代理ip获取接口，获取一个代理IP，然后设置request的proxy属性达到更换代理的功能，当然，你也可以通过文件读取代理IP。<br/>
    在后者的方法里，我们判断了状态码为403的状况，因为这个状态码标识当前的request被反爬虫禁止侦测并禁止了，而我们要做的就是把这个禁止的request地址重新包装下放到Scrapy的爬取队列当中。</p>

<h4 id="toc_13">影评Item</h4>

<pre><code class="language-Python">class ReviewItem(scrapy.Item):
id = scrapy.Field()
username = scrapy.Field()
nickname = scrapy.Field()
avatar = scrapy.Field()
movieId = scrapy.Field()
rating = scrapy.Field()
create_time = scrapy.Field()
title = scrapy.Field()
summary = scrapy.Field()
alt = scrapy.Field()
</code></pre>

<p>没啥好说的，想存啥就写啥</p>

<h4 id="toc_14">影评Pipeline</h4>

<pre><code class="language-Python">
class ReviewPipeline(object):

    reviewInsert = &#39;&#39;&#39;insert into reviews(id,username,nickname,avatar,summary,title,movieId,rating,create_time,alt) values (&quot;{id}&quot;,&quot;{username}&quot;, &quot;{nickname}&quot;,&quot;{avatar}&quot;, &quot;{summary}&quot;,&quot;{title}&quot;,&quot;{movieId}&quot;,&quot;{rating}&quot;,&quot;{create_time}&quot;,&quot;{alt}&quot;)&#39;&#39;&#39;

    def process_item(self, item, spider):
        sql_insert = self.reviewInsert.format(
            id=item[&#39;id&#39;],
            username=pymysql.escape_string(item[&#39;username&#39;]),
            nickname=pymysql.escape_string(item[&#39;nickname&#39;]),
            avatar=pymysql.escape_string(item[&#39;avatar&#39;]),
            summary=pymysql.escape_string(item[&#39;summary&#39;]),
            title=pymysql.escape_string(item[&#39;title&#39;]),
            rating=item[&#39;rating&#39;],
            movieId=item[&#39;movieId&#39;],
            create_time=pymysql.escape_string(item[&#39;create_time&#39;]),
            alt=pymysql.escape_string(item[&#39;alt&#39;])
        )
        print(&quot;SQL:&quot;, sql_insert)
        self.cursor.execute(sql_insert)
        return item

    def open_spider(self, spider):
        self.connect = pymysql.connect(&#39;localhost&#39;,&#39;root&#39;,&#39;******&#39;,&#39;douban&#39;, charset=&#39;utf8&#39;, use_unicode=True)
        self.cursor = self.connect.cursor()
        self.connect.autocommit(True)


    def close_spider(self, spider):
        self.cursor.close()
        self.connect.close()
</code></pre>

<pre><code>和之前的电影的pipeline类似，就是基本的数据库写操作。
</code></pre>

<h4 id="toc_15">运行爬虫</h4>

<pre><code class="language-Shell">scrapy crawl review
</code></pre>

<p>在我写完这篇文章时，影评的爬虫仍然还在爬取当中：<br/>
<img src="https://pic.mylonly.com/2018-07-06-113420.png" alt=""/><br/>
查看数据库，已经有97W的数据了:</p>

<p><img src="https://pic.mylonly.com/2018-07-06-113210.png" alt=""/><br/>
<img src="https://pic.mylonly.com/2018-07-06-113607.png" alt=""/></p>

<p><em>如果你觉得我的文章对你有帮助，请赞助一杯☕️</em></p>

<p><img src="https://pic.mylonly.com/2018-07-06-IMG_2094-1.JPG" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[MySql 相关问题]]></title>
    <link href="https://blog.mylonly.com/15307740267953.html"/>
    <updated>2018-07-05T15:00:26+08:00</updated>
    <id>https://blog.mylonly.com/15307740267953.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>查看数据库服务器字符集</p>

<pre><code>mysql&gt; show variables like &#39;%char%&#39;;
+--------------------------+-------------------------------------+------
| Variable_name | Value |......
+--------------------------+-------------------------------------+------
| character_set_client | utf8 |...... -- 客户端字符集
| character_set_connection | utf8 |......
| character_set_database | utf8 |...... -- 数据库字符集
| character_set_filesystem | binary |......
| character_set_results | utf8 |......
| character_set_server | utf8 |...... -- 服务器字符集
| character_set_system | utf8 |......
| character_sets_dir | D:\MySQL Server 5.0\share\charsets\ |......
+--------------------------+-------------------------------------+------
</code></pre></li>
<li><p>查看数据表(table)字符集</p>

<pre><code>mysql&gt; show table status from sqlstudy_db like &#39;%countries%&#39;;
+-----------+--------+---------+------------+------+-----------------+------
| Name | Engine | Version | Row_format | Rows | Collation |......
+-----------+--------+---------+------------+------+-----------------+------
| countries | InnoDB | 10 | Compact | 11 | utf8_general_ci |......
+-----------+--------+---------+------------+------+-----------------+------
</code></pre></li>
<li><p>查看数据列(column)字符集</p>

<pre><code>mysql&gt; show full columns from countries;
+----------------------+-------------+-----------------+--------
| Field | Type | Collation | .......
+----------------------+-------------+-----------------+--------
| countries_id | int(11) | NULL | .......
| countries_name | varchar(64) | utf8_general_ci | .......
| countries_iso_code_2 | char(2) | utf8_general_ci | .......
| countries_iso_code_3 | char(3) | utf8_general_ci | .......
| address_format_id | int(11) | NULL | .......
+----------------------+-------------+-----------------+--------
</code></pre></li>
<li><p>修改数据库(database)，数据表(table)，数据列(column)字符集</p>

<pre><code>alter database name character set utf8;
create database name character set utf8;
alter table 表名 convert to character set gbk;
alter table 表名 modify column &#39;字段名&#39; varchar(30) character set gbk not null;
</code></pre></li>
<li><p>通过数据库my.cnf配置文件设置字符集</p>

<pre><code>vi /etc/my.cnf
#在[client]下添加
default-character-set=utf8
#在[mysqld]下添加
default-character-set=utf8
</code></pre>

<p>重启mysql</p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[制作Kali Linux 加密U盘启动盘]]></title>
    <link href="https://blog.mylonly.com/15278621156583.html"/>
    <updated>2018-06-01T22:08:35+08:00</updated>
    <id>https://blog.mylonly.com/15278621156583.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">前置条件</h2>

<ol>
<li>1个至少是32GB的U盘，最好是USB3.0接口的</li>
<li>一台可以安装Virtual Box的电脑</li>
</ol>

<h2 id="toc_1">需要下载的文件</h2>

<ol>
<li>Kali Linux 64位 iso镜像文件: <a href="https://www.kali.org/downloads/">下载地址</a></li>
<li>Virtual Box 安装包 <a href="https://www.virtualbox.org/wiki/Downloads">下载地址</a></li>
<li>Kali Linux OVA镜像文件(OVA格式) <a href="https://www.offensive-security.com/kali-linux-vm-vmware-virtualbox-hyperv-image-download/">下载地址</a></li>
<li>Virtual Box Extension Pack <a href="https://www.virtualbox.org/wiki/Downloads">下载地址</a></li>
</ol>

<h2 id="toc_2">安装Kali Linux 虚拟机</h2>

<p>我们需要借助Kali里面的一些工具来帮助我们制作加密U盘，所以我们需要首先安装一个Kali Linux虚拟机，如果你已经有了类似的虚拟机，可以忽略这个步骤。</p>

<p>利用安装包安装完VirtualBox之后：</p>

<ol>
<li>在菜单项<code>管理</code>当中选择<code>导入虚拟电脑</code>，然后选中之前下载好的Kali的OVA格式虚拟机镜像文件。</li>
<li>安装Virtual Box Extension Pack，这步主要是让虚拟机能支持USB3.0的设备</li>
<li>修改已经导入的Kali虚拟机的设置，在USB设备选项中选择<code>USB 3.0 控制器</code>
<img src="https://pic.mylonly.com/2018-06-01-589FAFA6C598ACDFD296B6D320D525C7.jpg" alt=""/></li>
<li>启动Kali虚拟机，进入Kali之后，点击VirtualBox的<code>设备</code>菜单项，选择<code>安装增强功能</code>,如果提示安装失败，可以直接将桌面上出现的光盘中的VBoxLinuxAdditions.run 拷贝至其他目录，修改其权限为755，然后手动运行此脚本即可。
<img src="https://pic.mylonly.com/2018-06-01-151200.png" alt=""/></li>
<li>设置共享文件夹，在VirtualBox当中操作，这步主要是方便后面将下载在windows里的Kali的iso文件拷贝至Kali当中。
<img src="https://pic.mylonly.com/2018-06-01-56483D9C90D387A88824988AFDE2DCF1.jpg" alt=""/></li>
</ol>

<h2 id="toc_3">U盘初始设置</h2>

<ol>
<li>选择VirtualBox的<code>设备</code>菜单，在<code>USB</code>子菜单选中已经插在主机上的U盘设备，将U盘映射到虚拟机当中
<img src="https://pic.mylonly.com/2018-06-01-151239.png" alt=""/></li>
<li><p>找到Kali当中的GParted工具打开，选择已经挂载的U盘(如果你的虚拟机之前没有挂载过其他U盘,默认的挂载分区应该是<code>/dev/sdb</code>,本文演示截图当中显示为/dev/sdc是因为之前已经挂载过一个U盘)<br/>
<img src="media/15278621156583/15278659829428.jpg" alt=""/></p></li>
<li><p>然后先将U盘卸载，然后删除分区（记得执行顶部工具栏的回车样式的按钮）<br/>
<img src="media/15278621156583/15278659963602.jpg" alt=""/></p></li>
</ol>

<h2 id="toc_4">写入镜像、U盘分区</h2>

<ol>
<li>将之前共享文件夹里已经下载好的Kali的iso文件拷贝至Kali当中</li>
<li>利用如下命令将该iso文件拷贝至U盘当中</li>
</ol>

<pre><code>dd if=kali-linux-2018.2-amd64.iso of=/dev/sdc bs=1M  
</code></pre>

<ol>
<li>利用parted工具对U盘当中未使用的其他部分进行分区，命令如下</li>
</ol>

<pre><code class="language-Bash">parted  #进入parted界面
select /dev/sdc #选择U盘分区
print #查看当前分区信息
mkpart primary 2937 23417 #制作新的分区，起始位置从2937M开始，到23417位置结束
print #查看新的分区信息
</code></pre>

<p><img src="https://pic.mylonly.com/2018-06-01-152223.jpg" alt=""/></p>

<h2 id="toc_5">制作加密U盘</h2>

<ol>
<li><p>制作加密分区</p>

<pre><code>cryptsetup --verbose --verify-passphrase luksFormat /dev/sdc3 
</code></pre>

<p>确认覆盖分区，输入大写的YES，然后输入两遍密码之后，加密分区制作完毕<br/>
<strong>注意/dev/sdc3,在上面利用parted制作新分区时，其属于/dev/sdc下面的第三个分区</strong><br/>
<img src="https://pic.mylonly.com/2018-06-01-15A3CBD2C0856CCC83089E164F496957.png" alt=""/></p></li>
<li><p>对加密分区格式化分区,分配卷标</p>

<pre><code class="language-Bash">cryptsetup luksOpen /dev/sdc3 usb  #打开加密的/dev/sdc3分区至usb文件

ls /dev/mapper/usb #上面的命令执行完毕之后会在/dev/mapper目录下生成一个usb文件

mkfs.ext4 /dev/mapper/usb #将打开的分区格式化成ext4格式

e2label /dev/mapper/usb persistence #将分区卷标指定为persistence,名字必须为persistence
</code></pre></li>
<li><p>挂载新的分区,写入验证文件</p>

<pre><code class="language-Bash">mkdir -p /mnt/usb 
mount /dev/mapper/usb /mnt/usb #将之前格式化的usb设备挂载到/mnt/usb目录下
echo &quot;/ union&quot; &gt; /mnt/usb/persistence.conf
#写入一个persistence.conf文件，此文件会在启动时用来确认此U盘是用来加密存储的分区
</code></pre></li>
<li><p>卸载分区，退出加密分区</p>

<pre><code class="language-Bash">umount /dev/mapper/usb
cryptsetup luksClose /dev/mapper/usb
</code></pre>

<p><img src="media/15278621156583/E0D62780718DE48D31F6CDB55916B0D3.png" alt="E0D62780718DE48D31F6CDB55916B0D3"/></p></li>
</ol>

<h2 id="toc_6">重启物理电脑，选择从U盘启动</h2>

<p>进入Boot Menu之后，选择LIve USB Encrypted Persistence 进入系统，之后会让你输入之前创建加密分区时输入的密码，密码输入正确才能正确的进入系统，至此，整个制作加密Kali Linux U盘启动盘过程结束。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[uwsgi重启shell脚本]]></title>
    <link href="https://blog.mylonly.com/15277767431151.html"/>
    <updated>2018-05-31T22:25:43+08:00</updated>
    <id>https://blog.mylonly.com/15277767431151.html</id>
    <content type="html"><![CDATA[
<pre><code>#!/bin/bash
if [ ! -n &quot;$1&quot; ]
then
    echo &quot;Usages: sh uwsgiserver.sh [start|stop|restart]&quot;
    exit 0
fi

if [ $1 = start ]
then
    psid=`ps aux | grep &quot;uwsgi&quot; | grep -v &quot;grep&quot; | wc -l`
    if [ $psid -gt 4 ]
    then
        echo &quot;uwsgi is running!&quot;
        exit 0
    else
        uwsgi /etc/uwsgi.ini
        echo &quot;Start uwsgi service [OK]&quot;
    fi
    

elif [ $1 = stop ];then
    killall -9 uwsgi
    echo &quot;Stop uwsgi service [OK]&quot;
elif [ $1 = restart ];then
    killall -9 uwsgi
    /usr/bin/uwsgi --ini /etc/uwsgi.ini #修改成自己业务的配置文件或命令
    echo &quot;Restart uwsgi service [OK]&quot;

else
    echo &quot;Usages: sh uwsgiserver.sh [start|stop|restart]&quot;
fi
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Linux问题集锦]]></title>
    <link href="https://blog.mylonly.com/15277762940932.html"/>
    <updated>2018-05-31T22:18:14+08:00</updated>
    <id>https://blog.mylonly.com/15277762940932.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>CentOS python升级之后yum无法使用解决</p>

<p>yum是基于python2.6.6实现的，修改办法如下,编辑yum脚本</p>

<pre><code>vim /usr/bin/yum
</code></pre>

<p>将文件头部的<code>#!/usr/bin/python</code>修改为<code>#!/usr/bin/python2.6.6</code></p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pyplot相关]]></title>
    <link href="https://blog.mylonly.com/15277545119360.html"/>
    <updated>2018-05-31T16:15:11+08:00</updated>
    <id>https://blog.mylonly.com/15277545119360.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>subplot</p>

<pre><code>plt.subplot(221) # 表示分成两行两列，占用第一个，即第一行第一列的子图
plt.subplot(222) # 表示分成两行两列，占用第二个，即第一行第二列的子图
plt.subplot(212) # 表示分成两行一列，占用第二个，即第二行第一列的子图
</code></pre>

<p>subplot(numRows, numCols, plotNum)<br/>
如果3个参数都小于10的话，可以统一成一个整数<br/>
subplot(221) = subplot(2,2,1)</p></li>
<li><p><img src="media/15277545119360/15277557443176.jpg" alt=""/></p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Pandas相关]]></title>
    <link href="https://blog.mylonly.com/15276700222671.html"/>
    <updated>2018-05-30T16:47:02+08:00</updated>
    <id>https://blog.mylonly.com/15276700222671.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>Pandas 读取列数据</p>

<pre><code class="language-Python">city_names = pd.Series([&#39;San Francisco&#39;, &#39;San Jose&#39;, &#39;Sacramento&#39;])
population = pd.Series([852469, 1015785, 485199])
data = pd.DataFrame({ &#39;City name&#39;: city_names, &#39;Population&#39;: population })

data[&#39;City_Name&#39;]  #获取&#39;City_Name&#39;这个列对象，返回值类型为Series
data[[&#39;City_Name&#39;]] #获取&#39;City_Name&#39;列包含的数据，返回值类型为DateFrame
</code></pre></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Tensorflow错误集锦]]></title>
    <link href="https://blog.mylonly.com/15274889837567.html"/>
    <updated>2018-05-28T14:29:43+08:00</updated>
    <id>https://blog.mylonly.com/15274889837567.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA<br/>
加入以下代码</p>

<pre><code>import os 
os.environ[&#39;TF_CPP_MIN_LOG_LEVEL&#39;] = &#39;2&#39; 
</code></pre></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Weex 问题集锦]]></title>
    <link href="https://blog.mylonly.com/15253577270506.html"/>
    <updated>2018-05-03T22:28:47+08:00</updated>
    <id>https://blog.mylonly.com/15253577270506.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>weex中通过ref访问组件的两种情况:</p>

<pre><code>&lt;div ref=&quot;test&quot;&gt;&lt;/div&gt;
</code></pre>

<p>这种直接写入的可以通过this.$refs[&#39;test&#39;]获取到组件对象</p>

<pre><code>&lt;div ref=&quot;&#39;test&#39;+index&quot; v-for=&quot;(item,index) in items&quot;&gt;&lt;/div&gt;
</code></pre>

<p>上面这种通过v-for或者其他vue语法动态嵌入的组件，则需要通过this.\(refs[`test\){index}<code>][0]去获取，因为淡出的this.$refs[&quot;</code>test${index}`&quot;]获取到的是包含一个元素的数组对象</p></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[解决VSCode编写Django代码时经常提示objects属性等不存在的错误]]></title>
    <link href="https://blog.mylonly.com/15249709962104.html"/>
    <updated>2018-04-29T11:03:16+08:00</updated>
    <id>https://blog.mylonly.com/15249709962104.html</id>
    <content type="html"><![CDATA[
<p>如果你仅仅是装了pylint用来检测python代码，那么你在写django程序，尤其是使用model的一些查询语句时，如</p>

<pre><code>App.objects.all()
</code></pre>

<p>肯定会经常会被VSCode提示<code>App 没有objects这个属性</code>，虽然这个不影响代码的运行，但作为有强迫症的我们，怎么能容忍我们的代码还没运行就被标识为错误,实在是很影响心情。</p>

<blockquote>
<p>Django使用了大量的元编程思想，其中会有大量的修改对象属性和行为的操作，pylint提示的不存在的属性和方法会在程序运行中被django动态的加入，所以并不会影响程序运行。</p>
</blockquote>

<p>所以在网上稍微找了下，发现这个叫做<code>pylint-django</code>的<code>pylint</code>的插件可以去掉这些恼人的提示。</p>

<p>安装很简单,和pylint一样</p>

<pre><code>pip3 install pylint-django
</code></pre>

<p>然后通过pylint加载这个插件</p>

<pre><code>pylint --load-plugins pylint_django 
</code></pre>

<p>在VSCode里可以通过修改setting中的<code>python.linting.pylintArgs</code>这个键的值达到同样的目的</p>

<pre><code>&quot;python.linting.pylintArgs&quot;: [&quot;--load-plugins&quot;, &quot;pylint_django&quot;]
</code></pre>

<p>然后重启VSCode就好了</p>

<p>参考:<br/>
<a href="https://blog.landscape.io/using-pylint-on-django-projects-with-pylint-django.html">blog.landscape.io</a></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[nginx 相关技巧]]></title>
    <link href="https://blog.mylonly.com/15247238721302.html"/>
    <updated>2018-04-26T14:24:32+08:00</updated>
    <id>https://blog.mylonly.com/15247238721302.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>防止域名被人恶意指向，由于nginx存在默认的空主机头问题，可以通过添加如下配置，将未配置的域名强行重定向，或者return 404</p>

<pre><code>  server {
      listen 80 default;
      server_name _;
      rewrite ^(.*) http://www.mylonly.com permanent;
    }
</code></pre></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Django问题集锦]]></title>
    <link href="https://blog.mylonly.com/15244672126240.html"/>
    <updated>2018-04-23T15:06:52+08:00</updated>
    <id>https://blog.mylonly.com/15244672126240.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>Csrf验证</p>

<pre><code>需要在表单中加入`csrfmiddlewaretoken`或者在http头中`X-CSRFToken`请求头，具体值来自cookie当中的`csrftoken`
</code></pre></li>
<li><p>Django允许跨域</p>

<p>安装<code>django-cors-headers</code></p>

<pre><code>pip install django-cors-headers
</code></pre>

<p>在setting.py中增加如下代码</p>

<pre><code>INSTALLED_APPS = [
...
&#39;corsheaders&#39;，
...
 ] 

MIDDLEWARE_CLASSES = (
    ...
    &#39;corsheaders.middleware.CorsMiddleware&#39;,
    &#39;django.middleware.common.CommonMiddleware&#39;, # 注意顺序
    ...
)
#跨域增加忽略
CORS_ALLOW_CREDENTIALS = True
CORS_ORIGIN_ALLOW_ALL = True
CORS_ORIGIN_WHITELIST = (
    &#39;*&#39;
)

CORS_ALLOW_METHODS = (
    &#39;DELETE&#39;,
    &#39;GET&#39;,
    &#39;OPTIONS&#39;,
    &#39;PATCH&#39;,
    &#39;POST&#39;,
    &#39;PUT&#39;,
    &#39;VIEW&#39;,
)

CORS_ALLOW_HEADERS = (
    &#39;XMLHttpRequest&#39;,
    &#39;X_FILENAME&#39;,
    &#39;accept-encoding&#39;,
    &#39;authorization&#39;,
    &#39;content-type&#39;,
    &#39;dnt&#39;,
    &#39;origin&#39;,
    &#39;user-agent&#39;,
    &#39;x-csrftoken&#39;,
    &#39;x-requested-with&#39;,
    &#39;Pragma&#39;,
)
</code></pre></li>
<li><p>axios 默认支持csrf功能，需要修改其属性和服务器返回的csrfcookiename一致</p>

<pre><code>#Django返回的cookie中的csrf字段名字为csrftoken,会从request的header中读取X-CSRFTOKEN来校验csrf
axios.defaults.xsrfHeaderName = &quot;X-CSRFTOKEN&quot;;
axios.defaults.xsrfCookieName = &quot;csrftoken&quot;;
</code></pre>

<blockquote>
<p>如果axios要访问的接口和前端页面属于跨域状态，那么axios无法读取xsrfCookie,则也无法设置xsrfHeader,此时则需要后端将csrftoken放入response中而不是cookie当中，axios在request阻截器中手动设置xsrfHeader</p>
</blockquote>

<p><img src="media/15244672126240/15258764072402.jpg" alt=""/></p></li>
<li><p>Django使用django-rest-framework 单独某个view取消csrf验证</p>

<p>需要两步：</p>

<ul>
<li>关掉django的csrf验证,利用<code>@csrf_exempt</code>(针对function view)，或者用<code>@method_decorator(csrf_exempt, name=&quot;dispatch&quot;)</code>(针对class view)</li>
<li>关掉drf框架的验证，在class view 中将<code>authentication_classes</code>设置为<code>BasicAuthentication</code></li>
</ul>

<pre><code>@method_decorator(csrf_exempt, name=&quot;dispatch&quot;)
class login(views.APIView):
authentication_classes = (BasicAuthentication,)
def post(self, request, *args, **kwargs):

username = request.data.get(&quot;username&quot;)
password = request.data.get(&quot;password&quot;)

user = auth.authenticate(request, username=username, password=password)
if user is not None:
  auth.login(request, user)
  return ErosResponse()
else:
  return ErosResponse(status=ErosResponseStatus.INVALID_USER)
</code></pre></li>
<li><p>使用<code>OrderFilter</code>过程中遇到<code>OrderingFilter object has no attribute &#39;filter_queryset&#39;</code>:</p>

<p>需要使用从rest_framework.filters中导入的OrderingFilter，而不是django_filters</p>

<pre><code class="language-Python">from django_filters.rest_framework import DjangoFilterBackend
from rest_framework.filters import OrderingFilter
</code></pre></li>
<li><p>Django链接Mysql 8.0 出现错误(1045:Access denied for user &#39;root&#39;@&#39;localhost&#39; (using password: NO) 的一种解决方法</p>

<p>出现此错误的原因是MySQL8.0 密码的加密方式发生了改变，采用了cha2加密方法，我们可以利用下面的语句将MySQL的密码加密方式修改为之前的版本</p>

<pre><code>mysql -u root -p
use mysql；
ALTER USER &#39;root&#39;@&#39;localhost&#39; IDENTIFIED WITH mysql_native_password BY &#39;newpassword&#39;;  
FLUSH PRIVILEGES; 
</code></pre></li>
<li><p>Django-Filter自定义查询过滤字段</p>

<pre><code class="language-Python">from django_filters import FilterSet
from django_filters import CharFilter

class TagFilter(FilterSet):
    ##自定义search字段
    search = CharFilter(name=&quot;name&quot;, lookup_expr=&quot;contains&quot;)

    class Meta:
        model = Tag
        fields = {
          &#39;id&#39;: [&#39;exact&#39;],
          &#39;name&#39;: [&#39;exact&#39;, &#39;contains&#39;],
          &#39;search&#39;: [&#39;exact&#39;],
        }
</code></pre>

<p>具体Django-Filter的用法可以<a href="http://django-filter.readthedocs.io/en/1.1.0/">参考文档</a></p></li>
<li><p>DRF的OrderingFilter默认排序</p>

<pre><code class="language-Python">filter_backends = (DjangoFilterBackend, OrderingFilter)
ordering_fields = (&#39;id&#39;, &#39;hot&#39;)
ordering = (&#39;-hot&#39;,)
</code></pre></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Python 错误集锦]]></title>
    <link href="https://blog.mylonly.com/15244545128606.html"/>
    <updated>2018-04-23T11:35:12+08:00</updated>
    <id>https://blog.mylonly.com/15244545128606.html</id>
    <content type="html"><![CDATA[
<ol>
<li><p>ValueError: unknown locale: UTF-8 <br/>
<img src="https://pic.mylonly.com/2018-04-23-15244561735556.jpg" alt=""/></p>

<p>添加下面代码至用户目录的.bash_profile文件   </p>

<pre><code class="language-Bash">export LANG=&quot;en_US.UTF-8&quot;
export LC_COLLATE=&quot;en_US.UTF-8&quot;
export LC_CTYPE=&quot;en_US.UTF-8&quot;
export LC_MESSAGES=&quot;en_US.UTF-8&quot;
export LC_MONETARY=&quot;en_US.UTF-8&quot;
export LC_NUMERIC=&quot;en_US.UTF-8&quot;
export LC_TIME=&quot;en_US.UTF-8&quot;
export LC_ALL=
</code></pre></li>
<li><p>zip()函数在Python2和Python3中的区别</p>

<p>在Python2中，zip返回的是元祖的列表<br/>
而在Python3中，zip返回的是元祖组成的迭代器，得套个list函数才能将其转换为列表</p>

<pre><code>x = [1, 2, 3]
y = [a, b, c]

#python3
n = list(zip(x,y))
print(n)
#python2
n = zip(x,y)
print n
</code></pre></li>
</ol>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[利用GitLab提供的GitLab-CI以及GitLab-Runner搭建持续集成/部署环境]]></title>
    <link href="https://blog.mylonly.com/15241105353902.html"/>
    <updated>2018-04-19T12:02:15+08:00</updated>
    <id>https://blog.mylonly.com/15241105353902.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">简介</h2>

<ol>
<li><p>GitLab</p>

<blockquote>
<p>是一套基于Ruby开发的开源Git项目管理应用，其提供的功能和Github类似，不同的是GitLab提供一个GitLab CE社区版本，用户可以将其部署在自己的服务器上，这样就可以用于团队内部的项目代码托管仓库。</p>
</blockquote></li>
<li><p>GitLab CI</p>

<blockquote>
<p>是GitLab 提供的持续集成服务(从8.0版本之后，GitLab CI已经集成在GitLab中了)，只要在你的仓库根目录下创建一个.gitlab-ci.yml 文件， 并为该项目指派一个Runner，当有合并请求或者Push操作时，你写在.gitlab-ci.yml中的构建脚本就会开始执行。</p>
</blockquote></li>
<li><p>GitLab Runner</p>

<blockquote>
<p>是配合GitLab CI进行构建任务的应用程序，GitLab CI负责yml文件中各种阶段流程的执行，而GitLab Runner就是具体的负责执行每个阶段的脚本执行，一般来说GitLab Runner需要安装在单独的机器上通过其提供的注册操作跟GitLab CI进行绑定，当然，你也可以让其和GitLab安装在一起，只是有的情况下，你代码的构建过程对资源消耗十分严重的时候，会拖累GitLab给其他用户提供政策的Git服务。</p>
</blockquote></li>
<li><p>持续集成/部署环境</p>

<blockquote>
<p>持续集成是程序开发人员在频繁的提交代码之后，能有相应的环境能对其提交的代码自动执行构建(Build)、测试(Test),然后根据测试结果判断新提交的代码能否合并加入主分支当中,而持续部署也就是在持续集成之后自动将代码部署(Deploy)到生成环境上</p>
</blockquote></li>
</ol>

<h2 id="toc_1">开启GitLab CI功能</h2>

<p>在GitLab 8.0版本之后,你可以通过如下两部启用GitLab CI功能</p>

<ol>
<li> 新建一个<code>.gitlab-ci.yml</code>文件在你项目的根目录</li>
<li> 为你的项目配置一个GitLab Runner</li>
</ol>

<h4 id="toc_2">配置一个<code>.gitlab-ci.yml</code>文件</h4>

<p><code>.gitlab-ci.yml</code>文件是用来配置GitLab CI进行构建流程的配置文件，其采用YAML语法,所以你需要额外注意要用空格来代替缩进，而不是Tabs。下面通过我自己项目中的<code>.gitlab-ci.yml</code>文件来具体介绍其规则</p>

<pre><code class="language-YAML">stages:
  - init
  - check
  - build
  - deploy

cache:
  key: ${CI_BUILD_REF_NAME}
  paths:
  - node_modules/
  - dist/

#定义一个叫init的Job任务
init:
  stage: init
  script:
  -  cnpm install

#master_check Job:检查master分支上关键内容
master_check:
  stage: check
  script:
  - echo &quot;Start Check Eros Config ...&quot;
  - bash check.sh release 
  only:
  - master
  
#dev_check Job: 检查dev分支上关键内容
dev_check:
  stage: check
  script:
  - echo &quot;Start Check Eros Config ...&quot;
  - bash check.sh debug
  only:
  - dev

js_build:
  stage: build
  script:
  - eros build

master_deploy:
  stage: deploy
  script:
  - bash deploy.sh release
  only:
  - master

dev_deploy:
  stage: deploy
  script:
  - bash deploy.sh debug
  only:
  - dev

</code></pre>

<p>在上面的例子中，我们利用<code>stages</code>关键字来定义持续构建过程中的四个阶段init、chec、build、deploy</p>

<blockquote>
<p>关于GitLab CI中的stages,有如下几个特点:</p>
</blockquote>

<pre><code>1. 所有 Stages 会按照顺序运行，即当一个 Stage 完成后，下一个 Stage 才会开始
2. 只有当所有 Stages 完成后，该构建任务 (Pipeline) 才会成功
3. 如果任何一个 Stage 失败，那么后面的 Stages 不会执行，该构建任务 (Pipeline) 失败
</code></pre>

<p>然后我们利用<code>caches</code>字段来指定下面将要进行的job任务中需要共享的文件目录,如果没有，每个Job开始的时候，GitLab Runner都会删掉<code>.gitignore</code>里面的文件</p>

<p>紧接着，我们定义了一个叫做<code>init</code>的job，其通过stage字段声明属于<code>init</code>阶段，因此，这个job会第一个执行，我们在这个job当中，执行一些环境的初始化工作。</p>

<p>接下来是<code>check</code>阶段,用来检查代码的一些基础错误(代码规范之类不会被编译器发现的问题)，以及一些配置文件的检查，我将其命名为<code>master_check</code>和<code>dev_check</code>,通过only字段来告诉GitLab CI 只有当对应的分支有push操作的时候才会触发这个job。</p>

<p>然后就是代码的<code>build</code>阶段，由于此阶段不像上个极端，没有需要区分不同分支的命令，所以就只需要定义一个job就够了</p>

<p>最后的<code>deploy</code>，因为不同的分支需要发布到不同的环境，所以依然通过only来区分两个job。</p>

<blockquote>
<p>关于GitLab CI中的Jobs,也有如下几个特点:</p>
</blockquote>

<pre><code>1. 相同 Stage 中的 Jobs 会并行执行
2. 相同 Stage 中的 Jobs 都执行成功时，该 Stage 才会成功
3. 如果任何一个 Job 失败，那么该 Stage 失败，即该构建任务 (Pipeline) 失败
</code></pre>

<p>在我的这个构建任务当中，根据我的业务情况只用到了少许关键字,还有更多的类似于<code>before_script</code>、<code>after_script</code>等关键字，具体的可以参阅<a href="https://docs.gitlab.com/ce/ci/yaml/README.html">GitLab的官方文档</a></p>

<p>在我们完成<code>.gitlab-ci.yml</code>的流程编写之后，就可以将其放在项目的根目录下，然后push到我们的GitLab上，这时，如果你打开项目首页的Piplines标签页，会发现一个状态标识为<code>pending</code>的构建任务，如下图所示：<br/>
<img src="https://pic.mylonly.com/2018-04-19-075009.jpg" alt=""/><br/>
这时由于这个构建任务还有找到可用的GitLab Runner来执行其构建脚本，等我们接下来为我们的项目接入GitLab Runner之后，这些任务的状态就会由<code>pendding</code>变成<code>running</code>了</p>

<h4 id="toc_3">安装GitLab Runner</h4>

<p>找一台适合安装GitLab Runner的机器，无论是Windows或者Mac还是Linux都行，最好是那种比较空闲的能24小时开启的机器，我们在<a href="https://docs.gitlab.com/runner/install/">GitLab Runner的官网</a>找到我们平台的安装文件，以及对应的安装流程。由于笔者我准备安装GitLab Runner是一台闲置的iMac电脑，因此我就在演示MacOS下GitLab Runner的安装：</p>

<blockquote>
<p>GitLab Runner 在macOS和Linux/UNIX下安装流程是一样的，都是直接下载已编译好的二进制包</p>
</blockquote>

<ol>
<li><p>下载对应GitLab 版本的GitLab Runner</p>

<ul>
<li>如果你的GitLab是10.0之后的版本，GitLab Runner可执行文件改名为<code>gitlab-runner</code></li>
</ul>

<pre><code>sudo curl --output /usr/local/bin/gitlab-runner https://gitlab-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-runner-darwin-amd64
sudo chmod +x /usr/local/bin/gitlab-runner
</code></pre>

<ul>
<li>9.0~10.0之间的版本</li>
</ul>

<pre><code>sudo curl --output /usr/local/bin/gitlab-ci-multi-runner https://gitlab-ci-multi-runner-downloads.s3.amazonaws.com/latest/binaries/gitlab-ci-multi-runner-darwin-amd64
sudo chmod +x /usr/local/bin/gitlab-ci-multi-runner
</code></pre>

<ul>
<li>9.0 之前,由于9.0之后启用全新的API4接口，所以如果你的GitLab是9.0以前的版本,需要下载下面的版本,否则会导致你的GitLab Runner注册不上</li>
</ul>

<pre><code>sudo curl --output /usr/local/bin/gitlab-ci-multi-runnerhttps://gitlab-ci-multi-runner-downloads.s3.amazonaws.com/v1.11.1/binaries/gitlab-ci-multi-runner-darwin-amd64
sudo chmod +x /usr/local/bin/gitlab-ci-multi-runner
</code></pre>

<blockquote>
<p>不知道各位有没有注意到上面下载地址链接当中的v1.11.1,这个就是对应的Gitlab Runner,如果你的GitLab是9.0之前的版本，使用GitLab Runner v1.11.1这个版本仍然注册不上，可以尝试使用降几个版本的GitLab Runner,所有GitLab Runner发行的版本可以在<a href="https://gitlab.com/gitlab-org/gitlab-runner/tags">GitLab Runner Tags</a>找到</p>
</blockquote>

<p>假如你遇到不能通过登录服务器来确定GitLab版本号时，可以通过直接访问gitlab的首页，后面加上help，如下图：<br/>
<img src="https://pic.mylonly.com/2018-04-19-083100.png" alt=""/></p></li>
<li><p>注册GitLab Runner</p>

<p>执行下面命令，</p>

<pre><code>sudo gitlab-ci-multi-runner register
</code></pre>

<p>如果你的终端提示找不到命令，请通过<code>export PATH=/usr/local/bin:$PATH</code>将/usr/local/bin目录加入环境变量,或者你遗漏了上面的chmod命令导致文件不可执行。</p>

<p>执行完上面的命令之后，会让你输入下面的信息:</p>

<ul>
<li>Please enter the gitlab-ci coordinator URL:</li>
<li>Please enter the gitlab-ci token for this runner:</li>
<li>Please enter the gitlab-ci description for this runner</li>
<li>Please enter the gitlab-ci tags for this runner (comma separated):</li>
<li>Whether to run untagged builds [true/false]:</li>
<li>Please enter the executor:</li>
</ul>

<p>其中coordinator URL和token可以在你需要进行持续集成的项目的Runner标签页中找到<br/>
<img src="https://pic.mylonly.com/2018-04-19-084819.jpg" alt=""/><br/>
description和tags可以自己定义，是否build没有tag的提交这个也是根据你自己的需求来选择，默认是false，executer选择shell</p>

<p>填写完成之后如果提示<code>Registering runner... succeeded</code>表明这个Runner已经被注册成功了，之后你在返回进入项目的Runners页面，会发现下面多了一个处于Active状态的Runner</p>

<p>紧接着最后一步，启动我们刚注册的Runner</p>

<pre><code>sudo  gitlab-ci-multi-runner start
</code></pre>

<p>现在，我们切回项目的Pipelines当中，肯定会发现之前处于peding状态的任务已经开始running了，我们可以通过点击这个状态按钮来实时查看每个阶段的输出日志<br/>
<img src="https://pic.mylonly.com/2018-04-19-085612.jpg" alt=""/></p></li>
</ol>

<h2 id="toc_4">开启构建状态的邮件提醒</h2>

<p>如果你想收到关于每个构建任务的实时状态的邮件，你可以在在项目中的service标签野种启用Build Emails这个服务。</p>

<p><img src="https://pic.mylonly.com/2018-04-19-085906.png" alt=""/></p>

<h2 id="toc_5">结语</h2>

<p>搭建一个快速方便的持续集成、持续部署环境对于项目的开发来说是一个很重要的举措，无论你是采用大名鼎鼎的jenkins还是本文介绍的GitLab CI,它不仅仅可以帮助我们节省大量的时间在调试发布部署当中，也减少了我们因为人为因素导致的发布过程中出现的意外，有效的较低了项目开发当中的风险。</p>

<h4 id="toc_6">如果你想和我交流，可以关注我的订阅号:</h4>

<p><img src="https://pic.mylonly.com/2018-04-19-wechat-mylonly.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[在wxc-rich-text以及wxc-special-rich-text的思路上实现类似QQ表情的富文本多行排版]]></title>
    <link href="https://blog.mylonly.com/15241067803366.html"/>
    <updated>2018-04-19T10:59:40+08:00</updated>
    <id>https://blog.mylonly.com/15241067803366.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">先上效果图:</h2>

<p><img src="https://pic.mylonly.com/2018-03-30-093130.jpg" alt=""/></p>

<h2 id="toc_1">wxc-rich-text和wxc-special-rich-text的实现思路</h2>

<p>weex-ui中为我们提供了<code>wxc-rich-text</code>和<code>wxc-special-rich-text</code>两种富文本控件，其中，<code>wxc-rich-text</code> 只支持单行富文本显示,而<code>wxc-special-rich-text</code>只能支持最多两行特定种类的图文混排(<code>标签+文本</code>以及<code>图标+文本</code>) </p>

<p>首先我们来分析<code>wxc-rich-text</code>的源码来看看为什么这个控件不支持多行的样式</p>

<p>在<code>wxc-rich-text.vue</code>的代码中，我们找到这样一段css的样式,下图红色箭头所指处</p>

<p><img src="https://pic.mylonly.com/2018-03-30-093159.jpg" alt=""/></p>

<p>这个样式中并没有指定flex-wrap属性，而在flex布局中，flex-wrap的默认属性就是<code>nowrap</code>不换行。</p>

<pre><code>是不是就是这个原因导致`wxc-rich-text`不支持换行呢？
如果真是这样，那么weex-ui的开发者为什么不把flex-wrap属性设置为wrap而提供一个支持多行的`wxc-rich-text`控件呢？
</code></pre>

<p>带着这个疑问，我们手动在<code>wxc-rich-text.vue</code>中将flex-wrap属性加上，这段css样式改为:</p>

<pre><code class="language-css">.wxc-rich-text {
    align-items: center;
    flex-direction: row;
    flex-wrap: wrap;
} 
</code></pre>

<p>好了，我们尝试输入一个多行的文本来看看效果:<br/>
<img src="https://pic.mylonly.com/2018-03-30-093213.jpg" alt=""/></p>

<p>在上图中我们发现，虽然整个控件的内容虽然确实有两行了，但是并不是我们想要的效果，文字部分并没有紧接着前面一个文字或者图片的后面，我想这也是为什么weex-ui的开发者在<code>wxc-rich-text</code>控件中不将flex-wrap属性设置为wrap的原因了。</p>

<p>好了，分析完<code>wxc-rich-text</code>不支持多行的原因，我们再来看看为什么<code>wxc-special-rich-text</code>可以支持两行的富文本呢？</p>

<p>当然还是先看源码,在<code>wxc-special-rich-text.vue</code>中，有下面一段代码:<br/>
<img src="https://pic.mylonly.com/2018-03-30-093224.jpg" alt=""/></p>

<p>代码当中有两个text控件，且两个text控件分别读取了newList[0]和newList[1]中的数据,为什么要如此呢，我们来看下面的js代码，在vue的computed当中，我们找到了名为<code>newList</code>的计算属性，代码有点长，我分别截了两张图<br/>
<img src="https://pic.mylonly.com/2018-03-30-093720.png" alt=""/><img src="https://pic.mylonly.com/2018-03-30-093640.png" alt=""/></p>

<p>如果你不想看上面的代码，可以直接看下面的结论：</p>

<p><code>这个newList中，就是将configLis中有的text文本内容切割成了两段文本分别放进两个text控件当中（依据前面已有的icon或者tag控件来计算第一行可以塞下的字符长度，其余字符就是第二行的文本内容）</code></p>

<p>好了，这下知道为什么<code>wxc-special-rich-text.vue</code>只能显示不超过两行的的富文本了吧，而且该富文本还必须要是<code>icon+text</code>或者<code>tag+text</code>的格式。</p>

<h2 id="toc_2">我的多行富文本实现思路</h2>

<p>既然<code>wxc-special-rich-text</code>通过将文本内容切割成两段文本来实现两行富文本的功能，那我能否通过将文本切割成粒度更小的的内容来解决多行富文本呢，这个粒度又是多少才最合适呢，我想，作为程序员，粒度为1应该是很容易想出来的一个数字，我也是如此。在粒度为1的情况下是不会存在<code>wxc-rich-text</code>中出现的因为第一行排列不下而将自己移至第二行从而导致第一行末端出现大量空白的情况。</p>

<p>那我们就在这个思路下实现我们自己的多行富文本</p>

<p>首先第一步，在<code>wxc-rich-text.vue</code>当中设置flex-wrap为wrap值，如上面文章所示。</p>

<p>如果你是用npm包的方式引入的weex-ui，那你可能就需要将wxc-rich-text.vue的代码拷贝一份，重新起个别的名字的控件了，然后再将flex-wrap属性设置成wrap。</p>

<p>第二步，将你的富文本内容切割成最小粒度的config，塞入<code>wxc-rich-text</code>的configList当中，我的实现如下:</p>

<pre><code class="language-javascript"> addNormalMessage:function(msg_id,sender,sender_level,title,message){
        console.log(&quot;normal message:&quot;,msg_id,sender,sender_level,title,message)
        var configList = []
        var config = this.addGrade(sender_level,title)
        if (config != {}){
          configList.push(config)
        }
        configList.push( {
          type: &#39;text&#39;,
          value: sender+&quot;:&quot;,
          theme: &#39;blue&#39;
        })

        var message_list = this.addMessage(message)

        message_list.forEach(config =&gt; {
          configList.push(config)
        })     
        this.messages.push(configList)
        this.messagesDict[msg_id] = this.messages.length - 1
        this.scroolToEnd()
    },
</code></pre>

<p>代码当中有三个部分的内容，addGrade是增加徽标的config，此处代码有点啰嗦，因为当时应该是直接复制拷贝的，可以通过设置value，color等等一些变量让代码精简很多。</p>

<pre><code class="language-javascript">addGrade(sender_level,title){
          console.log(&quot;grade:&quot;,sender_level,title)
          var config = {}
            if (sender_level == 500){
              config = {
                  type: &#39;tag&#39;,
                  value: &#39;讲师&#39;,
                  style: {
                    fontSize: 30,
                    color: &#39;#ffffff&#39;,
                    borderColor: &#39;#2d9b3a&#39;,
                    backgroundColor: &#39;#2d9b3a&#39;,
                    height: 40
                  }
              }
            }else if(sender_level == 900){
              config = {
                type:&#39;tag&#39;,
                value:&#39;管理&#39;,
                style: {
                  fontSize: 30,
                  color: &#39;#ffffff&#39;,
                  borderColor: &#39;#ec24dd&#39;,
                  backgroundColor: &#39;#ec24dd&#39;,
                  height: 40
                }
              }
            }else if(sender_level == 2000){
              config = {
                type:&#39;tag&#39;,
                value:&#39;室主&#39;,
                style:{
                  fontSize:30,
                  color: &#39;#ffffff&#39;,
                  borderColor: &#39;#e80c19&#39;,
                  backgroundColor: &#39;#e80c19&#39;,
                  height: 40
                }
              }
            }else{
              if(title &gt;= 0){
                config = {
                  type:&#39;icon&#39;,
                  src:&quot;bmlocal://assets/grade/Grade_&quot;+title+&quot;.png&quot;,
                  style: {
                    width: 90,
                    height: 40
                  }
                }
              }
            }
            return config
        },
</code></pre>

<p>然后加上发送人的姓名，最后加上聊天的内容addMessage,在addMessage当中我实现了QQ表情的匹配已经文本内容的切割</p>

<pre><code>addMessage(message){
      var char_list = []
      var chars = message.split(&#39;&#39;)
      var startIndex = -1
      chars.forEach((char,index,array) =&gt; {
        if(char == &#39;[&#39;){
          startIndex = index
        }else if(char == &#39;]&#39;){
          if(startIndex != -1){
            var emotionStr = array.slice(startIndex,index+1).join(&quot;&quot;)
    
            if (emotionStr.indexOf(&quot;http&quot;) &gt; -1){  //图文消息
              var imageUrl = emotionStr.slice(1,index)
              char_list.push(this.oneImageConfig(imageUrl))
            }else{//表情文字
              char_list.push(this.oneEmojConfig(emotionStr))
            }
    
            startIndex = -1
        
          }else{
            char_list.push(this.oneCharConfig(char))
          }
        }else{
          if (startIndex == -1){
            char_list.push(this.oneCharConfig(char))
          }
        }
      });
      return char_list
    },
</code></pre>

<p>我们的业务当中，和大部分公司类似，我们的QQ表情是类似于[微笑][害羞]这种格式的。其中在查找表情的代码中，也许你们会疑问，为什么我不使用正则去匹配QQ表情，其实在我之前未重构之前的项目中（使用swift的作为开发语言），此处业务逻辑就是通过正则将QQ表情替换出来，但是在此处，由于要切割字符串为每个字符串返回一个配置，字符串的逐个遍历已经不可避免，如果在加上正则匹配，时间负责度反而会增加一倍，所以我直接在遍历循环中加入了查找QQ表情的代码，希望能减轻少许我这种投机取巧的方法实现多行富文本样式带来的性能损耗。</p>

<pre><code class="language-javascript">oneEmojConfig(emojName){
  var localPath = emotion.emojLocalPath(emojName)
  console.log(&quot;localpath:&quot;,localPath)
  if( localPath != null){
    return {
      type:&#39;icon&#39;,
      src:localPath,
      style:{
        width:40,
        height:40
      }
    }
  }else{
    return {
      type:&#39;text&#39;,
      value:emojName,
      theme:&quot;yellow&quot;
    }
  }
},
</code></pre>

<p>在设置QQ表情的config当中，我写了一个emotion.js的工具函数，用来返回QQ表情名字所对应的本地图片的路径</p>

<p><code>emotion.js</code></p>

<pre><code class="language-javascript">/**
 * 表情转换工具类
 * @authors Root (root@mylonly.com)
 * @date    2018-03-30 08:48:19
 * @version 1.0.0
 */



let emotionFunc = {
    emotionArray : [&quot;[微笑]&quot;,&quot;[撇嘴]&quot;,&quot;[色]&quot;,&quot;[发呆]&quot;,&quot;[得意]&quot;,&quot;[流泪]&quot;,&quot;[害羞]&quot;,&quot;[闭嘴]&quot;,&quot;[睡]&quot;,&quot;[大哭]&quot;,&quot;[尴尬]&quot;,&quot;[发怒]&quot;,&quot;[调皮]&quot;,&quot;[呲牙]&quot;,&quot;[惊讶]&quot;,&quot;[难过]&quot;,&quot;[酷]&quot;,&quot;[冷汗]&quot;,&quot;[抓狂]&quot;,&quot;[吐]&quot;,&quot;[偷笑]&quot;,&quot;[可爱]&quot;,
&quot;[白眼]&quot;,&quot;[傲慢]&quot;,&quot;[饥饿]&quot;,&quot;[困]&quot;,&quot;[惊恐]&quot;,&quot;[流汗]&quot;,&quot;[憨笑]&quot;,&quot;[大兵]&quot;,&quot;[奋斗]&quot;,&quot;[咒骂]&quot;,&quot;[疑问]&quot;,&quot;[嘘]&quot;,&quot;[晕]&quot;,&quot;[折磨]&quot;,&quot;[衰]&quot;,&quot;[骷髅]&quot;,&quot;[敲打]&quot;,&quot;[再见]&quot;,&quot;[擦汗]&quot;,&quot;[抠鼻]&quot;,&quot;[鼓掌]&quot;,&quot;[糗大了]&quot;,&quot;[坏笑]&quot;,&quot;[左哼哼]&quot;,&quot;[右哼哼]&quot;,&quot;[哈欠]&quot;,
&quot;[鄙视]&quot;,&quot;[委屈]&quot;,&quot;[快哭了]&quot;,&quot;[阴险]&quot;,&quot;[亲亲]&quot;,&quot;[吓]&quot;,&quot;[可怜]&quot;,&quot;[菜刀]&quot;,&quot;[西瓜]&quot;,&quot;[啤酒]&quot;,&quot;[篮球]&quot;,&quot;[乒乓]&quot;,&quot;[咖啡]&quot;,&quot;[饭]&quot;,&quot;[猪头]&quot;,&quot;[玫瑰]&quot;,&quot;[凋谢]&quot;,&quot;[示爱]&quot;,&quot;[爱心]&quot;,&quot;[心碎]&quot;,&quot;[蛋糕]&quot;,&quot;[闪电]&quot;,&quot;[炸弹]&quot;,&quot;[刀]&quot;,&quot;[握手]&quot;,&quot;[胜利]&quot;,
&quot;[便便]&quot;,&quot;[NO]&quot;,&quot;[OK]&quot;,&quot;[抱拳]&quot;,&quot;[弱]&quot;,&quot;[强]&quot;],

    emojLocalPath:function(emoj_name){
      var index = this.emotionArray.indexOf(emoj_name)
      if (index &gt; -1){
          return &quot;bmlocal://assets/emotions/&quot;+index+&quot;@2x.png&quot;
      }
      return null
    },
    emojName:function(index){
        if (index &lt; this.emotionArray.length){
            return this.emotionArray[index]
        }
        return null
    }
}

export default emotionFunc;

</code></pre>

<p>至此，我的整个实现已经完全呈现出来了，如果你能看懂我上面所写，正好你也正好有和我类似的需求，相信你也能在我的思路下实现自己的业务代码。</p>

<p>最后，愿世界和平！！！</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Ubuntu 安装uwsgi]]></title>
    <link href="https://blog.mylonly.com/15126534156721.html"/>
    <updated>2017-12-07T21:30:15+08:00</updated>
    <id>https://blog.mylonly.com/15126534156721.html</id>
    <content type="html"><![CDATA[
<p>先源码安装python3.6（记得先用apt-get 安装好一些系统库zlib_dev,openssl_dev）<br/>
然后利用pip3 安装uwsgi</p>

<pre><code class="language-Shell">wget https://www.python.org/ftp/python/3.6.3/Python-3.6.3.tar.xz
tar -xvf Python-3.6.3.tar.xz
cd Python-3.6.3/

apt-get install zlib1g-dev
apt-get install libssl-dev
apt-get install sqlite3


./configure --enable-loadable-sqlite-extensions

make &amp;&amp; sudo make install


pip3 install uwsgi

uwsgi -i /data/web/xxxx.ini
</code></pre>

<p>uwsgi 配置侦测文件改动自动自动重启，在ini中加入</p>

<pre><code>py-autoreload = 1
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Django 发送html格式的邮件]]></title>
    <link href="https://blog.mylonly.com/15119665687635.html"/>
    <updated>2017-11-29T22:42:48+08:00</updated>
    <id>https://blog.mylonly.com/15119665687635.html</id>
    <content type="html"><![CDATA[
<p>django中默认提供了发送邮件的库<code>mail</code>，通过这个库我们可以很方便的通过django发送一份电子邮件</p>

<h5 id="toc_0">1. 在setting 中指定邮件服务器的基本信息</h5>

<pre><code class="language-Python">EMAIL_USE_SSL = True    
EMAIL_HOST = &#39;smtp.exmail.qq.com&#39;  # 如果是 163 改成 smtp.163.com
EMAIL_PORT = 465
EMAIL_HOST_USER = &#39;***@domain.com&#39; # 帐号
EMAIL_HOST_PASSWORD = &#39;password&#39;  # 密码
DEFAULT_FROM_EMAIL = EMAIL_HOST_USER
</code></pre>

<h5 id="toc_1">2. 引用mail库发送邮件</h5>

<pre><code class="language-Python">from django.core.mail import EmailMessage
from Across.settings import EMAIL_HOST_USER

msg = EmailMessage(&quot;邮件标题&quot;,&quot;邮件内容&quot;,EMAIL_HOST_USER,[接受邮件列表])
msg.send()
</code></pre>

<h5 id="toc_2">3.发送html模板邮件</h5>

<p>可以利用Django的template库读取指定的html模板，然后将参数代入，首选需要在settings中设置template的目录</p>

<pre><code class="language-Python">TEMPLATES = [
    {
        &#39;BACKEND&#39;: &#39;django.template.backends.django.DjangoTemplates&#39;,
        &#39;DIRS&#39;: [
            os.path.join(BASE_DIR,&#39;templates&#39;)  ##你的模板目录
        ],
        &#39;APP_DIRS&#39;: True,
        &#39;OPTIONS&#39;: {
            &#39;context_processors&#39;: [
                &#39;django.template.context_processors.debug&#39;,
                &#39;django.template.context_processors.request&#39;,
                &#39;django.contrib.auth.context_processors.auth&#39;,
                &#39;django.contrib.messages.context_processors.messages&#39;,
            ],
        },
    },
]
</code></pre>

<p>然后将你准备好的html模板放入该目录，然后就是利用temlate的loader函数加载模板传入指定参数</p>

<pre><code class="language-Python">html_content = loader.render_to_string(&#39;email.html&#39;,{&#39;authcode&#39;:random,&#39;title&#39;:&quot;标题标题&quot;,&#39;operation&#39;:operation_str})
msg = EmailMessage(&quot;您的验证码&quot;,html_content,EMAIL_HOST_USER,[email])
msg.content_subtype = &quot;html&quot; # Main content is now text/html
msg.send()
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Django扩展默认的User Model]]></title>
    <link href="https://blog.mylonly.com/15117871248079.html"/>
    <updated>2017-11-27T20:52:04+08:00</updated>
    <id>https://blog.mylonly.com/15117871248079.html</id>
    <content type="html"><![CDATA[
<p>Django中为我们默认提供了用户模块<code>User</code>,但是其当中的字段可能并不能完全满足我们的需求，这时我们就需要自定义一个<code>User Model</code>出来,如果你对自带的User模型很满意，仅仅是需要添加几个额外的字段，你就可以新建一个model类继承<code>AbstractUser</code>, 或者你完全不需要User模型中提供的类似<code>first_name</code>、<code>last_name</code>这些字段，你可以将<code>model</code>类继承<code>AbstractBaseUser</code>,这样只会保留<code>password</code>,<code>last_login</code>,<code>is_active</code>这三个字段。</p>

<blockquote>
<p>本文介绍的是继承自<code>AbstractUser</code>的用法</p>
</blockquote>

<p>在models.py中新建一个model继承自AbstractUser</p>

<pre><code>from django.db import models

# Create your models here.
from django.contrib.auth.models import AbstractUser
from enum import Enum


class UserProfile(AbstractUser):

    GENDER = [
        (0,&quot;未知&quot;),
        (1,&quot;男性&quot;),
        (2,&quot;女性&quot;)
    ]

    nickname = models.CharField(max_length=50)
    intro = models.CharField(max_length=500)
    mobile = models.CharField(max_length=13)
    gender = models.IntegerField(choices=GENDER,default=0)

    REQUIRED_FIELDS = [&#39;nickname&#39;, &#39;intro&#39;, &#39;mobile&#39;, &#39;gender&#39;]

</code></pre>

<p>在setting.py中设置 <code>AUTH_USER_MODEL</code></p>

<pre><code>AUTH_USER_MODEL = &#39;User.UserProfile&#39;
</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[利用Scrapy-Splash抓取JS动态渲染的网页数据]]></title>
    <link href="https://blog.mylonly.com/15052000291680.html"/>
    <updated>2017-09-12T15:07:09+08:00</updated>
    <id>https://blog.mylonly.com/15052000291680.html</id>
    <content type="html"><![CDATA[
<p>随着越来越多的网站开始用JS在客户端浏览器动态渲染网站，导致很多我们需要的数据并不能由原始的html中获取，再加上Scrapy本身并不提供JS渲染解析的功能，通常对这类网站数据的爬取我们一般采用两种方法：</p>

<ol>
<li>通过分析网站，找到对应数据的接口，模拟接口去获取我们需要的数据(参见<a href="https://www.mylonly.com/14945011244738.html">Scrapy抓取Ajax动态页面</a>),但是一旦该网站的接口隐藏的很深，或者接口的加密过于复杂，此种方法可能就有点行不通了</li>
<li>借助JS内核，将获取到的含有JS脚本的页面交由JS内核去渲染，最后将渲染后生成的html返回给Scrapy分析，比较常见的WebKit和Scrapy-Splash</li>
</ol>

<p>本篇文章的目的就是用来介绍如何使用Scrapy-Splash来配合Scrapy抓取动态页面这个问题。</p>

<h3 id="toc_0">准备工作</h3>

<ol>
<li><p>Docker安装,具体安装步骤参考<a href="https://docs.docker.com/engine/installation/">Docker官网</a></p>

<blockquote>
<p>为什么要安装Docker?<br/>
因为Scrapy-Splash使用了<code>Splash HTTP API</code>,所以你需要提供一个Splash实例，而在Docker镜像中已经有现成的Splash实例了，可以很方便的使用。</p>
</blockquote></li>
<li><p>Docker镜像源更改,参考<a href="https://ieevee.com/tech/2016/09/28/docker-mirror.html">国内 docker 仓库镜像对比</a></p></li>
<li><p>安装运行Splash</p>

<pre><code>docker pull scrapinghub/splash   #从docker镜像中拉取splash实例
docker run -p 8050:8050 scrapinghub/splash  #启动splash实例
</code></pre></li>
</ol>

<h3 id="toc_1">Scrapy配置</h3>

<p>在Scrapy项目的setting.py中加入如下内容：</p>

<pre><code class="language-Python">SPLASH_URL = &#39;http://localhost:8050&#39;  

DOWNLOADER_MIDDLEWARES = {
&#39;scrapy_splash.SplashCookiesMiddleware&#39;: 723,
&#39;scrapy_splash.SplashMiddleware&#39;: 725,
&#39;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&#39;: 810,
}

SPIDER_MIDDLEWARES = {
&#39;scrapy_splash.SplashDeduplicateArgsMiddleware&#39;: 100,
}

DUPEFILTER_CLASS = &#39;scrapy_splash.SplashAwareDupeFilter&#39;

HTTPCACHE_STORAGE = &#39;scrapy_splash.SplashAwareFSCacheStorage&#39;

</code></pre>

<h3 id="toc_2">实际代码解析</h3>

<p>我们以<a href="http://stock.qq.com/l/stock/ywq/list20150423143546.htm">腾讯证券</a>这个页面为例，腾讯的证券新闻列表是js动态渲染而成</p>

<p>我们直接打开这个链接，然后打开开发者工具，定位到新闻列表处:<br/>
<img src="https://pic.mylonly.com/2017-09-12-074653.jpg" alt=""/></p>

<p>我们在从network中查看第一次请求的Response时发现，返回的html中该列表页处是空的<br/><br/>
<img src="https://pic.mylonly.com/2017-09-12-075028.jpg" alt=""/></p>

<p>实际的数据被藏着JS里，加载完成后由JS操作DOM插入完成<br/>
<img src="https://pic.mylonly.com/2017-09-12-075305.jpg" alt=""/></p>

<p>此处由于实际数据被塞到了一段JS的变量里面，并不是由Ajax调用接口获取的，因此为了避免自己手动去截取js变量，我们便将该页面交给Scrapy-Splash渲染</p>

<pre><code class="language-Python">import scrapy
from FinancialInfoSpider.items import ArticleItem
from scrapy_splash import SplashRequest
from w3lib.html import remove_tags
import re
from bs4 import BeautifulSoup

class TencentStockSpider(scrapy.Spider):
    name = &quot;TencentStock&quot;
    def start_requests(self):
        urls = [
           &#39;http://stock.qq.com/l/stock/ywq/list20150423143546.htm&#39;,
        ]

        for url in urls:
            yield SplashRequest(url, self.parse, args={&#39;wait&#39;: 0.5})

    def parse(self,response):

        sel = scrapy.Selector(response)
        links = sel.xpath(&quot;//div[@class=&#39;qq_main&#39;]//ul[@class=&#39;listInfo&#39;]//li//div[@class=&#39;info&#39;]//h3//a/@href&quot;).extract()
        requests = []
        
        for link in links:
            request = scrapy.Request(link, callback =self.parse_article)
            requests.append(request)
        return requests

    def parse_article(self,response):

        sel = scrapy.Selector(response)

        article = ArticleItem()
        article[&#39;title&#39;] = sel.xpath(&#39;//*[@id=&quot;Main-Article-QQ&quot;]/div/div[1]/div[1]/div[1]/h1/text()&#39;).extract()[0]
        article[&#39;source&#39;] = sel.xpath(&#39;//*[@id=&quot;Main-Article-QQ&quot;]/div/div[1]/div[1]/div[1]/div/div[1]/span[2]&#39;).xpath(&#39;string(.)&#39;).extract()[0]
        article[&#39;pub_time&#39;] = sel.xpath(&#39;//*[@id=&quot;Main-Article-QQ&quot;]/div/div[1]/div[1]/div[1]/div/div[1]/span[3]/text()&#39;).extract()[0]
        
        html_content = sel.xpath(&#39;//*[@id=&quot;Cnt-Main-Article-QQ&quot;]&#39;).extract()[0]
        article[&#39;content&#39;] = self.remove_html_tags(html_content)
        return article


    def remove_html_tags(self,html):
        
        soup = BeautifulSoup(html)
        [s.extract() for s in soup(&#39;script&#39;)]
        [s.extract() for s in soup(&#39;style&#39;)] 
         
        content = &#39;&#39;
        for substring in soup.stripped_strings:
            content = content + substring

        return content       
</code></pre>

<p>主要代码就一句，将获取到的页面发送给本地的Splash实例去渲染解析，最后将结果返回给parse函数解析</p>

<pre><code>SplashRequest(url, self.parse, args={&#39;wait&#39;: 0.5})
</code></pre>

<p>里面用了BeautifulSoup这个库去除了html中得script和style标签，具体用法可以参考这两篇文章:</p>

<p><a href="http://cuiqingcai.com/1319.html">Python爬虫利器二之Beautiful Soup的用法</a><br/>
<a href="https://my.oschina.net/letiantian/blog/504669">使用BeautifulSoup删除html中的script、注释</a></p>

<p>输出结果:</p>

<p><img src="https://pic.mylonly.com/2017-09-12-080930.jpg" alt=""/></p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[CentOS6 安装部署Zabbix]]></title>
    <link href="https://blog.mylonly.com/14972546183511.html"/>
    <updated>2017-06-12T16:03:38+08:00</updated>
    <id>https://blog.mylonly.com/14972546183511.html</id>
    <content type="html"><![CDATA[
<blockquote>
<p>Zabbix是一个基于WEB界面的提供分布式系统监视以及网络监视功能的企业级的开源解决方案。zabbix能监视各种网络参数，保证服务器系统的安全运营；并提供灵活的通知机制以让系统管理员快速定位/解决存在的各种问题。</p>
</blockquote>

<h2 id="toc_0">部署LAMP|LNMP环境</h2>

<p>由于Zabbix的Web管理页面是由php写的，所以需要服务器PHP环境的支持，Linux部署LAMP或者LNMP的教程很多，在此就不在赘述。<br/>
需要注意的是:Zabbix支持的PHP最低版本是PHP5.4,而目前CentOS6中通过yum安装的php版本为5.3，所以最好采用源码编译安装。<br/>
当然，如果你觉得自己搞不定源码安装的各种编译参数，本文也给出一种yum安装php5.5版本的方法</p>

<h3 id="toc_1">Yum 安装PHP5.5</h3>

<ol>
<li>CentOS6安装PHP5.5源
<code>rpm -ivh http://mirror.webtatic.com/yum/el6/latest.rpm</code></li>
<li>安装php5.5
<code>yum install php55w php55w-gd php55w-mbstring php55w-mysql php55w-fpm php55w-bcmath php55w-xmlreader php55w-xmlwriter</code></li>
</ol>

<p>修改PHP配置，编辑/etc/php.ini,修改下列参数</p>

<pre><code>max_execution_time = 300
max_input_time = 300
memory_limit = 128M
post_max_size = 32M
date.timezone = Asia/Shanghai
</code></pre>

<h2 id="toc_2">zabbix数据库</h2>

<ol>
<li>从<a href="http://sourceforge.net/projects/zabbix/files/ZABBIX%20Latest%20Stable/3.2.6/zabbix-3.2.6.tar.gz/download%5D">zabbix-3.2.6</a>下载最新版本的zabbix源码并解压至当前目录</li>
<li>进入mysql数据库  <code>mysql -u root -p</code></li>
<li>创建zabbix数据库 <code>create database zabbix character set utf8;</code></li>
<li>创建zabbix用户并给与进入zabbix数据库的权限（用户名:zabbix,密码:123456） `grant all privileges on zabbix.* to zabbix@&#39;%&#39; identified by &#39;123456&#39;;</li>
<li>刷新系统权限表 <code>flush privileges;</code></li>
<li><p>进入之前下载的zabbix源码目录，找到database文件夹，里面有<code>schema.sql</code>、<code>images.sql</code>、<code>data.sql</code>，将这三个sql文件依次导入之前创建好的zabbix数据库中（PS,请按照schema-&gt;images-&gt;data的顺序依次导入，否则会出错）</p>

<pre><code>mysql&gt; use zabbix;
mysql&gt; source /root/zabbix-3.2.6/database/schema.sql;
mysql&gt; source /root/zabbix-3.2.6/database/images.sql;
mysql&gt; source /root/zabbix-3.2.6/database/data.sql;
</code></pre></li>
</ol>

<h2 id="toc_3">添加zabbix用户和组</h2>

<pre><code>groupadd zabbix
useradd -g zabbix -m zabbix
</code></pre>

<h2 id="toc_4">安装zabbix其他依赖包</h2>

<pre><code>yum install net-snmp-devel curl curl-devel mysql-devel
</code></pre>

<h2 id="toc_5">编译安装zabbix</h2>

<ol>
<li><p>进入zabbix源码根目录，生成makefile</p>

<pre><code>./configure --prefix=/usr/local/zabbix --with-mysql --with-net-snmp --with-libcurl --enable-server --enable-agent --enable-proxy
</code></pre></li>
<li><p>安装</p>

<pre><code>make
make install
</code></pre></li>
</ol>

<h2 id="toc_6">添加zabbix服务对应的端口</h2>

<pre><code>vim /etc/services ##在文件中加入以下端口
zabbix-agent 10050/tcp # Zabbix Agent
zabbix-agent 10050/udp # Zabbix Agent
zabbix-trapper 10051/tcp # Zabbix Trapper
zabbix-trapper 10051/udp # Zabbix Trapper
</code></pre>

<h2 id="toc_7">修改zabbix配置</h2>

<ol>
<li><p>修改 /usr/local/zabbix/etc/zabbix_server.conf</p>

<pre><code>DBName=zabbix #数据库名称

DBUser=zabbix #数据库用户名

DBPassword=123456 #数据库密码

ListenIP=127.0.0.1 #数据库ip地址

AlertScriptsPath=/usr/local/zabbix/share/zabbix/alertscripts #zabbix运行脚本存放目录
</code></pre></li>
<li><p>修改 /usr/local/zabbix/etc/zabbix_agentd.conf</p>

<pre><code>Include=/usr/local/zabbix/etc/zabbix_agentd.conf.d/
</code></pre></li>
</ol>

<h2 id="toc_8">让zabbix开机启动</h2>

<ol>
<li><p>拷贝可执行文件至init.d目录</p>

<pre><code>cp /usr/local/zabbix/sbin/zabbix_server /etc/rc.d/init.d/zabbix_server  #服务端
cp /usr/local/zabbix/sbin/zabbix_agentd /etc/rc.d/init.d/zabbix_agentd  #客户端
</code></pre></li>
<li><p>添加脚本执行权限</p>

<pre><code>chmod +x /etc/rc.d/init.d/zabbix_server
chmod +x /etc/rc.d/init.d/zabbix_agentd 
</code></pre></li>
<li><p>加入开机启动</p>

<pre><code>chkconfig zabbix_server on
chkconfig zabbix_agentd on
</code></pre></li>
<li><p>修改开启启动脚本中zabbix的安装路径,分别用vim打开/etc/rc.d/init.d/zabbix_server和/etc/rc.d/init.d/zabbix_agentd,修改BASEDIR参数为/usr/local/zabbix</p>

<pre><code>BASEDIR=/usr/local/zabbix/ #zabbix安装目录
</code></pre></li>
</ol>

<h2 id="toc_9">安装zabbix web管理页面</h2>

<ol>
<li><p>拷贝zabbix源码当中的frontends/php下面的所有文件至web服务器的根目录，重启web服务器，如果你的LNMP环境或者LAMP环境没有问题的话，浏览器中输入服务器地址应该会出现zabbix的setup页面</p></li>
<li><p>点击next按钮，之后会检测zabbix所需要的条件是否完全支持(PS:缺少什么就安装什么，一定要全部OK才可继续下去)</p></li>
<li><p>之后会出现连接数据库的页面,填入之前创建zabbix数据库时的信息</p></li>
<li><p>第4步出现当前zabbix服务器的一些基本信息，name填一下</p></li>
<li><p>安装向导会自动在服务器根目录下的conf下创建一个zabbix.conf.php文件,如果你的根目录权限设置不正确导致向导无法写入文件，这时就需要你手动将这个文件上传到服务器的正确目录</p></li>
<li><p>一切OK之后就会出现登录页面，初始用户名admin,初始密码zabbix</p></li>
</ol>

]]></content>
  </entry>
  
</feed>
